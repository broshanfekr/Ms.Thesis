Parameters
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=100
CHECKPOINT_EVERY=5
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=150
EVALUATE_EVERY=10
FILTER_SIZES=3,4,5
L2_CONSTRAINT=3.0
L2_REG_LAMBDA=1.0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=50
NUM_FILTERS=150
Loading data...
Train/Dev split: 25000/25000
epoch number is: 0
2016-05-20T16:04:13.480080: train-step 1, loss 4.40738, acc 0.54
2016-05-20T16:04:21.092250: train-step 2, loss 3.6642, acc 0.5
2016-05-20T16:04:28.855914: train-step 3, loss 4.17146, acc 0.45
2016-05-20T16:04:38.204585: train-step 4, loss 3.9789, acc 0.48
2016-05-20T16:04:47.595174: train-step 5, loss 4.19736, acc 0.44
2016-05-20T16:04:56.846640: train-step 6, loss 3.81918, acc 0.5
2016-05-20T16:05:06.321996: train-step 7, loss 3.54437, acc 0.51
2016-05-20T16:05:15.642283: train-step 8, loss 3.71359, acc 0.46
2016-05-20T16:05:24.727342: train-step 9, loss 3.73022, acc 0.48
2016-05-20T16:05:34.218461: train-step 10, loss 3.36543, acc 0.57
2016-05-20T16:05:43.779055: train-step 11, loss 3.69552, acc 0.48
2016-05-20T16:05:53.135546: train-step 12, loss 3.66641, acc 0.51
2016-05-20T16:06:02.623004: train-step 13, loss 3.44331, acc 0.48
2016-05-20T16:06:11.771344: train-step 14, loss 3.22159, acc 0.6
2016-05-20T16:06:20.870646: train-step 15, loss 3.67989, acc 0.44
2016-05-20T16:06:29.707996: train-step 16, loss 3.47962, acc 0.52
2016-05-20T16:06:38.493457: train-step 17, loss 3.16003, acc 0.52
2016-05-20T16:06:46.710288: train-step 18, loss 3.46404, acc 0.44
2016-05-20T16:06:56.208958: train-step 19, loss 3.33266, acc 0.5
2016-05-20T16:07:03.875132: train-step 20, loss 3.40088, acc 0.52
2016-05-20T16:07:11.750688: train-step 21, loss 3.02841, acc 0.56
2016-05-20T16:07:19.486394: train-step 22, loss 3.45562, acc 0.49
2016-05-20T16:07:27.144366: train-step 23, loss 3.18543, acc 0.51
2016-05-20T16:07:34.821575: train-step 24, loss 3.1101, acc 0.49
2016-05-20T16:07:42.563162: train-step 25, loss 3.43421, acc 0.51
2016-05-20T16:07:51.076242: train-step 26, loss 2.99893, acc 0.48
2016-05-20T16:08:00.017076: train-step 27, loss 3.09746, acc 0.54
2016-05-20T16:08:09.287009: train-step 28, loss 3.23488, acc 0.48
2016-05-20T16:08:18.548903: train-step 29, loss 2.91608, acc 0.6
2016-05-20T16:08:27.824890: train-step 30, loss 2.93077, acc 0.49
2016-05-20T16:08:36.832619: train-step 31, loss 3.12682, acc 0.46
2016-05-20T16:08:45.970103: train-step 32, loss 2.88232, acc 0.55
2016-05-20T16:08:55.241372: train-step 33, loss 3.00336, acc 0.53
2016-05-20T16:09:04.749165: train-step 34, loss 2.74737, acc 0.66
2016-05-20T16:09:14.110997: train-step 35, loss 2.68119, acc 0.56
2016-05-20T16:09:23.245036: train-step 36, loss 2.93422, acc 0.45
2016-05-20T16:09:32.543818: train-step 37, loss 2.734, acc 0.57
2016-05-20T16:09:41.617634: train-step 38, loss 2.65965, acc 0.57
2016-05-20T16:09:50.284119: train-step 39, loss 2.67317, acc 0.59
2016-05-20T16:09:58.267024: train-step 40, loss 2.9832, acc 0.5
2016-05-20T16:10:07.354743: train-step 41, loss 2.88054, acc 0.46
2016-05-20T16:10:15.067593: train-step 42, loss 2.66431, acc 0.59
2016-05-20T16:10:22.945464: train-step 43, loss 2.84045, acc 0.5
2016-05-20T16:10:31.236770: train-step 44, loss 2.8323, acc 0.5
2016-05-20T16:10:38.983004: train-step 45, loss 2.90172, acc 0.4
2016-05-20T16:10:46.911230: train-step 46, loss 2.78972, acc 0.47
2016-05-20T16:10:55.103032: train-step 47, loss 2.75158, acc 0.49
2016-05-20T16:11:03.697518: train-step 48, loss 2.65591, acc 0.55
2016-05-20T16:11:12.543169: train-step 49, loss 2.6621, acc 0.48
2016-05-20T16:11:21.902023: train-step 50, loss 2.76192, acc 0.48
2016-05-20T16:11:31.245082: train-step 51, loss 2.58069, acc 0.54
2016-05-20T16:11:40.523601: train-step 52, loss 2.36759, acc 0.55
2016-05-20T16:11:50.024554: train-step 53, loss 2.64378, acc 0.49
2016-05-20T16:11:59.390128: train-step 54, loss 2.37437, acc 0.56
2016-05-20T16:12:08.742583: train-step 55, loss 2.54806, acc 0.51
2016-05-20T16:12:18.157236: train-step 56, loss 2.20018, acc 0.64
2016-05-20T16:12:27.580143: train-step 57, loss 2.54157, acc 0.55
2016-05-20T16:12:37.024399: train-step 58, loss 2.63017, acc 0.49
2016-05-20T16:12:46.436439: train-step 59, loss 2.58985, acc 0.48
2016-05-20T16:12:55.607843: train-step 60, loss 2.24984, acc 0.53
2016-05-20T16:13:04.068667: train-step 61, loss 2.2802, acc 0.51
2016-05-20T16:13:13.264818: train-step 62, loss 2.11865, acc 0.58
2016-05-20T16:13:21.431536: train-step 63, loss 2.09634, acc 0.59
2016-05-20T16:13:29.168387: train-step 64, loss 2.3566, acc 0.54
2016-05-20T16:13:36.766675: train-step 65, loss 2.05311, acc 0.62
2016-05-20T16:13:44.545499: train-step 66, loss 2.29646, acc 0.51
2016-05-20T16:13:52.604034: train-step 67, loss 2.2082, acc 0.56
2016-05-20T16:14:00.535495: train-step 68, loss 1.99171, acc 0.58
2016-05-20T16:14:08.490703: train-step 69, loss 2.20584, acc 0.56
2016-05-20T16:14:17.138996: train-step 70, loss 2.21367, acc 0.5
2016-05-20T16:14:26.338966: train-step 71, loss 2.0823, acc 0.58
2016-05-20T16:14:35.536158: train-step 72, loss 2.22119, acc 0.52
2016-05-20T16:14:44.790913: train-step 73, loss 2.26793, acc 0.47
2016-05-20T16:14:53.917317: train-step 74, loss 1.92153, acc 0.54
2016-05-20T16:15:03.292282: train-step 75, loss 2.02344, acc 0.52
2016-05-20T16:15:12.698762: train-step 76, loss 1.88386, acc 0.56
2016-05-20T16:15:22.027846: train-step 77, loss 2.02491, acc 0.51
2016-05-20T16:15:31.335962: train-step 78, loss 2.11297, acc 0.51
2016-05-20T16:15:40.517769: train-step 79, loss 2.16963, acc 0.53
2016-05-20T16:15:49.459844: train-step 80, loss 2.01662, acc 0.54
2016-05-20T16:15:58.536630: train-step 81, loss 2.05364, acc 0.57
2016-05-20T16:16:07.774461: train-step 82, loss 1.81028, acc 0.59
2016-05-20T16:16:16.151962: train-step 83, loss 1.86759, acc 0.63
2016-05-20T16:16:25.686958: train-step 84, loss 2.03114, acc 0.43
2016-05-20T16:16:33.379671: train-step 85, loss 1.64273, acc 0.64
2016-05-20T16:16:41.127424: train-step 86, loss 1.75898, acc 0.59
2016-05-20T16:16:48.847839: train-step 87, loss 1.90386, acc 0.52
2016-05-20T16:16:56.717422: train-step 88, loss 1.85741, acc 0.5
2016-05-20T16:17:04.579312: train-step 89, loss 1.96866, acc 0.5
2016-05-20T16:17:12.337956: train-step 90, loss 1.81881, acc 0.61
2016-05-20T16:17:20.308950: train-step 91, loss 1.77957, acc 0.54
2016-05-20T16:17:28.900029: train-step 92, loss 1.58933, acc 0.67
2016-05-20T16:17:38.157921: train-step 93, loss 1.69686, acc 0.63
2016-05-20T16:17:47.523133: train-step 94, loss 1.70316, acc 0.62
2016-05-20T16:17:56.701799: train-step 95, loss 1.85978, acc 0.54
2016-05-20T16:18:06.011565: train-step 96, loss 1.6537, acc 0.61
2016-05-20T16:18:15.264114: train-step 97, loss 1.81625, acc 0.49
2016-05-20T16:18:24.591224: train-step 98, loss 1.72917, acc 0.56
2016-05-20T16:18:33.835007: train-step 99, loss 1.59993, acc 0.56
2016-05-20T16:18:43.120963: train-step 100, loss 1.60453, acc 0.57
2016-05-20T16:18:52.317615: train-step 101, loss 1.53091, acc 0.55
2016-05-20T16:19:01.369770: train-step 102, loss 1.58881, acc 0.57
2016-05-20T16:19:10.461478: train-step 103, loss 1.80764, acc 0.52
2016-05-20T16:19:19.409956: train-step 104, loss 1.70867, acc 0.52
2016-05-20T16:19:27.715137: train-step 105, loss 1.57314, acc 0.57
2016-05-20T16:19:37.251486: train-step 106, loss 1.62397, acc 0.54
2016-05-20T16:19:45.071206: train-step 107, loss 1.52342, acc 0.55
2016-05-20T16:19:52.894589: train-step 108, loss 1.58839, acc 0.56
2016-05-20T16:20:01.035235: train-step 109, loss 1.58785, acc 0.51
2016-05-20T16:20:08.507799: train-step 110, loss 1.51444, acc 0.59
2016-05-20T16:20:16.109987: train-step 111, loss 1.4919, acc 0.55
2016-05-20T16:20:24.182539: train-step 112, loss 1.49557, acc 0.52
2016-05-20T16:20:32.564089: train-step 113, loss 1.36147, acc 0.68
2016-05-20T16:20:41.708132: train-step 114, loss 1.48403, acc 0.58
2016-05-20T16:20:51.335355: train-step 115, loss 1.62266, acc 0.52
2016-05-20T16:21:00.362936: train-step 116, loss 1.50372, acc 0.59
2016-05-20T16:21:09.862405: train-step 117, loss 1.46561, acc 0.56
2016-05-20T16:21:19.346327: train-step 118, loss 1.49449, acc 0.53
2016-05-20T16:21:28.914706: train-step 119, loss 1.34749, acc 0.64
2016-05-20T16:21:38.182259: train-step 120, loss 1.4953, acc 0.52
2016-05-20T16:21:47.387612: train-step 121, loss 1.40092, acc 0.55
2016-05-20T16:21:56.592169: train-step 122, loss 1.47623, acc 0.58
2016-05-20T16:22:05.671805: train-step 123, loss 1.44479, acc 0.55
2016-05-20T16:22:14.925669: train-step 124, loss 1.24107, acc 0.64
2016-05-20T16:22:23.951807: train-step 125, loss 1.30305, acc 0.63
2016-05-20T16:22:32.428413: train-step 126, loss 1.40451, acc 0.49
2016-05-20T16:22:41.662028: train-step 127, loss 1.41945, acc 0.46
2016-05-20T16:22:49.925474: train-step 128, loss 1.39797, acc 0.55
2016-05-20T16:22:57.703068: train-step 129, loss 1.50063, acc 0.48
2016-05-20T16:23:05.216654: train-step 130, loss 1.42835, acc 0.48
2016-05-20T16:23:13.144386: train-step 131, loss 1.26629, acc 0.61
2016-05-20T16:23:20.926805: train-step 132, loss 1.25692, acc 0.58
2016-05-20T16:23:28.515683: train-step 133, loss 1.39795, acc 0.57
2016-05-20T16:23:36.607720: train-step 134, loss 1.26925, acc 0.6
2016-05-20T16:23:45.325169: train-step 135, loss 1.22702, acc 0.68
2016-05-20T16:23:54.822595: train-step 136, loss 1.18858, acc 0.67
2016-05-20T16:24:04.053958: train-step 137, loss 1.22669, acc 0.55
2016-05-20T16:24:13.375674: train-step 138, loss 1.20665, acc 0.59
2016-05-20T16:24:22.349203: train-step 139, loss 1.16863, acc 0.67
2016-05-20T16:24:31.709756: train-step 140, loss 1.11606, acc 0.66
2016-05-20T16:24:41.012780: train-step 141, loss 1.18403, acc 0.64
2016-05-20T16:24:50.488749: train-step 142, loss 1.32749, acc 0.5
2016-05-20T16:24:59.716541: train-step 143, loss 1.11974, acc 0.61
2016-05-20T16:25:08.618160: train-step 144, loss 1.11485, acc 0.63
2016-05-20T16:25:17.819906: train-step 145, loss 1.15203, acc 0.57
2016-05-20T16:25:26.854753: train-step 146, loss 1.16183, acc 0.61
2016-05-20T16:25:35.723902: train-step 147, loss 1.20289, acc 0.55
2016-05-20T16:25:44.079390: train-step 148, loss 1.13759, acc 0.58
2016-05-20T16:25:53.582714: train-step 149, loss 1.11702, acc 0.59
2016-05-20T16:26:01.267420: train-step 150, loss 1.09545, acc 0.61
2016-05-20T16:26:09.262428: train-step 151, loss 1.08325, acc 0.66
2016-05-20T16:26:16.966172: train-step 152, loss 1.2141, acc 0.55
2016-05-20T16:26:24.896538: train-step 153, loss 1.14415, acc 0.57
2016-05-20T16:26:32.526056: train-step 154, loss 0.999893, acc 0.69
2016-05-20T16:26:40.187543: train-step 155, loss 1.01872, acc 0.66
2016-05-20T16:26:48.333366: train-step 156, loss 1.05615, acc 0.63
2016-05-20T16:26:56.929134: train-step 157, loss 1.10946, acc 0.61
2016-05-20T16:27:05.805730: train-step 158, loss 1.0135, acc 0.68
2016-05-20T16:27:14.862809: train-step 159, loss 1.13194, acc 0.51
2016-05-20T16:27:23.998421: train-step 160, loss 1.11602, acc 0.56
2016-05-20T16:27:33.422465: train-step 161, loss 1.13675, acc 0.54
2016-05-20T16:27:42.700712: train-step 162, loss 1.06516, acc 0.61
2016-05-20T16:27:52.094085: train-step 163, loss 1.0638, acc 0.64
2016-05-20T16:28:01.495457: train-step 164, loss 1.08528, acc 0.52
2016-05-20T16:28:10.658754: train-step 165, loss 1.17114, acc 0.51
2016-05-20T16:28:19.848632: train-step 166, loss 1.06112, acc 0.54
2016-05-20T16:28:29.103611: train-step 167, loss 1.01483, acc 0.64
2016-05-20T16:28:38.018242: train-step 168, loss 1.0371, acc 0.61
2016-05-20T16:28:46.876412: train-step 169, loss 1.10749, acc 0.54
2016-05-20T16:28:55.548530: train-step 170, loss 1.00427, acc 0.55
2016-05-20T16:29:04.594593: train-step 171, loss 1.03812, acc 0.58
2016-05-20T16:29:12.731684: train-step 172, loss 1.09095, acc 0.52
2016-05-20T16:29:20.557907: train-step 173, loss 0.977657, acc 0.6
2016-05-20T16:29:28.393542: train-step 174, loss 1.02802, acc 0.61
2016-05-20T16:29:36.269273: train-step 175, loss 1.04137, acc 0.56
2016-05-20T16:29:43.929545: train-step 176, loss 1.05127, acc 0.55
2016-05-20T16:29:51.850063: train-step 177, loss 0.997872, acc 0.62
2016-05-20T16:30:00.199365: train-step 178, loss 1.04549, acc 0.5
2016-05-20T16:30:08.876894: train-step 179, loss 1.0666, acc 0.53
2016-05-20T16:30:17.915844: train-step 180, loss 0.989843, acc 0.59
2016-05-20T16:30:27.319438: train-step 181, loss 0.978097, acc 0.62
2016-05-20T16:30:36.535352: train-step 182, loss 0.953341, acc 0.62
2016-05-20T16:30:45.646872: train-step 183, loss 1.0305, acc 0.58
2016-05-20T16:30:54.987350: train-step 184, loss 0.959361, acc 0.58
2016-05-20T16:31:04.055177: train-step 185, loss 0.916589, acc 0.68
2016-05-20T16:31:13.173711: train-step 186, loss 0.980975, acc 0.55
2016-05-20T16:31:22.280253: train-step 187, loss 1.00866, acc 0.55
2016-05-20T16:31:31.552291: train-step 188, loss 0.965374, acc 0.59
2016-05-20T16:31:41.129398: train-step 189, loss 1.01904, acc 0.52
2016-05-20T16:31:50.259660: train-step 190, loss 0.998631, acc 0.54
2016-05-20T16:31:59.263795: train-step 191, loss 0.896724, acc 0.65
2016-05-20T16:32:07.437119: train-step 192, loss 0.954431, acc 0.53
2016-05-20T16:32:16.757199: train-step 193, loss 0.919147, acc 0.62
2016-05-20T16:32:24.687171: train-step 194, loss 0.955546, acc 0.58
2016-05-20T16:32:32.441877: train-step 195, loss 0.874321, acc 0.62
2016-05-20T16:32:39.987333: train-step 196, loss 0.888718, acc 0.62
2016-05-20T16:32:48.033809: train-step 197, loss 0.959011, acc 0.54
2016-05-20T16:32:55.821303: train-step 198, loss 0.948406, acc 0.51
2016-05-20T16:33:04.043608: train-step 199, loss 0.882032, acc 0.64
2016-05-20T16:33:12.235766: train-step 200, loss 0.868513, acc 0.61
2016-05-20T16:33:20.801042: train-step 201, loss 0.88152, acc 0.63
2016-05-20T16:33:29.679661: train-step 202, loss 0.880982, acc 0.62
2016-05-20T16:33:38.934401: train-step 203, loss 0.904526, acc 0.56
2016-05-20T16:33:48.157996: train-step 204, loss 0.911173, acc 0.58
2016-05-20T16:33:57.634180: train-step 205, loss 0.814518, acc 0.62
2016-05-20T16:34:06.726143: train-step 206, loss 0.844386, acc 0.62
2016-05-20T16:34:16.127808: train-step 207, loss 0.910843, acc 0.56
2016-05-20T16:34:25.359057: train-step 208, loss 0.872503, acc 0.64
2016-05-20T16:34:34.602998: train-step 209, loss 0.863115, acc 0.62
2016-05-20T16:34:43.815665: train-step 210, loss 0.861022, acc 0.68
2016-05-20T16:34:52.833494: train-step 211, loss 0.838814, acc 0.66
2016-05-20T16:35:02.161050: train-step 212, loss 0.822598, acc 0.62
2016-05-20T16:35:11.149376: train-step 213, loss 0.813182, acc 0.7
2016-05-20T16:35:19.669381: train-step 214, loss 0.899984, acc 0.55
2016-05-20T16:35:28.024303: train-step 215, loss 0.849742, acc 0.61
2016-05-20T16:35:36.590309: train-step 216, loss 0.847899, acc 0.59
2016-05-20T16:35:44.337251: train-step 217, loss 0.843001, acc 0.63
2016-05-20T16:35:52.271457: train-step 218, loss 0.798693, acc 0.66
2016-05-20T16:36:00.733683: train-step 219, loss 0.83573, acc 0.65
2016-05-20T16:36:08.602029: train-step 220, loss 0.765306, acc 0.72
2016-05-20T16:36:16.568784: train-step 221, loss 0.802643, acc 0.68
2016-05-20T16:36:24.468084: train-step 222, loss 0.811234, acc 0.71
2016-05-20T16:36:33.090509: train-step 223, loss 0.839456, acc 0.59
2016-05-20T16:36:42.073409: train-step 224, loss 0.784277, acc 0.68
2016-05-20T16:36:51.139389: train-step 225, loss 0.801525, acc 0.65
2016-05-20T16:37:00.470302: train-step 226, loss 0.792061, acc 0.69
2016-05-20T16:37:09.797407: train-step 227, loss 0.777527, acc 0.7
2016-05-20T16:37:19.046725: train-step 228, loss 0.809569, acc 0.65
2016-05-20T16:37:28.620530: train-step 229, loss 0.816808, acc 0.61
2016-05-20T16:37:37.955957: train-step 230, loss 0.818572, acc 0.66
2016-05-20T16:37:47.278326: train-step 231, loss 0.853133, acc 0.58
2016-05-20T16:37:56.224121: train-step 232, loss 0.782404, acc 0.64
2016-05-20T16:38:05.514671: train-step 233, loss 0.793315, acc 0.66
2016-05-20T16:38:14.708424: train-step 234, loss 0.789465, acc 0.7
2016-05-20T16:38:23.838679: train-step 235, loss 0.770776, acc 0.65
2016-05-20T16:38:32.180343: train-step 236, loss 0.824248, acc 0.59
2016-05-20T16:38:41.277276: train-step 237, loss 0.79228, acc 0.68
2016-05-20T16:38:48.985628: train-step 238, loss 0.779481, acc 0.66
2016-05-20T16:38:56.981959: train-step 239, loss 0.800369, acc 0.66
2016-05-20T16:39:05.229592: train-step 240, loss 0.78913, acc 0.65
2016-05-20T16:39:13.001094: train-step 241, loss 0.79628, acc 0.56
2016-05-20T16:39:20.795879: train-step 242, loss 0.767621, acc 0.61
2016-05-20T16:39:28.538459: train-step 243, loss 0.793171, acc 0.64
2016-05-20T16:39:37.088932: train-step 244, loss 0.842648, acc 0.54
2016-05-20T16:39:46.310663: train-step 245, loss 0.803325, acc 0.59
2016-05-20T16:39:55.643789: train-step 246, loss 0.808062, acc 0.63
2016-05-20T16:40:04.894468: train-step 247, loss 0.765873, acc 0.65
2016-05-20T16:40:14.233169: train-step 248, loss 0.794906, acc 0.6
2016-05-20T16:40:23.350951: train-step 249, loss 0.784716, acc 0.65
2016-05-20T16:40:32.779968: train-step 250, loss 0.781797, acc 0.62
epoch number is: 1
2016-05-20T16:40:42.209247: train-step 251, loss 0.751007, acc 0.62
2016-05-20T16:40:51.484280: train-step 252, loss 0.719322, acc 0.71
2016-05-20T16:41:00.631938: train-step 253, loss 0.742303, acc 0.66
2016-05-20T16:41:09.861768: train-step 254, loss 0.749874, acc 0.64
2016-05-20T16:41:18.964441: train-step 255, loss 0.733648, acc 0.7
2016-05-20T16:41:27.918676: train-step 256, loss 0.790605, acc 0.61
2016-05-20T16:41:36.596709: train-step 257, loss 0.699506, acc 0.76
2016-05-20T16:41:44.663663: train-step 258, loss 0.671557, acc 0.75
2016-05-20T16:41:53.647410: train-step 259, loss 0.787364, acc 0.6
2016-05-20T16:42:01.365485: train-step 260, loss 0.752859, acc 0.64
2016-05-20T16:42:09.033742: train-step 261, loss 0.750936, acc 0.6
2016-05-20T16:42:17.220749: train-step 262, loss 0.732368, acc 0.63
2016-05-20T16:42:24.976492: train-step 263, loss 0.696565, acc 0.75
2016-05-20T16:42:32.903349: train-step 264, loss 0.74094, acc 0.72
2016-05-20T16:42:40.735729: train-step 265, loss 0.728593, acc 0.67
2016-05-20T16:42:49.180915: train-step 266, loss 0.673936, acc 0.73
2016-05-20T16:42:57.968630: train-step 267, loss 0.7265, acc 0.67
2016-05-20T16:43:07.070602: train-step 268, loss 0.733282, acc 0.62
2016-05-20T16:43:16.290690: train-step 269, loss 0.767079, acc 0.61
2016-05-20T16:43:25.722731: train-step 270, loss 0.690599, acc 0.67
2016-05-20T16:43:35.390698: train-step 271, loss 0.668729, acc 0.76
2016-05-20T16:43:44.727217: train-step 272, loss 0.657249, acc 0.74
2016-05-20T16:43:53.769273: train-step 273, loss 0.752615, acc 0.56
2016-05-20T16:44:02.837215: train-step 274, loss 0.750008, acc 0.62
2016-05-20T16:44:11.982168: train-step 275, loss 0.686383, acc 0.74
2016-05-20T16:44:21.449120: train-step 276, loss 0.724743, acc 0.62
2016-05-20T16:44:30.618351: train-step 277, loss 0.715643, acc 0.66
2016-05-20T16:44:39.472411: train-step 278, loss 0.701879, acc 0.66
2016-05-20T16:44:48.305961: train-step 279, loss 0.715664, acc 0.67
2016-05-20T16:44:56.534020: train-step 280, loss 0.667903, acc 0.7
2016-05-20T16:45:05.814849: train-step 281, loss 0.679739, acc 0.67
2016-05-20T16:45:13.460180: train-step 282, loss 0.721782, acc 0.71
2016-05-20T16:45:21.286870: train-step 283, loss 0.706194, acc 0.7
2016-05-20T16:45:29.297923: train-step 284, loss 0.708835, acc 0.68
2016-05-20T16:45:36.996704: train-step 285, loss 0.668352, acc 0.71
2016-05-20T16:45:44.747126: train-step 286, loss 0.661907, acc 0.73
2016-05-20T16:45:52.733929: train-step 287, loss 0.668454, acc 0.67
2016-05-20T16:46:00.974628: train-step 288, loss 0.71626, acc 0.62
2016-05-20T16:46:09.682920: train-step 289, loss 0.687969, acc 0.68
2016-05-20T16:46:18.536154: train-step 290, loss 0.76288, acc 0.63
2016-05-20T16:46:27.821804: train-step 291, loss 0.727895, acc 0.57
2016-05-20T16:46:36.684007: train-step 292, loss 0.654968, acc 0.71
2016-05-20T16:46:45.965407: train-step 293, loss 0.711037, acc 0.7
2016-05-20T16:46:55.036265: train-step 294, loss 0.699021, acc 0.69
2016-05-20T16:47:04.364765: train-step 295, loss 0.682556, acc 0.62
2016-05-20T16:47:13.559553: train-step 296, loss 0.719859, acc 0.62
2016-05-20T16:47:22.911265: train-step 297, loss 0.694943, acc 0.66
2016-05-20T16:47:32.036383: train-step 298, loss 0.664998, acc 0.72
2016-05-20T16:47:41.366143: train-step 299, loss 0.70157, acc 0.65
2016-05-20T16:47:50.446204: train-step 300, loss 0.657122, acc 0.71
2016-05-20T16:47:59.327976: train-step 301, loss 0.675996, acc 0.65
2016-05-20T16:48:07.676281: train-step 302, loss 0.659126, acc 0.71
2016-05-20T16:48:15.586301: train-step 303, loss 0.700358, acc 0.69
2016-05-20T16:48:24.542432: train-step 304, loss 0.667445, acc 0.74
2016-05-20T16:48:32.084154: train-step 305, loss 0.649519, acc 0.78
2016-05-20T16:48:40.409865: train-step 306, loss 0.655379, acc 0.7
2016-05-20T16:48:48.409374: train-step 307, loss 0.665425, acc 0.64
2016-05-20T16:48:56.469628: train-step 308, loss 0.719667, acc 0.65
2016-05-20T16:49:04.600440: train-step 309, loss 0.682637, acc 0.64
2016-05-20T16:49:12.364854: train-step 310, loss 0.658391, acc 0.67
2016-05-20T16:49:20.964646: train-step 311, loss 0.671399, acc 0.68
2016-05-20T16:49:30.069000: train-step 312, loss 0.635121, acc 0.74
2016-05-20T16:49:38.978614: train-step 313, loss 0.670369, acc 0.69
2016-05-20T16:49:48.289512: train-step 314, loss 0.70399, acc 0.67
2016-05-20T16:49:57.525013: train-step 315, loss 0.617976, acc 0.76
2016-05-20T16:50:06.708506: train-step 316, loss 0.650194, acc 0.71
2016-05-20T16:50:15.819680: train-step 317, loss 0.72827, acc 0.62
2016-05-20T16:50:24.938164: train-step 318, loss 0.665302, acc 0.65
2016-05-20T16:50:34.163643: train-step 319, loss 0.673756, acc 0.68
2016-05-20T16:50:43.316471: train-step 320, loss 0.623346, acc 0.73
2016-05-20T16:50:52.628958: train-step 321, loss 0.629645, acc 0.73
2016-05-20T16:51:01.998635: train-step 322, loss 0.645134, acc 0.74
2016-05-20T16:51:11.030187: train-step 323, loss 0.695165, acc 0.66
2016-05-20T16:51:19.887263: train-step 324, loss 0.65568, acc 0.7
2016-05-20T16:51:27.935954: train-step 325, loss 0.637497, acc 0.68
2016-05-20T16:51:37.513305: train-step 326, loss 0.621643, acc 0.74
2016-05-20T16:51:44.973698: train-step 327, loss 0.609168, acc 0.74
2016-05-20T16:51:52.939443: train-step 328, loss 0.677008, acc 0.68
2016-05-20T16:52:00.962659: train-step 329, loss 0.586757, acc 0.78
2016-05-20T16:52:08.890054: train-step 330, loss 0.632338, acc 0.74
2016-05-20T16:52:16.484593: train-step 331, loss 0.659848, acc 0.71
2016-05-20T16:52:24.426369: train-step 332, loss 0.658866, acc 0.68
2016-05-20T16:52:32.868422: train-step 333, loss 0.614622, acc 0.76
2016-05-20T16:52:41.471806: train-step 334, loss 0.614628, acc 0.71
2016-05-20T16:52:50.661022: train-step 335, loss 0.63168, acc 0.72
2016-05-20T16:53:00.105870: train-step 336, loss 0.640775, acc 0.68
2016-05-20T16:53:09.255701: train-step 337, loss 0.621047, acc 0.71
2016-05-20T16:53:18.595055: train-step 338, loss 0.599916, acc 0.76
2016-05-20T16:53:27.801127: train-step 339, loss 0.612639, acc 0.72
2016-05-20T16:53:36.701088: train-step 340, loss 0.602722, acc 0.74
2016-05-20T16:53:45.831809: train-step 341, loss 0.566295, acc 0.83
2016-05-20T16:53:54.946715: train-step 342, loss 0.598208, acc 0.75
2016-05-20T16:54:04.235435: train-step 343, loss 0.643819, acc 0.68
2016-05-20T16:54:13.756723: train-step 344, loss 0.63338, acc 0.73
2016-05-20T16:54:23.074275: train-step 345, loss 0.597304, acc 0.76
2016-05-20T16:54:32.211494: train-step 346, loss 0.662534, acc 0.69
2016-05-20T16:54:40.516144: train-step 347, loss 0.6101, acc 0.7
2016-05-20T16:54:49.927855: train-step 348, loss 0.587445, acc 0.75
2016-05-20T16:54:57.659324: train-step 349, loss 0.632873, acc 0.74
2016-05-20T16:55:05.656432: train-step 350, loss 0.679963, acc 0.66
2016-05-20T16:55:13.814920: train-step 351, loss 0.634362, acc 0.74
2016-05-20T16:55:21.556375: train-step 352, loss 0.597309, acc 0.77
2016-05-20T16:55:29.418138: train-step 353, loss 0.607558, acc 0.78
2016-05-20T16:55:37.319706: train-step 354, loss 0.599137, acc 0.73
2016-05-20T16:55:45.792230: train-step 355, loss 0.608202, acc 0.73
2016-05-20T16:55:54.577087: train-step 356, loss 0.580199, acc 0.76
2016-05-20T16:56:03.760851: train-step 357, loss 0.557753, acc 0.8
2016-05-20T16:56:12.822464: train-step 358, loss 0.643855, acc 0.66
2016-05-20T16:56:22.117333: train-step 359, loss 0.579741, acc 0.71
2016-05-20T16:56:31.667441: train-step 360, loss 0.575316, acc 0.78
2016-05-20T16:56:40.717371: train-step 361, loss 0.65551, acc 0.67
2016-05-20T16:56:49.909040: train-step 362, loss 0.596717, acc 0.75
2016-05-20T16:56:58.943357: train-step 363, loss 0.551828, acc 0.78
2016-05-20T16:57:08.248786: train-step 364, loss 0.641437, acc 0.63
2016-05-20T16:57:17.333673: train-step 365, loss 0.61186, acc 0.69
2016-05-20T16:57:26.613370: train-step 366, loss 0.592051, acc 0.73
2016-05-20T16:57:35.451930: train-step 367, loss 0.640084, acc 0.71
2016-05-20T16:57:44.361388: train-step 368, loss 0.603072, acc 0.68
2016-05-20T16:57:52.960450: train-step 369, loss 0.609071, acc 0.71
2016-05-20T16:58:02.387080: train-step 370, loss 0.572831, acc 0.78
2016-05-20T16:58:09.948182: train-step 371, loss 0.564683, acc 0.8
2016-05-20T16:58:17.777081: train-step 372, loss 0.550114, acc 0.83
2016-05-20T16:58:25.760567: train-step 373, loss 0.630274, acc 0.71
2016-05-20T16:58:33.813597: train-step 374, loss 0.610982, acc 0.68
2016-05-20T16:58:41.532919: train-step 375, loss 0.639783, acc 0.71
2016-05-20T16:58:49.799815: train-step 376, loss 0.561131, acc 0.75
2016-05-20T16:58:58.361268: train-step 377, loss 0.539851, acc 0.78
2016-05-20T16:59:07.196264: train-step 378, loss 0.585752, acc 0.78
2016-05-20T16:59:16.519765: train-step 379, loss 0.625716, acc 0.72
2016-05-20T16:59:25.904200: train-step 380, loss 0.611129, acc 0.73
2016-05-20T16:59:35.374577: train-step 381, loss 0.556487, acc 0.76
2016-05-20T16:59:44.345165: train-step 382, loss 0.619496, acc 0.69
2016-05-20T16:59:53.806970: train-step 383, loss 0.61562, acc 0.71
2016-05-20T17:00:03.211550: train-step 384, loss 0.599678, acc 0.72
2016-05-20T17:00:12.501414: train-step 385, loss 0.562296, acc 0.76
2016-05-20T17:00:22.034852: train-step 386, loss 0.565874, acc 0.73
2016-05-20T17:00:31.076129: train-step 387, loss 0.578209, acc 0.74
2016-05-20T17:00:40.409044: train-step 388, loss 0.577939, acc 0.75
2016-05-20T17:00:49.982904: train-step 389, loss 0.604935, acc 0.76
2016-05-20T17:00:58.650397: train-step 390, loss 0.582209, acc 0.75
2016-05-20T17:01:06.488387: train-step 391, loss 0.526235, acc 0.81
2016-05-20T17:01:15.965281: train-step 392, loss 0.592966, acc 0.72
2016-05-20T17:01:23.501243: train-step 393, loss 0.57184, acc 0.76
2016-05-20T17:01:31.139505: train-step 394, loss 0.591286, acc 0.75
2016-05-20T17:01:38.888254: train-step 395, loss 0.591679, acc 0.74
2016-05-20T17:01:46.601203: train-step 396, loss 0.61138, acc 0.74
2016-05-20T17:01:54.427134: train-step 397, loss 0.553576, acc 0.77
2016-05-20T17:02:02.530303: train-step 398, loss 0.565584, acc 0.76
2016-05-20T17:02:11.081845: train-step 399, loss 0.590312, acc 0.75
2016-05-20T17:02:20.143815: train-step 400, loss 0.593385, acc 0.74
2016-05-20T17:02:28.999945: train-step 401, loss 0.55808, acc 0.76
2016-05-20T17:02:38.198123: train-step 402, loss 0.537864, acc 0.82
2016-05-20T17:02:47.554175: train-step 403, loss 0.55549, acc 0.79
2016-05-20T17:02:56.761710: train-step 404, loss 0.601321, acc 0.7
2016-05-20T17:03:06.192331: train-step 405, loss 0.530345, acc 0.81
2016-05-20T17:03:15.694923: train-step 406, loss 0.607452, acc 0.78
2016-05-20T17:03:25.059794: train-step 407, loss 0.558221, acc 0.8
2016-05-20T17:03:34.481486: train-step 408, loss 0.535591, acc 0.83
2016-05-20T17:03:43.528922: train-step 409, loss 0.567832, acc 0.73
2016-05-20T17:03:52.552722: train-step 410, loss 0.593592, acc 0.77
2016-05-20T17:04:01.534388: train-step 411, loss 0.571521, acc 0.78
2016-05-20T17:04:09.972991: train-step 412, loss 0.503199, acc 0.8
2016-05-20T17:04:17.890357: train-step 413, loss 0.533295, acc 0.81
2016-05-20T17:04:26.972179: train-step 414, loss 0.540252, acc 0.8
2016-05-20T17:04:34.551604: train-step 415, loss 0.565309, acc 0.74
2016-05-20T17:04:42.339406: train-step 416, loss 0.561628, acc 0.78
2016-05-20T17:04:50.606115: train-step 417, loss 0.573261, acc 0.78
2016-05-20T17:04:58.215276: train-step 418, loss 0.532587, acc 0.8
2016-05-20T17:05:06.311850: train-step 419, loss 0.486478, acc 0.85
2016-05-20T17:05:14.469737: train-step 420, loss 0.595794, acc 0.79
2016-05-20T17:05:22.809706: train-step 421, loss 0.5294, acc 0.78
2016-05-20T17:05:31.614826: train-step 422, loss 0.535387, acc 0.79
2016-05-20T17:05:40.792350: train-step 423, loss 0.528843, acc 0.79
2016-05-20T17:05:50.016938: train-step 424, loss 0.55164, acc 0.77
2016-05-20T17:05:59.109214: train-step 425, loss 0.575378, acc 0.75
2016-05-20T17:06:08.545020: train-step 426, loss 0.604972, acc 0.71
2016-05-20T17:06:17.833518: train-step 427, loss 0.500419, acc 0.8
2016-05-20T17:06:26.870084: train-step 428, loss 0.538638, acc 0.77
2016-05-20T17:06:36.045963: train-step 429, loss 0.538637, acc 0.81
2016-05-20T17:06:45.131951: train-step 430, loss 0.523239, acc 0.79
2016-05-20T17:06:54.038634: train-step 431, loss 0.541745, acc 0.81
2016-05-20T17:07:03.181080: train-step 432, loss 0.555829, acc 0.8
2016-05-20T17:07:12.338252: train-step 433, loss 0.525407, acc 0.81
2016-05-20T17:07:20.947684: train-step 434, loss 0.534237, acc 0.77
2016-05-20T17:07:29.064405: train-step 435, loss 0.554499, acc 0.76
2016-05-20T17:07:38.414528: train-step 436, loss 0.545041, acc 0.75
2016-05-20T17:07:45.929722: train-step 437, loss 0.62717, acc 0.7
2016-05-20T17:07:53.534528: train-step 438, loss 0.532011, acc 0.79
2016-05-20T17:08:01.224931: train-step 439, loss 0.534743, acc 0.75
2016-05-20T17:08:09.165375: train-step 440, loss 0.573708, acc 0.72
2016-05-20T17:08:16.909957: train-step 441, loss 0.643627, acc 0.69
2016-05-20T17:08:25.187032: train-step 442, loss 0.584716, acc 0.74
2016-05-20T17:08:33.380597: train-step 443, loss 0.540976, acc 0.76
2016-05-20T17:08:42.460046: train-step 444, loss 0.459377, acc 0.9
2016-05-20T17:08:51.818546: train-step 445, loss 0.569991, acc 0.74
2016-05-20T17:09:01.512433: train-step 446, loss 0.561192, acc 0.78
2016-05-20T17:09:10.702849: train-step 447, loss 0.557772, acc 0.77
2016-05-20T17:09:19.742437: train-step 448, loss 0.527268, acc 0.79
2016-05-20T17:09:29.022884: train-step 449, loss 0.541652, acc 0.81
2016-05-20T17:09:38.661429: train-step 450, loss 0.586526, acc 0.72
2016-05-20T17:09:48.000517: train-step 451, loss 0.52402, acc 0.79
2016-05-20T17:09:57.484874: train-step 452, loss 0.552107, acc 0.75
2016-05-20T17:10:06.810902: train-step 453, loss 0.530986, acc 0.75
2016-05-20T17:10:15.879680: train-step 454, loss 0.519366, acc 0.79
2016-05-20T17:10:24.778454: train-step 455, loss 0.549741, acc 0.73
2016-05-20T17:10:33.660232: train-step 456, loss 0.531, acc 0.78
2016-05-20T17:10:41.892452: train-step 457, loss 0.50349, acc 0.8
2016-05-20T17:10:51.369458: train-step 458, loss 0.525328, acc 0.79
2016-05-20T17:10:58.903858: train-step 459, loss 0.549775, acc 0.77
2016-05-20T17:11:06.602661: train-step 460, loss 0.564084, acc 0.73
2016-05-20T17:11:14.311676: train-step 461, loss 0.588926, acc 0.78
2016-05-20T17:11:22.099485: train-step 462, loss 0.567452, acc 0.77
2016-05-20T17:11:29.865342: train-step 463, loss 0.524861, acc 0.78
2016-05-20T17:11:37.899518: train-step 464, loss 0.608407, acc 0.73
2016-05-20T17:11:46.472564: train-step 465, loss 0.54612, acc 0.77
2016-05-20T17:11:55.122634: train-step 466, loss 0.598928, acc 0.73
2016-05-20T17:12:04.204199: train-step 467, loss 0.61996, acc 0.67
2016-05-20T17:12:13.449049: train-step 468, loss 0.504393, acc 0.82
2016-05-20T17:12:22.781494: train-step 469, loss 0.534895, acc 0.79
2016-05-20T17:12:31.857600: train-step 470, loss 0.562018, acc 0.8
2016-05-20T17:12:41.120942: train-step 471, loss 0.48427, acc 0.81
2016-05-20T17:12:50.300443: train-step 472, loss 0.565145, acc 0.72
2016-05-20T17:12:59.528885: train-step 473, loss 0.549087, acc 0.76
2016-05-20T17:13:08.655750: train-step 474, loss 0.476813, acc 0.83
2016-05-20T17:13:17.668741: train-step 475, loss 0.505917, acc 0.84
2016-05-20T17:13:26.686533: train-step 476, loss 0.537789, acc 0.74
2016-05-20T17:13:36.017765: train-step 477, loss 0.529541, acc 0.8
2016-05-20T17:13:44.880209: train-step 478, loss 0.494633, acc 0.82
2016-05-20T17:13:53.109087: train-step 479, loss 0.514523, acc 0.81
2016-05-20T17:14:02.464868: train-step 480, loss 0.511408, acc 0.8
2016-05-20T17:14:10.004132: train-step 481, loss 0.520791, acc 0.75
2016-05-20T17:14:17.949447: train-step 482, loss 0.535696, acc 0.79
2016-05-20T17:14:25.972250: train-step 483, loss 0.49786, acc 0.81
2016-05-20T17:14:33.908143: train-step 484, loss 0.537485, acc 0.8
2016-05-20T17:14:42.128737: train-step 485, loss 0.532788, acc 0.75
2016-05-20T17:14:50.097575: train-step 486, loss 0.564518, acc 0.77
2016-05-20T17:14:58.594494: train-step 487, loss 0.542799, acc 0.76
2016-05-20T17:15:07.613034: train-step 488, loss 0.503225, acc 0.85
2016-05-20T17:15:16.745291: train-step 489, loss 0.504762, acc 0.8
2016-05-20T17:15:26.043711: train-step 490, loss 0.535557, acc 0.74
2016-05-20T17:15:35.344549: train-step 491, loss 0.638749, acc 0.72
2016-05-20T17:15:44.594838: train-step 492, loss 0.513527, acc 0.78
2016-05-20T17:15:54.043001: train-step 493, loss 0.468834, acc 0.87
2016-05-20T17:16:03.302096: train-step 494, loss 0.522203, acc 0.8
2016-05-20T17:16:12.483630: train-step 495, loss 0.550734, acc 0.76
2016-05-20T17:16:21.506089: train-step 496, loss 0.500915, acc 0.85
2016-05-20T17:16:30.421315: train-step 497, loss 0.542213, acc 0.77
2016-05-20T17:16:39.398002: train-step 498, loss 0.478195, acc 0.86
2016-05-20T17:16:48.393162: train-step 499, loss 0.50716, acc 0.81
2016-05-20T17:16:56.940888: train-step 500, loss 0.497587, acc 0.85
epoch number is: 2
2016-05-20T17:17:05.240824: train-step 501, loss 0.561808, acc 0.74
2016-05-20T17:17:14.474434: train-step 502, loss 0.49426, acc 0.82
2016-05-20T17:17:22.090394: train-step 503, loss 0.539969, acc 0.77
2016-05-20T17:17:29.944788: train-step 504, loss 0.568297, acc 0.75
2016-05-20T17:17:37.466688: train-step 505, loss 0.480813, acc 0.81
2016-05-20T17:17:45.152581: train-step 506, loss 0.456779, acc 0.84
2016-05-20T17:17:53.024373: train-step 507, loss 0.478154, acc 0.8
2016-05-20T17:18:00.941204: train-step 508, loss 0.530634, acc 0.74
2016-05-20T17:18:09.112653: train-step 509, loss 0.49711, acc 0.79
2016-05-20T17:18:17.720131: train-step 510, loss 0.458651, acc 0.86
2016-05-20T17:18:26.611841: train-step 511, loss 0.476988, acc 0.85
2016-05-20T17:18:35.675921: train-step 512, loss 0.483833, acc 0.79
2016-05-20T17:18:44.964319: train-step 513, loss 0.524109, acc 0.82
2016-05-20T17:18:54.389991: train-step 514, loss 0.486524, acc 0.82
2016-05-20T17:19:03.745500: train-step 515, loss 0.594958, acc 0.74
2016-05-20T17:19:12.742704: train-step 516, loss 0.609018, acc 0.7
2016-05-20T17:19:21.786200: train-step 517, loss 0.517649, acc 0.8
2016-05-20T17:19:30.846569: train-step 518, loss 0.501844, acc 0.79
2016-05-20T17:19:40.017796: train-step 519, loss 0.538762, acc 0.8
2016-05-20T17:19:48.951983: train-step 520, loss 0.480379, acc 0.8
2016-05-20T17:19:58.265865: train-step 521, loss 0.509319, acc 0.84
2016-05-20T17:20:07.541010: train-step 522, loss 0.55848, acc 0.75
2016-05-20T17:20:15.975238: train-step 523, loss 0.482633, acc 0.79
2016-05-20T17:20:24.695696: train-step 524, loss 0.569588, acc 0.77
2016-05-20T17:20:33.229485: train-step 525, loss 0.461456, acc 0.83
2016-05-20T17:20:40.985761: train-step 526, loss 0.583289, acc 0.73
2016-05-20T17:20:48.483045: train-step 527, loss 0.538699, acc 0.75
2016-05-20T17:20:56.108782: train-step 528, loss 0.49892, acc 0.79
2016-05-20T17:21:03.698491: train-step 529, loss 0.490695, acc 0.84
2016-05-20T17:21:11.272623: train-step 530, loss 0.48413, acc 0.81
2016-05-20T17:21:18.918839: train-step 531, loss 0.515317, acc 0.8
2016-05-20T17:21:27.425509: train-step 532, loss 0.605066, acc 0.72
2016-05-20T17:21:36.299383: train-step 533, loss 0.499228, acc 0.8
2016-05-20T17:21:45.205976: train-step 534, loss 0.53221, acc 0.81
2016-05-20T17:21:54.397760: train-step 535, loss 0.451833, acc 0.84
2016-05-20T17:22:03.484287: train-step 536, loss 0.550764, acc 0.75
2016-05-20T17:22:12.790322: train-step 537, loss 0.477907, acc 0.81
2016-05-20T17:22:22.152432: train-step 538, loss 0.432815, acc 0.84
2016-05-20T17:22:31.255149: train-step 539, loss 0.542769, acc 0.81
2016-05-20T17:22:40.721385: train-step 540, loss 0.486872, acc 0.83
2016-05-20T17:22:49.934102: train-step 541, loss 0.504248, acc 0.79
2016-05-20T17:22:58.951436: train-step 542, loss 0.485249, acc 0.82
2016-05-20T17:23:07.867495: train-step 543, loss 0.455463, acc 0.82
2016-05-20T17:23:17.054385: train-step 544, loss 0.508497, acc 0.84
2016-05-20T17:23:25.885902: train-step 545, loss 0.500007, acc 0.79
2016-05-20T17:23:34.200450: train-step 546, loss 0.520347, acc 0.79
2016-05-20T17:23:43.489416: train-step 547, loss 0.500766, acc 0.8
2016-05-20T17:23:51.108892: train-step 548, loss 0.524012, acc 0.77
2016-05-20T17:23:59.364018: train-step 549, loss 0.466653, acc 0.82
2016-05-20T17:24:07.539446: train-step 550, loss 0.460052, acc 0.83
2016-05-20T17:24:15.096240: train-step 551, loss 0.466031, acc 0.84
2016-05-20T17:24:22.851081: train-step 552, loss 0.542447, acc 0.76
2016-05-20T17:24:30.632701: train-step 553, loss 0.459933, acc 0.84
2016-05-20T17:24:38.848265: train-step 554, loss 0.446349, acc 0.85
2016-05-20T17:24:47.779691: train-step 555, loss 0.519747, acc 0.78
2016-05-20T17:24:56.998483: train-step 556, loss 0.478815, acc 0.82
2016-05-20T17:25:06.108216: train-step 557, loss 0.493527, acc 0.8
2016-05-20T17:25:15.310253: train-step 558, loss 0.410208, acc 0.86
2016-05-20T17:25:24.831457: train-step 559, loss 0.481578, acc 0.84
2016-05-20T17:25:34.158599: train-step 560, loss 0.441281, acc 0.87
2016-05-20T17:25:43.444749: train-step 561, loss 0.502594, acc 0.81
2016-05-20T17:25:52.663023: train-step 562, loss 0.480771, acc 0.78
2016-05-20T17:26:01.752780: train-step 563, loss 0.499861, acc 0.82
2016-05-20T17:26:11.009933: train-step 564, loss 0.526243, acc 0.78
2016-05-20T17:26:19.589224: train-step 565, loss 0.490738, acc 0.79
2016-05-20T17:26:28.778385: train-step 566, loss 0.484778, acc 0.8
2016-05-20T17:26:37.436854: train-step 567, loss 0.489772, acc 0.83
2016-05-20T17:26:45.743763: train-step 568, loss 0.425603, acc 0.82
2016-05-20T17:26:54.946857: train-step 569, loss 0.467105, acc 0.84
2016-05-20T17:27:02.629293: train-step 570, loss 0.441445, acc 0.84
2016-05-20T17:27:10.512303: train-step 571, loss 0.44486, acc 0.84
2016-05-20T17:27:18.204378: train-step 572, loss 0.464846, acc 0.87
2016-05-20T17:27:26.186654: train-step 573, loss 0.470328, acc 0.8
2016-05-20T17:27:33.998230: train-step 574, loss 0.518315, acc 0.79
2016-05-20T17:27:41.765912: train-step 575, loss 0.507298, acc 0.77
2016-05-20T17:27:50.076490: train-step 576, loss 0.43632, acc 0.85
2016-05-20T17:27:58.943476: train-step 577, loss 0.450684, acc 0.83
2016-05-20T17:28:08.264980: train-step 578, loss 0.413166, acc 0.86
2016-05-20T17:28:17.581128: train-step 579, loss 0.428137, acc 0.85
2016-05-20T17:28:26.984654: train-step 580, loss 0.47568, acc 0.85
2016-05-20T17:28:36.452046: train-step 581, loss 0.506371, acc 0.83
2016-05-20T17:28:45.771394: train-step 582, loss 0.526552, acc 0.75
2016-05-20T17:28:54.808975: train-step 583, loss 0.468322, acc 0.8
2016-05-20T17:29:03.963211: train-step 584, loss 0.451075, acc 0.85
2016-05-20T17:29:13.402424: train-step 585, loss 0.492727, acc 0.84
2016-05-20T17:29:22.701474: train-step 586, loss 0.448132, acc 0.8
2016-05-20T17:29:31.994147: train-step 587, loss 0.518106, acc 0.78
2016-05-20T17:29:41.129518: train-step 588, loss 0.453413, acc 0.81
2016-05-20T17:29:49.922468: train-step 589, loss 0.520663, acc 0.75
2016-05-20T17:29:58.416441: train-step 590, loss 0.475343, acc 0.78
2016-05-20T17:30:07.976769: train-step 591, loss 0.43317, acc 0.86
2016-05-20T17:30:15.625048: train-step 592, loss 0.480468, acc 0.8
2016-05-20T17:30:23.549589: train-step 593, loss 0.487276, acc 0.78
2016-05-20T17:30:31.488665: train-step 594, loss 0.4466, acc 0.81
2016-05-20T17:30:39.352023: train-step 595, loss 0.445093, acc 0.85
2016-05-20T17:30:47.065806: train-step 596, loss 0.452595, acc 0.85
2016-05-20T17:30:55.135355: train-step 597, loss 0.488035, acc 0.81
2016-05-20T17:31:03.565089: train-step 598, loss 0.494034, acc 0.8
2016-05-20T17:31:12.585065: train-step 599, loss 0.573702, acc 0.77
2016-05-20T17:31:21.471385: train-step 600, loss 0.454704, acc 0.85
2016-05-20T17:31:30.828647: train-step 601, loss 0.466323, acc 0.83
2016-05-20T17:31:40.013836: train-step 602, loss 0.494604, acc 0.79
2016-05-20T17:31:49.358595: train-step 603, loss 0.449482, acc 0.86
2016-05-20T17:31:58.700741: train-step 604, loss 0.45277, acc 0.81
2016-05-20T17:32:08.054256: train-step 605, loss 0.503215, acc 0.75
2016-05-20T17:32:17.598535: train-step 606, loss 0.463534, acc 0.82
2016-05-20T17:32:26.737866: train-step 607, loss 0.465399, acc 0.82
2016-05-20T17:32:35.556650: train-step 608, loss 0.443325, acc 0.82
2016-05-20T17:32:44.898581: train-step 609, loss 0.450049, acc 0.85
2016-05-20T17:32:53.880498: train-step 610, loss 0.477726, acc 0.79
2016-05-20T17:33:02.823272: train-step 611, loss 0.418315, acc 0.87
2016-05-20T17:33:10.900331: train-step 612, loss 0.528453, acc 0.79
2016-05-20T17:33:19.947974: train-step 613, loss 0.535067, acc 0.75
2016-05-20T17:33:27.578015: train-step 614, loss 0.577645, acc 0.72
2016-05-20T17:33:35.873020: train-step 615, loss 0.382884, acc 0.86
2016-05-20T17:33:43.771590: train-step 616, loss 0.484668, acc 0.78
2016-05-20T17:33:51.501172: train-step 617, loss 0.533044, acc 0.78
2016-05-20T17:33:59.248179: train-step 618, loss 0.465039, acc 0.78
2016-05-20T17:34:07.249442: train-step 619, loss 0.421851, acc 0.82
2016-05-20T17:34:15.806177: train-step 620, loss 0.453513, acc 0.84
2016-05-20T17:34:24.843046: train-step 621, loss 0.515676, acc 0.77
2016-05-20T17:34:33.908348: train-step 622, loss 0.478956, acc 0.77
2016-05-20T17:34:43.243486: train-step 623, loss 0.475269, acc 0.77
2016-05-20T17:34:52.520239: train-step 624, loss 0.537332, acc 0.75
2016-05-20T17:35:01.652691: train-step 625, loss 0.526589, acc 0.81
2016-05-20T17:35:11.071747: train-step 626, loss 0.451885, acc 0.8
2016-05-20T17:35:20.287372: train-step 627, loss 0.457773, acc 0.88
2016-05-20T17:35:29.567380: train-step 628, loss 0.465129, acc 0.83
2016-05-20T17:35:38.550742: train-step 629, loss 0.507212, acc 0.81
2016-05-20T17:35:47.810162: train-step 630, loss 0.482774, acc 0.83
2016-05-20T17:35:57.023477: train-step 631, loss 0.428435, acc 0.82
2016-05-20T17:36:06.189017: train-step 632, loss 0.448486, acc 0.86
2016-05-20T17:36:14.814931: train-step 633, loss 0.434593, acc 0.91
2016-05-20T17:36:22.717188: train-step 634, loss 0.459202, acc 0.82
2016-05-20T17:36:31.900947: train-step 635, loss 0.530423, acc 0.79
2016-05-20T17:36:39.516933: train-step 636, loss 0.561153, acc 0.78
2016-05-20T17:36:47.346833: train-step 637, loss 0.469526, acc 0.83
2016-05-20T17:36:55.005434: train-step 638, loss 0.453852, acc 0.82
2016-05-20T17:37:02.565944: train-step 639, loss 0.423784, acc 0.84
2016-05-20T17:37:10.466068: train-step 640, loss 0.558617, acc 0.78
2016-05-20T17:37:18.704719: train-step 641, loss 0.4548, acc 0.81
2016-05-20T17:37:27.595981: train-step 642, loss 0.463032, acc 0.83
2016-05-20T17:37:36.598254: train-step 643, loss 0.452071, acc 0.84
2016-05-20T17:37:45.952222: train-step 644, loss 0.508443, acc 0.81
2016-05-20T17:37:54.863008: train-step 645, loss 0.472469, acc 0.83
2016-05-20T17:38:03.847463: train-step 646, loss 0.503471, acc 0.81
2016-05-20T17:38:13.101826: train-step 647, loss 0.513299, acc 0.8
2016-05-20T17:38:22.473540: train-step 648, loss 0.463446, acc 0.8
2016-05-20T17:38:31.916120: train-step 649, loss 0.548389, acc 0.74
2016-05-20T17:38:40.956512: train-step 650, loss 0.491799, acc 0.83
2016-05-20T17:38:50.041077: train-step 651, loss 0.352324, acc 0.93
2016-05-20T17:38:59.426103: train-step 652, loss 0.448609, acc 0.81
2016-05-20T17:39:08.613845: train-step 653, loss 0.429931, acc 0.86
2016-05-20T17:39:17.788688: train-step 654, loss 0.491474, acc 0.81
2016-05-20T17:39:26.796995: train-step 655, loss 0.455494, acc 0.87
2016-05-20T17:39:34.772494: train-step 656, loss 0.424735, acc 0.86
2016-05-20T17:39:43.878634: train-step 657, loss 0.475491, acc 0.82
2016-05-20T17:39:51.563172: train-step 658, loss 0.47227, acc 0.83
2016-05-20T17:39:59.385324: train-step 659, loss 0.44469, acc 0.85
2016-05-20T17:40:07.225393: train-step 660, loss 0.46136, acc 0.8
2016-05-20T17:40:14.748591: train-step 661, loss 0.490886, acc 0.82
2016-05-20T17:40:22.607634: train-step 662, loss 0.468018, acc 0.82
2016-05-20T17:40:30.484223: train-step 663, loss 0.491976, acc 0.78
2016-05-20T17:40:38.617198: train-step 664, loss 0.426019, acc 0.84
2016-05-20T17:40:47.317219: train-step 665, loss 0.417031, acc 0.83
2016-05-20T17:40:56.350275: train-step 666, loss 0.489663, acc 0.81
2016-05-20T17:41:05.345670: train-step 667, loss 0.446514, acc 0.83
2016-05-20T17:41:14.531876: train-step 668, loss 0.410419, acc 0.87
2016-05-20T17:41:23.705995: train-step 669, loss 0.514979, acc 0.79
2016-05-20T17:41:33.090710: train-step 670, loss 0.524544, acc 0.78
2016-05-20T17:41:42.226147: train-step 671, loss 0.479921, acc 0.79
2016-05-20T17:41:51.381825: train-step 672, loss 0.449947, acc 0.82
2016-05-20T17:42:00.804587: train-step 673, loss 0.517367, acc 0.77
2016-05-20T17:42:09.956538: train-step 674, loss 0.427353, acc 0.9
2016-05-20T17:42:18.980415: train-step 675, loss 0.415664, acc 0.83
2016-05-20T17:42:28.347670: train-step 676, loss 0.447095, acc 0.85
2016-05-20T17:42:37.058072: train-step 677, loss 0.382212, acc 0.89
2016-05-20T17:42:45.440767: train-step 678, loss 0.466021, acc 0.83
2016-05-20T17:42:54.905992: train-step 679, loss 0.485566, acc 0.8
2016-05-20T17:43:02.612114: train-step 680, loss 0.494285, acc 0.83
2016-05-20T17:43:10.475841: train-step 681, loss 0.477599, acc 0.78
2016-05-20T17:43:18.165499: train-step 682, loss 0.411092, acc 0.86
2016-05-20T17:43:26.107831: train-step 683, loss 0.450626, acc 0.81
2016-05-20T17:43:33.856062: train-step 684, loss 0.44509, acc 0.82
2016-05-20T17:43:41.652503: train-step 685, loss 0.390154, acc 0.9
2016-05-20T17:43:49.802053: train-step 686, loss 0.46918, acc 0.86
2016-05-20T17:43:58.446597: train-step 687, loss 0.531955, acc 0.76
2016-05-20T17:44:07.843744: train-step 688, loss 0.44187, acc 0.82
2016-05-20T17:44:16.655862: train-step 689, loss 0.43716, acc 0.86
2016-05-20T17:44:25.796660: train-step 690, loss 0.48541, acc 0.83
2016-05-20T17:44:35.029086: train-step 691, loss 0.438098, acc 0.88
2016-05-20T17:44:44.021650: train-step 692, loss 0.513552, acc 0.77
2016-05-20T17:44:53.163212: train-step 693, loss 0.525524, acc 0.78
2016-05-20T17:45:02.395596: train-step 694, loss 0.456686, acc 0.84
2016-05-20T17:45:11.418491: train-step 695, loss 0.404161, acc 0.86
2016-05-20T17:45:20.380955: train-step 696, loss 0.529616, acc 0.79
2016-05-20T17:45:29.391826: train-step 697, loss 0.427167, acc 0.85
2016-05-20T17:45:38.677047: train-step 698, loss 0.387994, acc 0.87
2016-05-20T17:45:47.471065: train-step 699, loss 0.524348, acc 0.79
2016-05-20T17:45:55.939092: train-step 700, loss 0.419367, acc 0.88
2016-05-20T17:46:04.328002: train-step 701, loss 0.453514, acc 0.86
2016-05-20T17:46:12.996996: train-step 702, loss 0.42554, acc 0.82
2016-05-20T17:46:20.836297: train-step 703, loss 0.508324, acc 0.8
2016-05-20T17:46:28.407106: train-step 704, loss 0.421455, acc 0.88
2016-05-20T17:46:36.070576: train-step 705, loss 0.465768, acc 0.85
2016-05-20T17:46:43.780151: train-step 706, loss 0.449321, acc 0.81
2016-05-20T17:46:51.986071: train-step 707, loss 0.466943, acc 0.85
2016-05-20T17:47:00.499724: train-step 708, loss 0.50959, acc 0.78
2016-05-20T17:47:09.192177: train-step 709, loss 0.473619, acc 0.83
2016-05-20T17:47:18.350666: train-step 710, loss 0.533862, acc 0.75
2016-05-20T17:47:27.483363: train-step 711, loss 0.461663, acc 0.81
2016-05-20T17:47:36.847824: train-step 712, loss 0.437052, acc 0.85
2016-05-20T17:47:46.120722: train-step 713, loss 0.484389, acc 0.81
2016-05-20T17:47:55.252739: train-step 714, loss 0.472676, acc 0.83
2016-05-20T17:48:04.254247: train-step 715, loss 0.478278, acc 0.8
2016-05-20T17:48:13.149445: train-step 716, loss 0.411687, acc 0.88
2016-05-20T17:48:22.342245: train-step 717, loss 0.478832, acc 0.81
2016-05-20T17:48:31.707158: train-step 718, loss 0.440227, acc 0.81
2016-05-20T17:48:40.763226: train-step 719, loss 0.442711, acc 0.88
2016-05-20T17:48:49.847750: train-step 720, loss 0.467225, acc 0.84
2016-05-20T17:48:58.565994: train-step 721, loss 0.535307, acc 0.76
2016-05-20T17:49:06.939678: train-step 722, loss 0.415186, acc 0.89
2016-05-20T17:49:16.064395: train-step 723, loss 0.448226, acc 0.81
2016-05-20T17:49:23.652644: train-step 724, loss 0.443808, acc 0.86
2016-05-20T17:49:31.348200: train-step 725, loss 0.411025, acc 0.86
2016-05-20T17:49:38.935664: train-step 726, loss 0.42242, acc 0.85
2016-05-20T17:49:46.778372: train-step 727, loss 0.431663, acc 0.86
2016-05-20T17:49:54.476896: train-step 728, loss 0.445715, acc 0.83
2016-05-20T17:50:02.199325: train-step 729, loss 0.479115, acc 0.82
2016-05-20T17:50:10.259064: train-step 730, loss 0.420857, acc 0.85
2016-05-20T17:50:19.252363: train-step 731, loss 0.428373, acc 0.83
2016-05-20T17:50:28.139628: train-step 732, loss 0.452617, acc 0.82
2016-05-20T17:50:37.361793: train-step 733, loss 0.439771, acc 0.87
2016-05-20T17:50:46.438996: train-step 734, loss 0.346667, acc 0.91
2016-05-20T17:50:56.061448: train-step 735, loss 0.383015, acc 0.87
2016-05-20T17:51:05.130823: train-step 736, loss 0.403686, acc 0.87
2016-05-20T17:51:14.048253: train-step 737, loss 0.420262, acc 0.86
2016-05-20T17:51:23.216838: train-step 738, loss 0.527377, acc 0.72
2016-05-20T17:51:32.612837: train-step 739, loss 0.45953, acc 0.81
2016-05-20T17:51:41.760412: train-step 740, loss 0.41455, acc 0.87
2016-05-20T17:51:51.124079: train-step 741, loss 0.471452, acc 0.82
2016-05-20T17:52:00.362552: train-step 742, loss 0.508317, acc 0.82
2016-05-20T17:52:09.216765: train-step 743, loss 0.456029, acc 0.84
2016-05-20T17:52:17.586468: train-step 744, loss 0.489653, acc 0.77
2016-05-20T17:52:25.966084: train-step 745, loss 0.438616, acc 0.82
2016-05-20T17:52:34.925451: train-step 746, loss 0.451223, acc 0.82
2016-05-20T17:52:42.657143: train-step 747, loss 0.3843, acc 0.84
2016-05-20T17:52:50.302110: train-step 748, loss 0.426429, acc 0.87
2016-05-20T17:52:58.080527: train-step 749, loss 0.42861, acc 0.89
2016-05-20T17:53:05.563422: train-step 750, loss 0.476572, acc 0.8
epoch number is: 3
2016-05-20T17:53:13.646850: train-step 751, loss 0.416676, acc 0.84
2016-05-20T17:53:21.711212: train-step 752, loss 0.397567, acc 0.86
2016-05-20T17:53:30.384987: train-step 753, loss 0.428311, acc 0.89
2016-05-20T17:53:39.422854: train-step 754, loss 0.481847, acc 0.8
2016-05-20T17:53:48.711005: train-step 755, loss 0.45784, acc 0.85
2016-05-20T17:53:57.810610: train-step 756, loss 0.387474, acc 0.91
2016-05-20T17:54:07.114900: train-step 757, loss 0.41104, acc 0.85
2016-05-20T17:54:16.232105: train-step 758, loss 0.394418, acc 0.89
2016-05-20T17:54:25.603139: train-step 759, loss 0.429606, acc 0.85
2016-05-20T17:54:34.777270: train-step 760, loss 0.379544, acc 0.89
2016-05-20T17:54:44.068575: train-step 761, loss 0.504133, acc 0.76
2016-05-20T17:54:53.188465: train-step 762, loss 0.420321, acc 0.83
2016-05-20T17:55:02.582260: train-step 763, loss 0.42764, acc 0.84
2016-05-20T17:55:11.991569: train-step 764, loss 0.367232, acc 0.91
2016-05-20T17:55:20.752083: train-step 765, loss 0.404723, acc 0.84
2016-05-20T17:55:29.472373: train-step 766, loss 0.388654, acc 0.85
2016-05-20T17:55:37.185665: train-step 767, loss 0.528483, acc 0.75
2016-05-20T17:55:46.708447: train-step 768, loss 0.488199, acc 0.76
2016-05-20T17:55:54.107674: train-step 769, loss 0.446637, acc 0.81
2016-05-20T17:56:01.935902: train-step 770, loss 0.502202, acc 0.79
2016-05-20T17:56:09.686962: train-step 771, loss 0.451461, acc 0.8
2016-05-20T17:56:17.473191: train-step 772, loss 0.398876, acc 0.88
2016-05-20T17:56:25.291727: train-step 773, loss 0.477769, acc 0.84
2016-05-20T17:56:33.083230: train-step 774, loss 0.47855, acc 0.82
2016-05-20T17:56:41.504221: train-step 775, loss 0.436518, acc 0.85
2016-05-20T17:56:50.338945: train-step 776, loss 0.412237, acc 0.86
2016-05-20T17:56:59.354863: train-step 777, loss 0.416286, acc 0.83
2016-05-20T17:57:08.528817: train-step 778, loss 0.49242, acc 0.8
2016-05-20T17:57:17.509750: train-step 779, loss 0.415638, acc 0.86
2016-05-20T17:57:26.756474: train-step 780, loss 0.338481, acc 0.93
2016-05-20T17:57:35.954054: train-step 781, loss 0.406218, acc 0.88
2016-05-20T17:57:45.180687: train-step 782, loss 0.380242, acc 0.88
2016-05-20T17:57:54.260033: train-step 783, loss 0.46111, acc 0.82
2016-05-20T17:58:03.568269: train-step 784, loss 0.382063, acc 0.9
2016-05-20T17:58:12.719394: train-step 785, loss 0.427976, acc 0.85
2016-05-20T17:58:21.912908: train-step 786, loss 0.450723, acc 0.83
2016-05-20T17:58:31.160402: train-step 787, loss 0.364455, acc 0.94
2016-05-20T17:58:39.759074: train-step 788, loss 0.483876, acc 0.78
2016-05-20T17:58:47.533801: train-step 789, loss 0.422772, acc 0.86
2016-05-20T17:58:56.793766: train-step 790, loss 0.463655, acc 0.85
2016-05-20T17:59:04.684297: train-step 791, loss 0.428521, acc 0.84
2016-05-20T17:59:12.560888: train-step 792, loss 0.469521, acc 0.83
2016-05-20T17:59:20.701221: train-step 793, loss 0.443712, acc 0.86
2016-05-20T17:59:28.247685: train-step 794, loss 0.548915, acc 0.78
2016-05-20T17:59:36.510078: train-step 795, loss 0.415845, acc 0.87
2016-05-20T17:59:44.639766: train-step 796, loss 0.446798, acc 0.83
2016-05-20T17:59:52.996977: train-step 797, loss 0.44702, acc 0.85
2016-05-20T18:00:01.889269: train-step 798, loss 0.398942, acc 0.88
2016-05-20T18:00:11.061259: train-step 799, loss 0.429009, acc 0.85
2016-05-20T18:00:20.236883: train-step 800, loss 0.504633, acc 0.84
2016-05-20T18:00:29.602488: train-step 801, loss 0.427726, acc 0.86
2016-05-20T18:00:38.844846: train-step 802, loss 0.382624, acc 0.89
2016-05-20T18:00:48.136570: train-step 803, loss 0.492784, acc 0.82
2016-05-20T18:00:57.341113: train-step 804, loss 0.397838, acc 0.88
2016-05-20T18:01:06.435445: train-step 805, loss 0.441731, acc 0.85
2016-05-20T18:01:15.480475: train-step 806, loss 0.443099, acc 0.83
2016-05-20T18:01:24.447354: train-step 807, loss 0.383588, acc 0.89
2016-05-20T18:01:33.875623: train-step 808, loss 0.469431, acc 0.78
2016-05-20T18:01:42.903540: train-step 809, loss 0.47309, acc 0.77
2016-05-20T18:01:51.675192: train-step 810, loss 0.563471, acc 0.77
2016-05-20T18:01:59.853771: train-step 811, loss 0.440506, acc 0.84
2016-05-20T18:02:09.135571: train-step 812, loss 0.458957, acc 0.84
2016-05-20T18:02:16.613157: train-step 813, loss 0.377262, acc 0.88
2016-05-20T18:02:24.295739: train-step 814, loss 0.39502, acc 0.82
2016-05-20T18:02:31.928455: train-step 815, loss 0.441951, acc 0.8
2016-05-20T18:02:39.853292: train-step 816, loss 0.418383, acc 0.82
2016-05-20T18:02:47.524937: train-step 817, loss 0.425929, acc 0.85
2016-05-20T18:02:55.641693: train-step 818, loss 0.396648, acc 0.87
2016-05-20T18:03:03.751718: train-step 819, loss 0.387549, acc 0.85
2016-05-20T18:03:12.434867: train-step 820, loss 0.407624, acc 0.86
2016-05-20T18:03:21.578335: train-step 821, loss 0.439586, acc 0.85
2016-05-20T18:03:30.660007: train-step 822, loss 0.43149, acc 0.87
2016-05-20T18:03:39.941780: train-step 823, loss 0.410172, acc 0.86
2016-05-20T18:03:49.188449: train-step 824, loss 0.428648, acc 0.85
2016-05-20T18:03:58.516413: train-step 825, loss 0.366083, acc 0.9
2016-05-20T18:04:07.687747: train-step 826, loss 0.438051, acc 0.87
2016-05-20T18:04:16.924442: train-step 827, loss 0.418341, acc 0.83
2016-05-20T18:04:26.090032: train-step 828, loss 0.401244, acc 0.9
2016-05-20T18:04:35.267203: train-step 829, loss 0.439237, acc 0.86
2016-05-20T18:04:44.275341: train-step 830, loss 0.447806, acc 0.82
2016-05-20T18:04:53.317970: train-step 831, loss 0.487314, acc 0.79
2016-05-20T18:05:01.968069: train-step 832, loss 0.369095, acc 0.86
2016-05-20T18:05:10.250094: train-step 833, loss 0.473833, acc 0.81
2016-05-20T18:05:19.888990: train-step 834, loss 0.513989, acc 0.79
2016-05-20T18:05:27.653188: train-step 835, loss 0.414342, acc 0.88
2016-05-20T18:05:35.344394: train-step 836, loss 0.376704, acc 0.88
2016-05-20T18:05:42.836591: train-step 837, loss 0.413228, acc 0.85
2016-05-20T18:05:51.603237: train-step 838, loss 0.393901, acc 0.86
2016-05-20T18:05:59.887314: train-step 839, loss 0.489317, acc 0.79
2016-05-20T18:06:07.894003: train-step 840, loss 0.45784, acc 0.84
2016-05-20T18:06:16.001169: train-step 841, loss 0.432931, acc 0.84
2016-05-20T18:06:24.471010: train-step 842, loss 0.454888, acc 0.82
2016-05-20T18:06:33.549221: train-step 843, loss 0.428629, acc 0.85
2016-05-20T18:06:42.793970: train-step 844, loss 0.353077, acc 0.9
2016-05-20T18:06:52.011030: train-step 845, loss 0.43503, acc 0.84
2016-05-20T18:07:01.402150: train-step 846, loss 0.442538, acc 0.83
2016-05-20T18:07:10.453879: train-step 847, loss 0.401542, acc 0.88
2016-05-20T18:07:19.799520: train-step 848, loss 0.496812, acc 0.73
2016-05-20T18:07:28.898687: train-step 849, loss 0.422608, acc 0.84
2016-05-20T18:07:38.091243: train-step 850, loss 0.329878, acc 0.9
2016-05-20T18:07:47.053134: train-step 851, loss 0.377102, acc 0.9
2016-05-20T18:07:56.215256: train-step 852, loss 0.468922, acc 0.81
2016-05-20T18:08:05.307347: train-step 853, loss 0.420013, acc 0.85
2016-05-20T18:08:14.348784: train-step 854, loss 0.443604, acc 0.82
2016-05-20T18:08:22.684897: train-step 855, loss 0.376331, acc 0.89
2016-05-20T18:08:31.532453: train-step 856, loss 0.423739, acc 0.86
2016-05-20T18:08:39.782716: train-step 857, loss 0.377469, acc 0.86
2016-05-20T18:08:47.468487: train-step 858, loss 0.403582, acc 0.89
2016-05-20T18:08:54.938427: train-step 859, loss 0.375808, acc 0.9
2016-05-20T18:09:02.711439: train-step 860, loss 0.427659, acc 0.84
2016-05-20T18:09:10.164691: train-step 861, loss 0.50761, acc 0.76
2016-05-20T18:09:17.758460: train-step 862, loss 0.423002, acc 0.86
2016-05-20T18:09:25.923172: train-step 863, loss 0.434502, acc 0.81
2016-05-20T18:09:34.528742: train-step 864, loss 0.428919, acc 0.85
2016-05-20T18:09:43.307006: train-step 865, loss 0.493359, acc 0.82
2016-05-20T18:09:52.700768: train-step 866, loss 0.453535, acc 0.84
2016-05-20T18:10:01.812338: train-step 867, loss 0.425737, acc 0.85
2016-05-20T18:10:11.001339: train-step 868, loss 0.463975, acc 0.83
2016-05-20T18:10:20.493414: train-step 869, loss 0.415255, acc 0.85
2016-05-20T18:10:29.842069: train-step 870, loss 0.393454, acc 0.89
2016-05-20T18:10:38.873033: train-step 871, loss 0.457484, acc 0.82
2016-05-20T18:10:48.155746: train-step 872, loss 0.385375, acc 0.87
2016-05-20T18:10:57.514108: train-step 873, loss 0.411202, acc 0.84
2016-05-20T18:11:06.691724: train-step 874, loss 0.408187, acc 0.83
2016-05-20T18:11:15.871105: train-step 875, loss 0.43755, acc 0.84
2016-05-20T18:11:24.998599: train-step 876, loss 0.408027, acc 0.82
2016-05-20T18:11:33.763541: train-step 877, loss 0.424632, acc 0.87
2016-05-20T18:11:41.740141: train-step 878, loss 0.400303, acc 0.84
2016-05-20T18:11:50.974717: train-step 879, loss 0.421791, acc 0.84
2016-05-20T18:11:58.463795: train-step 880, loss 0.400302, acc 0.85
2016-05-20T18:12:06.267986: train-step 881, loss 0.388797, acc 0.87
2016-05-20T18:12:14.555069: train-step 882, loss 0.40565, acc 0.83
2016-05-20T18:12:22.334697: train-step 883, loss 0.369481, acc 0.88
2016-05-20T18:12:30.134718: train-step 884, loss 0.443794, acc 0.84
2016-05-20T18:12:37.927048: train-step 885, loss 0.415067, acc 0.87
2016-05-20T18:12:46.026644: train-step 886, loss 0.425417, acc 0.82
2016-05-20T18:12:54.837913: train-step 887, loss 0.430188, acc 0.85
2016-05-20T18:13:04.099732: train-step 888, loss 0.459229, acc 0.82
2016-05-20T18:13:13.263202: train-step 889, loss 0.395956, acc 0.86
2016-05-20T18:13:22.458490: train-step 890, loss 0.416419, acc 0.88
2016-05-20T18:13:31.749976: train-step 891, loss 0.494549, acc 0.79
2016-05-20T18:13:41.123626: train-step 892, loss 0.400486, acc 0.85
2016-05-20T18:13:50.409808: train-step 893, loss 0.428803, acc 0.83
2016-05-20T18:13:59.886211: train-step 894, loss 0.423138, acc 0.84
2016-05-20T18:14:09.047543: train-step 895, loss 0.430485, acc 0.84
2016-05-20T18:14:18.572266: train-step 896, loss 0.401444, acc 0.88
2016-05-20T18:14:27.659126: train-step 897, loss 0.423163, acc 0.85
2016-05-20T18:14:36.586549: train-step 898, loss 0.369537, acc 0.87
2016-05-20T18:14:45.322529: train-step 899, loss 0.397469, acc 0.85
2016-05-20T18:14:53.668704: train-step 900, loss 0.406253, acc 0.9
2016-05-20T18:15:03.290715: train-step 901, loss 0.458206, acc 0.8
2016-05-20T18:15:11.087793: train-step 902, loss 0.440242, acc 0.84
2016-05-20T18:15:18.806462: train-step 903, loss 0.359054, acc 0.89
2016-05-20T18:15:26.281970: train-step 904, loss 0.425527, acc 0.86
2016-05-20T18:15:34.641679: train-step 905, loss 0.423788, acc 0.88
2016-05-20T18:15:42.885841: train-step 906, loss 0.51893, acc 0.77
2016-05-20T18:15:50.701776: train-step 907, loss 0.35054, acc 0.9
2016-05-20T18:15:59.006156: train-step 908, loss 0.421611, acc 0.87
2016-05-20T18:16:07.705626: train-step 909, loss 0.401029, acc 0.87
2016-05-20T18:16:16.832466: train-step 910, loss 0.337195, acc 0.91
2016-05-20T18:16:26.013536: train-step 911, loss 0.48194, acc 0.82
2016-05-20T18:16:35.281458: train-step 912, loss 0.385077, acc 0.88
2016-05-20T18:16:44.731388: train-step 913, loss 0.40425, acc 0.86
2016-05-20T18:16:54.021767: train-step 914, loss 0.398421, acc 0.87
2016-05-20T18:17:03.247082: train-step 915, loss 0.410652, acc 0.85
2016-05-20T18:17:12.442284: train-step 916, loss 0.429695, acc 0.86
2016-05-20T18:17:21.720543: train-step 917, loss 0.383009, acc 0.89
2016-05-20T18:17:30.649181: train-step 918, loss 0.396151, acc 0.87
2016-05-20T18:17:39.593616: train-step 919, loss 0.478324, acc 0.76
2016-05-20T18:17:48.894450: train-step 920, loss 0.48918, acc 0.81
2016-05-20T18:17:57.618091: train-step 921, loss 0.397744, acc 0.88
2016-05-20T18:18:05.971498: train-step 922, loss 0.418506, acc 0.84
2016-05-20T18:18:15.210839: train-step 923, loss 0.377643, acc 0.88
2016-05-20T18:18:22.884567: train-step 924, loss 0.472466, acc 0.8
2016-05-20T18:18:30.636991: train-step 925, loss 0.403604, acc 0.89
2016-05-20T18:18:38.803314: train-step 926, loss 0.393753, acc 0.85
2016-05-20T18:18:46.674906: train-step 927, loss 0.457908, acc 0.82
2016-05-20T18:18:54.412129: train-step 928, loss 0.45864, acc 0.86
2016-05-20T18:19:02.284766: train-step 929, loss 0.37742, acc 0.87
2016-05-20T18:19:10.476232: train-step 930, loss 0.39248, acc 0.89
2016-05-20T18:19:19.263143: train-step 931, loss 0.403478, acc 0.91
2016-05-20T18:19:28.194186: train-step 932, loss 0.325713, acc 0.91
2016-05-20T18:19:37.185395: train-step 933, loss 0.462919, acc 0.8
2016-05-20T18:19:46.428019: train-step 934, loss 0.340236, acc 0.91
2016-05-20T18:19:55.784228: train-step 935, loss 0.35406, acc 0.88
2016-05-20T18:20:04.956179: train-step 936, loss 0.39046, acc 0.87
2016-05-20T18:20:14.106657: train-step 937, loss 0.455396, acc 0.85
2016-05-20T18:20:23.039713: train-step 938, loss 0.351254, acc 0.88
2016-05-20T18:20:32.204082: train-step 939, loss 0.462435, acc 0.84
2016-05-20T18:20:41.427240: train-step 940, loss 0.387637, acc 0.87
2016-05-20T18:20:50.461667: train-step 941, loss 0.378509, acc 0.83
2016-05-20T18:20:59.615450: train-step 942, loss 0.370691, acc 0.87
2016-05-20T18:21:08.564832: train-step 943, loss 0.389627, acc 0.87
2016-05-20T18:21:17.196676: train-step 944, loss 0.427177, acc 0.89
2016-05-20T18:21:25.278616: train-step 945, loss 0.41716, acc 0.85
2016-05-20T18:21:34.273430: train-step 946, loss 0.435831, acc 0.86
2016-05-20T18:21:42.121574: train-step 947, loss 0.425434, acc 0.85
2016-05-20T18:21:50.225644: train-step 948, loss 0.454101, acc 0.84
2016-05-20T18:21:58.119716: train-step 949, loss 0.450119, acc 0.83
2016-05-20T18:22:05.679406: train-step 950, loss 0.34015, acc 0.9
2016-05-20T18:22:13.585643: train-step 951, loss 0.341107, acc 0.9
2016-05-20T18:22:21.344279: train-step 952, loss 0.376229, acc 0.87
2016-05-20T18:22:29.783824: train-step 953, loss 0.479888, acc 0.81
2016-05-20T18:22:38.676874: train-step 954, loss 0.411039, acc 0.86
2016-05-20T18:22:47.992328: train-step 955, loss 0.408245, acc 0.86
2016-05-20T18:22:57.251032: train-step 956, loss 0.428317, acc 0.88
2016-05-20T18:23:06.265787: train-step 957, loss 0.366656, acc 0.84
2016-05-20T18:23:15.484711: train-step 958, loss 0.351211, acc 0.88
2016-05-20T18:23:24.731059: train-step 959, loss 0.359673, acc 0.92
2016-05-20T18:23:34.058731: train-step 960, loss 0.465985, acc 0.81
2016-05-20T18:23:43.232134: train-step 961, loss 0.487514, acc 0.82
2016-05-20T18:23:52.315963: train-step 962, loss 0.488235, acc 0.8
2016-05-20T18:24:01.507046: train-step 963, loss 0.466187, acc 0.82
2016-05-20T18:24:10.458814: train-step 964, loss 0.453755, acc 0.85
2016-05-20T18:24:19.351878: train-step 965, loss 0.313912, acc 0.91
2016-05-20T18:24:28.168361: train-step 966, loss 0.500578, acc 0.73
2016-05-20T18:24:36.386693: train-step 967, loss 0.49154, acc 0.79
2016-05-20T18:24:46.107633: train-step 968, loss 0.40969, acc 0.85
2016-05-20T18:24:53.709164: train-step 969, loss 0.426433, acc 0.85
2016-05-20T18:25:01.456619: train-step 970, loss 0.331547, acc 0.92
2016-05-20T18:25:09.178385: train-step 971, loss 0.402909, acc 0.84
2016-05-20T18:25:16.736228: train-step 972, loss 0.345695, acc 0.89
2016-05-20T18:25:24.763547: train-step 973, loss 0.463033, acc 0.81
2016-05-20T18:25:32.484107: train-step 974, loss 0.361153, acc 0.91
2016-05-20T18:25:40.838233: train-step 975, loss 0.414411, acc 0.85
2016-05-20T18:25:49.637521: train-step 976, loss 0.361097, acc 0.87
2016-05-20T18:25:58.551175: train-step 977, loss 0.413847, acc 0.85
2016-05-20T18:26:07.549549: train-step 978, loss 0.391949, acc 0.89
2016-05-20T18:26:16.691699: train-step 979, loss 0.453118, acc 0.81
2016-05-20T18:26:25.873983: train-step 980, loss 0.443924, acc 0.81
2016-05-20T18:26:35.221803: train-step 981, loss 0.376938, acc 0.88
2016-05-20T18:26:44.587543: train-step 982, loss 0.396223, acc 0.87
2016-05-20T18:26:53.885205: train-step 983, loss 0.388573, acc 0.85
2016-05-20T18:27:03.068326: train-step 984, loss 0.38348, acc 0.88
2016-05-20T18:27:12.255971: train-step 985, loss 0.409104, acc 0.86
2016-05-20T18:27:21.576975: train-step 986, loss 0.395062, acc 0.88
2016-05-20T18:27:30.502463: train-step 987, loss 0.414197, acc 0.86
2016-05-20T18:27:39.299035: train-step 988, loss 0.386926, acc 0.91
2016-05-20T18:27:47.562978: train-step 989, loss 0.444908, acc 0.84
2016-05-20T18:27:57.030278: train-step 990, loss 0.368986, acc 0.9
2016-05-20T18:28:04.778381: train-step 991, loss 0.349506, acc 0.87
2016-05-20T18:28:12.448360: train-step 992, loss 0.468926, acc 0.8
2016-05-20T18:28:19.893791: train-step 993, loss 0.405804, acc 0.87
2016-05-20T18:28:27.656159: train-step 994, loss 0.340295, acc 0.91
2016-05-20T18:28:35.398873: train-step 995, loss 0.401455, acc 0.85
2016-05-20T18:28:43.264827: train-step 996, loss 0.466285, acc 0.85
2016-05-20T18:28:51.695015: train-step 997, loss 0.36412, acc 0.89
2016-05-20T18:29:00.685744: train-step 998, loss 0.47145, acc 0.83
2016-05-20T18:29:09.883929: train-step 999, loss 0.363999, acc 0.86
2016-05-20T18:29:19.052014: train-step 1000, loss 0.389114, acc 0.84
epoch number is: 4
2016-05-20T18:29:28.664888: train-step 1001, loss 0.419811, acc 0.82
2016-05-20T18:29:37.748881: train-step 1002, loss 0.302446, acc 0.91
2016-05-20T18:29:46.831341: train-step 1003, loss 0.433378, acc 0.79
2016-05-20T18:29:56.070884: train-step 1004, loss 0.41386, acc 0.82
2016-05-20T18:30:05.222642: train-step 1005, loss 0.426614, acc 0.85
2016-05-20T18:30:13.957532: train-step 1006, loss 0.472427, acc 0.86
2016-05-20T18:30:23.095758: train-step 1007, loss 0.467277, acc 0.79
2016-05-20T18:30:32.475578: train-step 1008, loss 0.373253, acc 0.88
2016-05-20T18:30:41.630145: train-step 1009, loss 0.390661, acc 0.85
2016-05-20T18:30:50.867464: train-step 1010, loss 0.376059, acc 0.85
2016-05-20T18:30:59.251503: train-step 1011, loss 0.52671, acc 0.8
2016-05-20T18:31:07.248140: train-step 1012, loss 0.528406, acc 0.75
2016-05-20T18:31:16.242971: train-step 1013, loss 0.383931, acc 0.85
2016-05-20T18:31:24.122636: train-step 1014, loss 0.433268, acc 0.82
2016-05-20T18:31:31.648023: train-step 1015, loss 0.336327, acc 0.92
2016-05-20T18:31:39.332152: train-step 1016, loss 0.388886, acc 0.87
2016-05-20T18:31:46.997742: train-step 1017, loss 0.393292, acc 0.86
2016-05-20T18:31:54.828368: train-step 1018, loss 0.398238, acc 0.83
2016-05-20T18:32:02.693359: train-step 1019, loss 0.372572, acc 0.86
2016-05-20T18:32:11.301306: train-step 1020, loss 0.423357, acc 0.81
2016-05-20T18:32:19.966188: train-step 1021, loss 0.323518, acc 0.92
2016-05-20T18:32:29.649512: train-step 1022, loss 0.390628, acc 0.85
2016-05-20T18:32:39.346888: train-step 1023, loss 0.330721, acc 0.93
2016-05-20T18:32:48.479889: train-step 1024, loss 0.334838, acc 0.92
2016-05-20T18:32:57.414697: train-step 1025, loss 0.360373, acc 0.88
2016-05-20T18:33:06.455154: train-step 1026, loss 0.383664, acc 0.9
2016-05-20T18:33:15.573716: train-step 1027, loss 0.470008, acc 0.78
2016-05-20T18:33:24.545706: train-step 1028, loss 0.418202, acc 0.84
2016-05-20T18:33:33.699726: train-step 1029, loss 0.470388, acc 0.83
2016-05-20T18:33:42.782786: train-step 1030, loss 0.370421, acc 0.89
2016-05-20T18:33:52.076824: train-step 1031, loss 0.412616, acc 0.85
2016-05-20T18:34:00.998528: train-step 1032, loss 0.39396, acc 0.83
2016-05-20T18:34:09.552715: train-step 1033, loss 0.40556, acc 0.82
2016-05-20T18:34:18.492034: train-step 1034, loss 0.42029, acc 0.85
2016-05-20T18:34:26.818727: train-step 1035, loss 0.41405, acc 0.84
2016-05-20T18:34:34.445348: train-step 1036, loss 0.380598, acc 0.86
2016-05-20T18:34:41.893786: train-step 1037, loss 0.36825, acc 0.87
2016-05-20T18:34:49.807366: train-step 1038, loss 0.4021, acc 0.86
2016-05-20T18:34:58.381477: train-step 1039, loss 0.352497, acc 0.92
2016-05-20T18:35:06.282388: train-step 1040, loss 0.360052, acc 0.9
2016-05-20T18:35:14.416292: train-step 1041, loss 0.365157, acc 0.91
2016-05-20T18:35:23.020293: train-step 1042, loss 0.375823, acc 0.89
2016-05-20T18:35:31.970599: train-step 1043, loss 0.352951, acc 0.89
2016-05-20T18:35:41.102658: train-step 1044, loss 0.378316, acc 0.85
2016-05-20T18:35:50.314182: train-step 1045, loss 0.380229, acc 0.87
2016-05-20T18:35:59.732368: train-step 1046, loss 0.375268, acc 0.89
2016-05-20T18:36:08.780410: train-step 1047, loss 0.364238, acc 0.88
2016-05-20T18:36:17.656594: train-step 1048, loss 0.392314, acc 0.84
2016-05-20T18:36:27.059962: train-step 1049, loss 0.390717, acc 0.88
2016-05-20T18:36:36.337699: train-step 1050, loss 0.366056, acc 0.91
2016-05-20T18:36:45.736559: train-step 1051, loss 0.33509, acc 0.9
2016-05-20T18:36:55.098152: train-step 1052, loss 0.47253, acc 0.84
2016-05-20T18:37:04.130324: train-step 1053, loss 0.361125, acc 0.88
2016-05-20T18:37:13.180923: train-step 1054, loss 0.428196, acc 0.8
2016-05-20T18:37:21.785545: train-step 1055, loss 0.397791, acc 0.85
2016-05-20T18:37:29.848441: train-step 1056, loss 0.385578, acc 0.88
2016-05-20T18:37:39.046247: train-step 1057, loss 0.361213, acc 0.89
2016-05-20T18:37:46.878558: train-step 1058, loss 0.329581, acc 0.92
2016-05-20T18:37:54.668476: train-step 1059, loss 0.355403, acc 0.89
2016-05-20T18:38:02.723391: train-step 1060, loss 0.38529, acc 0.89
2016-05-20T18:38:10.473660: train-step 1061, loss 0.420208, acc 0.84
2016-05-20T18:38:18.848272: train-step 1062, loss 0.481012, acc 0.87
2016-05-20T18:38:26.644023: train-step 1063, loss 0.378226, acc 0.85
2016-05-20T18:38:35.126939: train-step 1064, loss 0.41716, acc 0.86
2016-05-20T18:38:44.036062: train-step 1065, loss 0.452114, acc 0.86
2016-05-20T18:38:53.361196: train-step 1066, loss 0.441461, acc 0.85
2016-05-20T18:39:02.227797: train-step 1067, loss 0.430015, acc 0.82
2016-05-20T18:39:11.463341: train-step 1068, loss 0.476395, acc 0.77
2016-05-20T18:39:20.525365: train-step 1069, loss 0.480017, acc 0.81
2016-05-20T18:39:29.727683: train-step 1070, loss 0.434769, acc 0.81
2016-05-20T18:39:39.202666: train-step 1071, loss 0.434018, acc 0.86
2016-05-20T18:39:48.397771: train-step 1072, loss 0.435789, acc 0.86
2016-05-20T18:39:57.816280: train-step 1073, loss 0.439659, acc 0.89
2016-05-20T18:40:06.921901: train-step 1074, loss 0.380921, acc 0.89
2016-05-20T18:40:16.168257: train-step 1075, loss 0.375583, acc 0.9
2016-05-20T18:40:25.396500: train-step 1076, loss 0.341342, acc 0.92
2016-05-20T18:40:33.904937: train-step 1077, loss 0.33538, acc 0.92
2016-05-20T18:40:41.878869: train-step 1078, loss 0.401322, acc 0.89
2016-05-20T18:40:51.101004: train-step 1079, loss 0.358602, acc 0.89
2016-05-20T18:40:58.728706: train-step 1080, loss 0.385714, acc 0.86
2016-05-20T18:41:06.751776: train-step 1081, loss 0.341737, acc 0.89
2016-05-20T18:41:14.881299: train-step 1082, loss 0.401172, acc 0.84
2016-05-20T18:41:22.708755: train-step 1083, loss 0.311323, acc 0.9
2016-05-20T18:41:30.576696: train-step 1084, loss 0.36048, acc 0.9
2016-05-20T18:41:38.300583: train-step 1085, loss 0.399896, acc 0.83
2016-05-20T18:41:46.437878: train-step 1086, loss 0.442955, acc 0.86
2016-05-20T18:41:55.282687: train-step 1087, loss 0.38852, acc 0.86
2016-05-20T18:42:04.644128: train-step 1088, loss 0.370181, acc 0.87
2016-05-20T18:42:13.780889: train-step 1089, loss 0.430612, acc 0.83
2016-05-20T18:42:22.894021: train-step 1090, loss 0.376352, acc 0.85
2016-05-20T18:42:32.351081: train-step 1091, loss 0.359981, acc 0.9
2016-05-20T18:42:41.757465: train-step 1092, loss 0.375166, acc 0.89
2016-05-20T18:42:51.102262: train-step 1093, loss 0.35323, acc 0.92
2016-05-20T18:43:00.100252: train-step 1094, loss 0.35281, acc 0.85
2016-05-20T18:43:09.294493: train-step 1095, loss 0.382152, acc 0.89
2016-05-20T18:43:18.564610: train-step 1096, loss 0.375814, acc 0.87
2016-05-20T18:43:27.688975: train-step 1097, loss 0.384786, acc 0.84
2016-05-20T18:43:36.732007: train-step 1098, loss 0.407811, acc 0.83
2016-05-20T18:43:45.579244: train-step 1099, loss 0.392446, acc 0.85
2016-05-20T18:43:53.601914: train-step 1100, loss 0.414134, acc 0.83
2016-05-20T18:44:02.895130: train-step 1101, loss 0.367829, acc 0.88
2016-05-20T18:44:10.294807: train-step 1102, loss 0.373803, acc 0.87
2016-05-20T18:44:18.276189: train-step 1103, loss 0.400542, acc 0.87
2016-05-20T18:44:26.161098: train-step 1104, loss 0.384671, acc 0.85
2016-05-20T18:44:34.042034: train-step 1105, loss 0.385226, acc 0.87
2016-05-20T18:44:41.708556: train-step 1106, loss 0.353909, acc 0.89
2016-05-20T18:44:49.497599: train-step 1107, loss 0.366436, acc 0.88
2016-05-20T18:44:57.527335: train-step 1108, loss 0.41465, acc 0.88
2016-05-20T18:45:06.547087: train-step 1109, loss 0.459063, acc 0.86
2016-05-20T18:45:15.690047: train-step 1110, loss 0.397646, acc 0.86
2016-05-20T18:45:24.761678: train-step 1111, loss 0.336557, acc 0.9
2016-05-20T18:45:34.187600: train-step 1112, loss 0.32978, acc 0.91
2016-05-20T18:45:43.672220: train-step 1113, loss 0.369355, acc 0.84
2016-05-20T18:45:52.936191: train-step 1114, loss 0.308845, acc 0.93
2016-05-20T18:46:02.082881: train-step 1115, loss 0.330816, acc 0.91
2016-05-20T18:46:11.328564: train-step 1116, loss 0.389109, acc 0.87
2016-05-20T18:46:20.446043: train-step 1117, loss 0.340379, acc 0.9
2016-05-20T18:46:29.429566: train-step 1118, loss 0.365795, acc 0.86
2016-05-20T18:46:38.603887: train-step 1119, loss 0.387454, acc 0.88
2016-05-20T18:46:47.573885: train-step 1120, loss 0.437474, acc 0.88
2016-05-20T18:46:56.437212: train-step 1121, loss 0.358198, acc 0.94
2016-05-20T18:47:04.363400: train-step 1122, loss 0.44372, acc 0.78
2016-05-20T18:47:13.322635: train-step 1123, loss 0.405767, acc 0.85
2016-05-20T18:47:20.721074: train-step 1124, loss 0.373598, acc 0.87
2016-05-20T18:47:28.556707: train-step 1125, loss 0.433352, acc 0.83
2016-05-20T18:47:36.145755: train-step 1126, loss 0.461, acc 0.85
2016-05-20T18:47:44.420244: train-step 1127, loss 0.385683, acc 0.83
2016-05-20T18:47:52.293543: train-step 1128, loss 0.370096, acc 0.88
2016-05-20T18:48:00.303779: train-step 1129, loss 0.39181, acc 0.87
2016-05-20T18:48:08.607748: train-step 1130, loss 0.378067, acc 0.88
2016-05-20T18:48:17.851823: train-step 1131, loss 0.379019, acc 0.85
2016-05-20T18:48:27.081371: train-step 1132, loss 0.417293, acc 0.85
2016-05-20T18:48:36.342071: train-step 1133, loss 0.336185, acc 0.91
2016-05-20T18:48:45.706912: train-step 1134, loss 0.394334, acc 0.85
2016-05-20T18:48:54.856225: train-step 1135, loss 0.319761, acc 0.92
2016-05-20T18:49:04.482510: train-step 1136, loss 0.352583, acc 0.87
2016-05-20T18:49:13.906707: train-step 1137, loss 0.396123, acc 0.83
2016-05-20T18:49:23.214285: train-step 1138, loss 0.353369, acc 0.9
2016-05-20T18:49:32.304925: train-step 1139, loss 0.450697, acc 0.85
2016-05-20T18:49:41.413394: train-step 1140, loss 0.405126, acc 0.85
2016-05-20T18:49:50.642659: train-step 1141, loss 0.357759, acc 0.93
2016-05-20T18:50:00.121780: train-step 1142, loss 0.356894, acc 0.89
2016-05-20T18:50:09.000116: train-step 1143, loss 0.30757, acc 0.89
2016-05-20T18:50:17.155125: train-step 1144, loss 0.454713, acc 0.8
2016-05-20T18:50:26.399592: train-step 1145, loss 0.391327, acc 0.86
2016-05-20T18:50:33.969147: train-step 1146, loss 0.342206, acc 0.91
2016-05-20T18:50:41.607951: train-step 1147, loss 0.370706, acc 0.89
2016-05-20T18:50:49.773627: train-step 1148, loss 0.422684, acc 0.84
2016-05-20T18:50:57.827973: train-step 1149, loss 0.406086, acc 0.88
2016-05-20T18:51:05.576816: train-step 1150, loss 0.418298, acc 0.83
2016-05-20T18:51:13.536424: train-step 1151, loss 0.406124, acc 0.85
2016-05-20T18:51:21.681042: train-step 1152, loss 0.38757, acc 0.85
2016-05-20T18:51:30.437089: train-step 1153, loss 0.394047, acc 0.84
2016-05-20T18:51:39.467358: train-step 1154, loss 0.444276, acc 0.82
2016-05-20T18:51:48.679438: train-step 1155, loss 0.355036, acc 0.86
2016-05-20T18:51:58.488572: train-step 1156, loss 0.396831, acc 0.83
2016-05-20T18:52:07.826575: train-step 1157, loss 0.39308, acc 0.85
2016-05-20T18:52:17.035724: train-step 1158, loss 0.437999, acc 0.85
2016-05-20T18:52:25.994993: train-step 1159, loss 0.357102, acc 0.87
2016-05-20T18:52:35.280644: train-step 1160, loss 0.366535, acc 0.89
2016-05-20T18:52:44.363082: train-step 1161, loss 0.385328, acc 0.86
2016-05-20T18:52:53.478146: train-step 1162, loss 0.387336, acc 0.9
2016-05-20T18:53:02.773767: train-step 1163, loss 0.396371, acc 0.86
2016-05-20T18:53:11.823869: train-step 1164, loss 0.467002, acc 0.81
2016-05-20T18:53:20.476023: train-step 1165, loss 0.373028, acc 0.87
2016-05-20T18:53:28.863525: train-step 1166, loss 0.366887, acc 0.84
2016-05-20T18:53:37.299161: train-step 1167, loss 0.385314, acc 0.85
2016-05-20T18:53:46.235256: train-step 1168, loss 0.316289, acc 0.93
2016-05-20T18:53:53.944762: train-step 1169, loss 0.383896, acc 0.86
2016-05-20T18:54:01.634762: train-step 1170, loss 0.401694, acc 0.86
2016-05-20T18:54:09.206394: train-step 1171, loss 0.356212, acc 0.87
2016-05-20T18:54:16.847927: train-step 1172, loss 0.34733, acc 0.91
2016-05-20T18:54:24.395982: train-step 1173, loss 0.415374, acc 0.88
2016-05-20T18:54:32.033596: train-step 1174, loss 0.412324, acc 0.86
2016-05-20T18:54:40.330492: train-step 1175, loss 0.353103, acc 0.86
2016-05-20T18:54:49.087335: train-step 1176, loss 0.383496, acc 0.9
2016-05-20T18:54:58.099529: train-step 1177, loss 0.406391, acc 0.85
2016-05-20T18:55:07.272674: train-step 1178, loss 0.38427, acc 0.85
2016-05-20T18:55:16.554706: train-step 1179, loss 0.348858, acc 0.9
2016-05-20T18:55:25.715543: train-step 1180, loss 0.406324, acc 0.82
2016-05-20T18:55:35.078341: train-step 1181, loss 0.393171, acc 0.89
2016-05-20T18:55:44.140965: train-step 1182, loss 0.456794, acc 0.85
2016-05-20T18:55:53.542354: train-step 1183, loss 0.433173, acc 0.82
2016-05-20T18:56:02.663688: train-step 1184, loss 0.414006, acc 0.84
2016-05-20T18:56:11.907797: train-step 1185, loss 0.384537, acc 0.9
2016-05-20T18:56:20.920901: train-step 1186, loss 0.375125, acc 0.87
2016-05-20T18:56:29.790800: train-step 1187, loss 0.310001, acc 0.92
2016-05-20T18:56:38.397886: train-step 1188, loss 0.395007, acc 0.87
2016-05-20T18:56:46.469072: train-step 1189, loss 0.413236, acc 0.87
2016-05-20T18:56:55.632091: train-step 1190, loss 0.346226, acc 0.89
2016-05-20T18:57:03.075218: train-step 1191, loss 0.294733, acc 0.94
2016-05-20T18:57:10.718613: train-step 1192, loss 0.398508, acc 0.85
2016-05-20T18:57:18.662267: train-step 1193, loss 0.37542, acc 0.88
2016-05-20T18:57:26.629607: train-step 1194, loss 0.360556, acc 0.9
2016-05-20T18:57:34.203461: train-step 1195, loss 0.336311, acc 0.9
2016-05-20T18:57:42.029463: train-step 1196, loss 0.468411, acc 0.82
2016-05-20T18:57:49.999780: train-step 1197, loss 0.424741, acc 0.84
2016-05-20T18:57:58.562026: train-step 1198, loss 0.392274, acc 0.85
2016-05-20T18:58:07.828394: train-step 1199, loss 0.409242, acc 0.82
2016-05-20T18:58:16.792540: train-step 1200, loss 0.366152, acc 0.86
2016-05-20T18:58:25.906435: train-step 1201, loss 0.386631, acc 0.86
2016-05-20T18:58:35.037370: train-step 1202, loss 0.425187, acc 0.87
2016-05-20T18:58:44.443145: train-step 1203, loss 0.371221, acc 0.88
2016-05-20T18:58:53.664831: train-step 1204, loss 0.419814, acc 0.89
2016-05-20T18:59:02.841165: train-step 1205, loss 0.357269, acc 0.9
2016-05-20T18:59:12.000573: train-step 1206, loss 0.331474, acc 0.88
2016-05-20T18:59:21.434729: train-step 1207, loss 0.381157, acc 0.85
2016-05-20T18:59:30.592717: train-step 1208, loss 0.403842, acc 0.88
2016-05-20T18:59:39.649474: train-step 1209, loss 0.35518, acc 0.89
2016-05-20T18:59:48.528314: train-step 1210, loss 0.317786, acc 0.92
2016-05-20T18:59:57.265222: train-step 1211, loss 0.352678, acc 0.9
2016-05-20T19:00:05.245450: train-step 1212, loss 0.4244, acc 0.79
2016-05-20T19:00:15.115010: train-step 1213, loss 0.322127, acc 0.9
2016-05-20T19:00:22.875181: train-step 1214, loss 0.471514, acc 0.85
2016-05-20T19:00:30.778926: train-step 1215, loss 0.32608, acc 0.92
2016-05-20T19:00:38.626823: train-step 1216, loss 0.365947, acc 0.89
2016-05-20T19:00:46.448715: train-step 1217, loss 0.392654, acc 0.84
2016-05-20T19:00:54.412744: train-step 1218, loss 0.373413, acc 0.88
2016-05-20T19:01:02.574755: train-step 1219, loss 0.349163, acc 0.86
2016-05-20T19:01:11.292737: train-step 1220, loss 0.325044, acc 0.92
2016-05-20T19:01:20.693362: train-step 1221, loss 0.379118, acc 0.9
2016-05-20T19:01:29.740260: train-step 1222, loss 0.34992, acc 0.9
2016-05-20T19:01:39.092565: train-step 1223, loss 0.350731, acc 0.92
2016-05-20T19:01:48.062626: train-step 1224, loss 0.320337, acc 0.92
2016-05-20T19:01:57.459502: train-step 1225, loss 0.357371, acc 0.87
2016-05-20T19:02:06.539434: train-step 1226, loss 0.385577, acc 0.83
2016-05-20T19:02:15.944885: train-step 1227, loss 0.431202, acc 0.85
2016-05-20T19:02:25.217915: train-step 1228, loss 0.38165, acc 0.88
2016-05-20T19:02:34.607626: train-step 1229, loss 0.402157, acc 0.88
2016-05-20T19:02:43.904561: train-step 1230, loss 0.302163, acc 0.93
2016-05-20T19:02:53.210276: train-step 1231, loss 0.39653, acc 0.89
2016-05-20T19:03:02.474052: train-step 1232, loss 0.40585, acc 0.83
2016-05-20T19:03:11.324049: train-step 1233, loss 0.350063, acc 0.89
2016-05-20T19:03:19.424064: train-step 1234, loss 0.299326, acc 0.93
2016-05-20T19:03:28.718337: train-step 1235, loss 0.336017, acc 0.9
2016-05-20T19:03:36.245012: train-step 1236, loss 0.378356, acc 0.88
2016-05-20T19:03:44.035021: train-step 1237, loss 0.397341, acc 0.86
2016-05-20T19:03:51.949076: train-step 1238, loss 0.398574, acc 0.86
2016-05-20T19:03:59.726969: train-step 1239, loss 0.354721, acc 0.87
2016-05-20T19:04:07.523507: train-step 1240, loss 0.396911, acc 0.85
2016-05-20T19:04:15.654910: train-step 1241, loss 0.434347, acc 0.83
2016-05-20T19:04:24.254609: train-step 1242, loss 0.358871, acc 0.87
2016-05-20T19:04:33.233806: train-step 1243, loss 0.398545, acc 0.86
2016-05-20T19:04:42.399245: train-step 1244, loss 0.425668, acc 0.86
2016-05-20T19:04:51.425912: train-step 1245, loss 0.404008, acc 0.86
2016-05-20T19:05:00.904267: train-step 1246, loss 0.385831, acc 0.83
2016-05-20T19:05:09.830144: train-step 1247, loss 0.358476, acc 0.88
2016-05-20T19:05:19.215727: train-step 1248, loss 0.392902, acc 0.87
2016-05-20T19:05:28.521193: train-step 1249, loss 0.378661, acc 0.91
2016-05-20T19:05:37.609113: train-step 1250, loss 0.456777, acc 0.84
epoch number is: 5
2016-05-20T19:05:47.235317: train-step 1251, loss 0.344496, acc 0.89
2016-05-20T19:05:56.608622: train-step 1252, loss 0.306123, acc 0.92
2016-05-20T19:06:05.641224: train-step 1253, loss 0.327123, acc 0.94
2016-05-20T19:06:14.763406: train-step 1254, loss 0.311916, acc 0.96
2016-05-20T19:06:23.542515: train-step 1255, loss 0.390769, acc 0.85
2016-05-20T19:06:31.816339: train-step 1256, loss 0.36822, acc 0.84
2016-05-20T19:06:41.086123: train-step 1257, loss 0.370994, acc 0.84
2016-05-20T19:06:48.926744: train-step 1258, loss 0.368311, acc 0.88
2016-05-20T19:06:56.964301: train-step 1259, loss 0.322487, acc 0.93
2016-05-20T19:07:05.008515: train-step 1260, loss 0.368106, acc 0.89
2016-05-20T19:07:13.050850: train-step 1261, loss 0.386352, acc 0.87
2016-05-20T19:07:20.797892: train-step 1262, loss 0.479144, acc 0.82
2016-05-20T19:07:28.838578: train-step 1263, loss 0.386345, acc 0.83
2016-05-20T19:07:37.101846: train-step 1264, loss 0.348197, acc 0.91
2016-05-20T19:07:45.875183: train-step 1265, loss 0.343592, acc 0.9
2016-05-20T19:07:54.958627: train-step 1266, loss 0.359027, acc 0.89
2016-05-20T19:08:04.090985: train-step 1267, loss 0.341793, acc 0.87
2016-05-20T19:08:13.548691: train-step 1268, loss 0.343184, acc 0.88
2016-05-20T19:08:23.117195: train-step 1269, loss 0.433688, acc 0.84
2016-05-20T19:08:32.228474: train-step 1270, loss 0.361626, acc 0.87
2016-05-20T19:08:41.422181: train-step 1271, loss 0.37784, acc 0.86
2016-05-20T19:08:50.690962: train-step 1272, loss 0.356205, acc 0.87
2016-05-20T19:08:59.834391: train-step 1273, loss 0.293081, acc 0.91
2016-05-20T19:09:09.137593: train-step 1274, loss 0.346508, acc 0.92
2016-05-20T19:09:18.505646: train-step 1275, loss 0.44139, acc 0.85
2016-05-20T19:09:27.431775: train-step 1276, loss 0.349597, acc 0.9
2016-05-20T19:09:36.127645: train-step 1277, loss 0.325947, acc 0.91
2016-05-20T19:09:43.933602: train-step 1278, loss 0.451096, acc 0.84
2016-05-20T19:09:53.011857: train-step 1279, loss 0.339773, acc 0.89
2016-05-20T19:10:00.475145: train-step 1280, loss 0.277871, acc 0.92
2016-05-20T19:10:08.365009: train-step 1281, loss 0.306135, acc 0.91
2016-05-20T19:10:16.221629: train-step 1282, loss 0.394997, acc 0.86
2016-05-20T19:10:23.847550: train-step 1283, loss 0.341955, acc 0.91
2016-05-20T19:10:31.558827: train-step 1284, loss 0.38109, acc 0.87
2016-05-20T19:10:39.269406: train-step 1285, loss 0.348621, acc 0.91
2016-05-20T19:10:47.553105: train-step 1286, loss 0.308065, acc 0.92
2016-05-20T19:10:56.352654: train-step 1287, loss 0.390942, acc 0.86
2016-05-20T19:11:05.482613: train-step 1288, loss 0.399467, acc 0.87
2016-05-20T19:11:14.887499: train-step 1289, loss 0.423412, acc 0.84
2016-05-20T19:11:24.344266: train-step 1290, loss 0.341602, acc 0.9
2016-05-20T19:11:33.430153: train-step 1291, loss 0.330658, acc 0.89
2016-05-20T19:11:42.533963: train-step 1292, loss 0.406016, acc 0.88
2016-05-20T19:11:51.646003: train-step 1293, loss 0.417859, acc 0.86
2016-05-20T19:12:00.601522: train-step 1294, loss 0.373222, acc 0.88
2016-05-20T19:12:09.687461: train-step 1295, loss 0.410077, acc 0.85
2016-05-20T19:12:18.586202: train-step 1296, loss 0.322913, acc 0.92
2016-05-20T19:12:27.993882: train-step 1297, loss 0.476323, acc 0.83
2016-05-20T19:12:37.147199: train-step 1298, loss 0.39578, acc 0.84
2016-05-20T19:12:45.970897: train-step 1299, loss 0.430919, acc 0.84
2016-05-20T19:12:54.127715: train-step 1300, loss 0.390692, acc 0.89
2016-05-20T19:13:03.519805: train-step 1301, loss 0.39948, acc 0.86
2016-05-20T19:13:11.219210: train-step 1302, loss 0.352396, acc 0.9
2016-05-20T19:13:18.926556: train-step 1303, loss 0.386681, acc 0.85
2016-05-20T19:13:26.453391: train-step 1304, loss 0.431064, acc 0.81
2016-05-20T19:13:34.261050: train-step 1305, loss 0.439707, acc 0.88
2016-05-20T19:13:42.202995: train-step 1306, loss 0.378722, acc 0.88
2016-05-20T19:13:50.351955: train-step 1307, loss 0.344473, acc 0.87
2016-05-20T19:13:58.475454: train-step 1308, loss 0.494731, acc 0.79
2016-05-20T19:14:07.154941: train-step 1309, loss 0.390973, acc 0.86
2016-05-20T19:14:16.220994: train-step 1310, loss 0.38073, acc 0.85
2016-05-20T19:14:25.584116: train-step 1311, loss 0.346166, acc 0.88
2016-05-20T19:14:34.647988: train-step 1312, loss 0.43198, acc 0.85
2016-05-20T19:14:43.924395: train-step 1313, loss 0.388378, acc 0.86
2016-05-20T19:14:53.180086: train-step 1314, loss 0.359172, acc 0.87
2016-05-20T19:15:02.365530: train-step 1315, loss 0.36903, acc 0.91
2016-05-20T19:15:11.832657: train-step 1316, loss 0.355833, acc 0.91
2016-05-20T19:15:20.974350: train-step 1317, loss 0.4037, acc 0.88
2016-05-20T19:15:30.384593: train-step 1318, loss 0.335043, acc 0.92
2016-05-20T19:15:39.395159: train-step 1319, loss 0.358496, acc 0.92
2016-05-20T19:15:48.645987: train-step 1320, loss 0.297621, acc 0.98
2016-05-20T19:15:57.698691: train-step 1321, loss 0.324052, acc 0.91
2016-05-20T19:16:06.057605: train-step 1322, loss 0.417792, acc 0.84
2016-05-20T19:16:13.907065: train-step 1323, loss 0.449773, acc 0.81
2016-05-20T19:16:23.260730: train-step 1324, loss 0.389533, acc 0.88
2016-05-20T19:16:30.859359: train-step 1325, loss 0.331403, acc 0.9
2016-05-20T19:16:38.536421: train-step 1326, loss 0.448164, acc 0.8
2016-05-20T19:16:46.324661: train-step 1327, loss 0.396987, acc 0.85
2016-05-20T19:16:54.034738: train-step 1328, loss 0.325305, acc 0.92
2016-05-20T19:17:01.841240: train-step 1329, loss 0.401953, acc 0.86
2016-05-20T19:17:09.439923: train-step 1330, loss 0.398143, acc 0.88
2016-05-20T19:17:18.026415: train-step 1331, loss 0.366641, acc 0.88
2016-05-20T19:17:26.862980: train-step 1332, loss 0.403629, acc 0.88
2016-05-20T19:17:36.102794: train-step 1333, loss 0.376068, acc 0.91
2016-05-20T19:17:45.501717: train-step 1334, loss 0.285095, acc 0.94
2016-05-20T19:17:54.648360: train-step 1335, loss 0.331516, acc 0.87
2016-05-20T19:18:04.260893: train-step 1336, loss 0.399758, acc 0.86
2016-05-20T19:18:13.528844: train-step 1337, loss 0.368454, acc 0.87
2016-05-20T19:18:22.706601: train-step 1338, loss 0.349906, acc 0.91
2016-05-20T19:18:31.823327: train-step 1339, loss 0.381028, acc 0.88
2016-05-20T19:18:41.164422: train-step 1340, loss 0.383939, acc 0.88
2016-05-20T19:18:50.873580: train-step 1341, loss 0.289503, acc 0.93
2016-05-20T19:18:59.777177: train-step 1342, loss 0.41275, acc 0.9
2016-05-20T19:19:08.672471: train-step 1343, loss 0.384584, acc 0.87
2016-05-20T19:19:17.474903: train-step 1344, loss 0.348208, acc 0.94
2016-05-20T19:19:26.938495: train-step 1345, loss 0.299672, acc 0.92
2016-05-20T19:19:34.528664: train-step 1346, loss 0.367623, acc 0.86
2016-05-20T19:19:42.556926: train-step 1347, loss 0.349401, acc 0.91
2016-05-20T19:19:50.409140: train-step 1348, loss 0.381166, acc 0.88
2016-05-20T19:19:58.278772: train-step 1349, loss 0.317861, acc 0.9
2016-05-20T19:20:05.982651: train-step 1350, loss 0.376491, acc 0.87
2016-05-20T19:20:13.875687: train-step 1351, loss 0.329789, acc 0.88
2016-05-20T19:20:22.093903: train-step 1352, loss 0.370099, acc 0.89
2016-05-20T19:20:31.027983: train-step 1353, loss 0.36495, acc 0.88
2016-05-20T19:20:40.279344: train-step 1354, loss 0.350005, acc 0.88
2016-05-20T19:20:49.526977: train-step 1355, loss 0.332707, acc 0.87
2016-05-20T19:20:58.784405: train-step 1356, loss 0.375305, acc 0.9
2016-05-20T19:21:07.877893: train-step 1357, loss 0.343779, acc 0.88
2016-05-20T19:21:17.238542: train-step 1358, loss 0.446262, acc 0.84
2016-05-20T19:21:26.342085: train-step 1359, loss 0.403699, acc 0.84
2016-05-20T19:21:35.397352: train-step 1360, loss 0.345483, acc 0.89
2016-05-20T19:21:44.344549: train-step 1361, loss 0.33247, acc 0.91
2016-05-20T19:21:53.463194: train-step 1362, loss 0.354782, acc 0.88
2016-05-20T19:22:02.828002: train-step 1363, loss 0.317037, acc 0.93
2016-05-20T19:22:11.823704: train-step 1364, loss 0.3339, acc 0.9
2016-05-20T19:22:20.710198: train-step 1365, loss 0.352599, acc 0.89
2016-05-20T19:22:29.128143: train-step 1366, loss 0.387901, acc 0.84
2016-05-20T19:22:37.030550: train-step 1367, loss 0.389375, acc 0.87
2016-05-20T19:22:46.261391: train-step 1368, loss 0.400897, acc 0.89
2016-05-20T19:22:53.836099: train-step 1369, loss 0.285844, acc 0.92
2016-05-20T19:23:02.110385: train-step 1370, loss 0.347086, acc 0.87
2016-05-20T19:23:10.046783: train-step 1371, loss 0.369371, acc 0.87
2016-05-20T19:23:17.745478: train-step 1372, loss 0.418293, acc 0.84
2016-05-20T19:23:25.696041: train-step 1373, loss 0.370741, acc 0.9
2016-05-20T19:23:33.827496: train-step 1374, loss 0.34693, acc 0.88
2016-05-20T19:23:42.785251: train-step 1375, loss 0.384225, acc 0.87
2016-05-20T19:23:51.746086: train-step 1376, loss 0.392417, acc 0.89
2016-05-20T19:24:01.391053: train-step 1377, loss 0.412527, acc 0.87
2016-05-20T19:24:10.622076: train-step 1378, loss 0.4576, acc 0.83
2016-05-20T19:24:19.533580: train-step 1379, loss 0.349906, acc 0.89
2016-05-20T19:24:28.682300: train-step 1380, loss 0.39339, acc 0.86
2016-05-20T19:24:38.066268: train-step 1381, loss 0.421314, acc 0.81
2016-05-20T19:24:47.470713: train-step 1382, loss 0.330944, acc 0.91
2016-05-20T19:24:56.725713: train-step 1383, loss 0.274251, acc 0.95
2016-05-20T19:25:06.009566: train-step 1384, loss 0.388059, acc 0.87
2016-05-20T19:25:15.588799: train-step 1385, loss 0.378381, acc 0.86
2016-05-20T19:25:24.582747: train-step 1386, loss 0.361662, acc 0.84
2016-05-20T19:25:33.534807: train-step 1387, loss 0.317436, acc 0.91
2016-05-20T19:25:41.758041: train-step 1388, loss 0.277665, acc 0.96
2016-05-20T19:25:51.081671: train-step 1389, loss 0.346668, acc 0.89
2016-05-20T19:25:58.796401: train-step 1390, loss 0.378253, acc 0.89
2016-05-20T19:26:06.559529: train-step 1391, loss 0.418367, acc 0.82
2016-05-20T19:26:14.056050: train-step 1392, loss 0.277384, acc 0.95
2016-05-20T19:26:21.865370: train-step 1393, loss 0.307044, acc 0.92
2016-05-20T19:26:29.518409: train-step 1394, loss 0.445102, acc 0.86
2016-05-20T19:26:37.784676: train-step 1395, loss 0.388952, acc 0.86
2016-05-20T19:26:46.155103: train-step 1396, loss 0.349131, acc 0.86
2016-05-20T19:26:54.958707: train-step 1397, loss 0.440849, acc 0.84
2016-05-20T19:27:04.408318: train-step 1398, loss 0.351999, acc 0.87
2016-05-20T19:27:13.841774: train-step 1399, loss 0.330995, acc 0.9
2016-05-20T19:27:22.999297: train-step 1400, loss 0.362404, acc 0.87
2016-05-20T19:27:32.062451: train-step 1401, loss 0.291799, acc 0.92
2016-05-20T19:27:41.178055: train-step 1402, loss 0.433279, acc 0.85
2016-05-20T19:27:50.451885: train-step 1403, loss 0.391625, acc 0.83
2016-05-20T19:27:59.655399: train-step 1404, loss 0.360305, acc 0.9
2016-05-20T19:28:08.808520: train-step 1405, loss 0.344049, acc 0.86
2016-05-20T19:28:17.937148: train-step 1406, loss 0.346958, acc 0.87
2016-05-20T19:28:27.313267: train-step 1407, loss 0.427318, acc 0.86
2016-05-20T19:28:36.434800: train-step 1408, loss 0.306709, acc 0.9
2016-05-20T19:28:45.422908: train-step 1409, loss 0.32605, acc 0.91
2016-05-20T19:28:54.052971: train-step 1410, loss 0.315521, acc 0.89
2016-05-20T19:29:02.319986: train-step 1411, loss 0.312667, acc 0.9
2016-05-20T19:29:11.239412: train-step 1412, loss 0.349055, acc 0.91
2016-05-20T19:29:19.405751: train-step 1413, loss 0.389099, acc 0.89
2016-05-20T19:29:27.400407: train-step 1414, loss 0.34091, acc 0.91
2016-05-20T19:29:35.271293: train-step 1415, loss 0.366491, acc 0.9
2016-05-20T19:29:42.977923: train-step 1416, loss 0.423425, acc 0.84
2016-05-20T19:29:50.872415: train-step 1417, loss 0.407623, acc 0.83
2016-05-20T19:29:58.958742: train-step 1418, loss 0.318841, acc 0.9
2016-05-20T19:30:07.906422: train-step 1419, loss 0.350508, acc 0.87
2016-05-20T19:30:17.594064: train-step 1420, loss 0.377171, acc 0.86
2016-05-20T19:30:26.566490: train-step 1421, loss 0.456866, acc 0.83
2016-05-20T19:30:36.043868: train-step 1422, loss 0.383329, acc 0.84
2016-05-20T19:30:45.149502: train-step 1423, loss 0.341503, acc 0.91
2016-05-20T19:30:54.412685: train-step 1424, loss 0.356433, acc 0.9
2016-05-20T19:31:03.888285: train-step 1425, loss 0.421733, acc 0.82
2016-05-20T19:31:13.142995: train-step 1426, loss 0.355493, acc 0.88
2016-05-20T19:31:22.226868: train-step 1427, loss 0.363995, acc 0.87
2016-05-20T19:31:31.411049: train-step 1428, loss 0.372259, acc 0.87
2016-05-20T19:31:40.309047: train-step 1429, loss 0.353546, acc 0.89
2016-05-20T19:31:49.672906: train-step 1430, loss 0.459549, acc 0.81
2016-05-20T19:31:58.785512: train-step 1431, loss 0.400544, acc 0.87
2016-05-20T19:32:07.076606: train-step 1432, loss 0.30609, acc 0.9
2016-05-20T19:32:16.698053: train-step 1433, loss 0.36246, acc 0.88
2016-05-20T19:32:24.310847: train-step 1434, loss 0.305787, acc 0.94
2016-05-20T19:32:32.023583: train-step 1435, loss 0.348837, acc 0.9
2016-05-20T19:32:40.023857: train-step 1436, loss 0.386481, acc 0.88
2016-05-20T19:32:48.310193: train-step 1437, loss 0.338522, acc 0.9
2016-05-20T19:32:56.005040: train-step 1438, loss 0.398253, acc 0.85
2016-05-20T19:33:03.813592: train-step 1439, loss 0.377501, acc 0.87
2016-05-20T19:33:12.010293: train-step 1440, loss 0.376685, acc 0.91
2016-05-20T19:33:20.848021: train-step 1441, loss 0.376984, acc 0.92
2016-05-20T19:33:30.167175: train-step 1442, loss 0.392769, acc 0.87
2016-05-20T19:33:39.466099: train-step 1443, loss 0.288629, acc 0.94
2016-05-20T19:33:48.872537: train-step 1444, loss 0.442743, acc 0.81
2016-05-20T19:33:58.113753: train-step 1445, loss 0.34305, acc 0.88
2016-05-20T19:34:07.010709: train-step 1446, loss 0.425278, acc 0.84
2016-05-20T19:34:16.295338: train-step 1447, loss 0.356148, acc 0.9
2016-05-20T19:34:25.424321: train-step 1448, loss 0.355428, acc 0.87
2016-05-20T19:34:34.853105: train-step 1449, loss 0.34589, acc 0.89
2016-05-20T19:34:43.962165: train-step 1450, loss 0.397111, acc 0.86
2016-05-20T19:34:53.195382: train-step 1451, loss 0.392894, acc 0.85
2016-05-20T19:35:02.256225: train-step 1452, loss 0.326816, acc 0.88
2016-05-20T19:35:11.182551: train-step 1453, loss 0.369116, acc 0.85
2016-05-20T19:35:19.476803: train-step 1454, loss 0.361771, acc 0.9
2016-05-20T19:35:28.794975: train-step 1455, loss 0.264245, acc 0.95
2016-05-20T19:35:37.315509: train-step 1456, loss 0.426789, acc 0.85
2016-05-20T19:35:45.111267: train-step 1457, loss 0.329436, acc 0.89
2016-05-20T19:35:52.707197: train-step 1458, loss 0.352139, acc 0.9
2016-05-20T19:36:00.567185: train-step 1459, loss 0.310405, acc 0.92
2016-05-20T19:36:08.192676: train-step 1460, loss 0.340343, acc 0.87
2016-05-20T19:36:16.418938: train-step 1461, loss 0.336449, acc 0.9
2016-05-20T19:36:24.633230: train-step 1462, loss 0.394134, acc 0.85
2016-05-20T19:36:33.214852: train-step 1463, loss 0.410413, acc 0.85
2016-05-20T19:36:42.113505: train-step 1464, loss 0.405149, acc 0.85
2016-05-20T19:36:51.335867: train-step 1465, loss 0.307851, acc 0.94
2016-05-20T19:37:00.782076: train-step 1466, loss 0.387793, acc 0.89
2016-05-20T19:37:10.071781: train-step 1467, loss 0.440473, acc 0.8
2016-05-20T19:37:19.154351: train-step 1468, loss 0.391505, acc 0.85
2016-05-20T19:37:28.362167: train-step 1469, loss 0.341492, acc 0.91
2016-05-20T19:37:37.464916: train-step 1470, loss 0.32901, acc 0.88
2016-05-20T19:37:46.176797: train-step 1471, loss 0.405415, acc 0.85
2016-05-20T19:37:55.347191: train-step 1472, loss 0.374537, acc 0.84
2016-05-20T19:38:04.414064: train-step 1473, loss 0.360515, acc 0.88
2016-05-20T19:38:13.541845: train-step 1474, loss 0.392631, acc 0.84
2016-05-20T19:38:22.490438: train-step 1475, loss 0.366581, acc 0.91
2016-05-20T19:38:31.116972: train-step 1476, loss 0.366787, acc 0.87
2016-05-20T19:38:39.088351: train-step 1477, loss 0.310167, acc 0.91
2016-05-20T19:38:48.329131: train-step 1478, loss 0.380209, acc 0.85
2016-05-20T19:38:56.226396: train-step 1479, loss 0.358185, acc 0.87
2016-05-20T19:39:03.574018: train-step 1480, loss 0.30228, acc 0.9
2016-05-20T19:39:11.394519: train-step 1481, loss 0.379136, acc 0.87
2016-05-20T19:39:19.016967: train-step 1482, loss 0.312022, acc 0.93
2016-05-20T19:39:26.670784: train-step 1483, loss 0.379887, acc 0.87
2016-05-20T19:39:34.721217: train-step 1484, loss 0.398889, acc 0.86
2016-05-20T19:39:43.285865: train-step 1485, loss 0.310121, acc 0.92
2016-05-20T19:39:52.434110: train-step 1486, loss 0.319277, acc 0.91
2016-05-20T19:40:01.680820: train-step 1487, loss 0.326608, acc 0.93
2016-05-20T19:40:10.836775: train-step 1488, loss 0.336395, acc 0.92
2016-05-20T19:40:19.852578: train-step 1489, loss 0.353368, acc 0.88
2016-05-20T19:40:29.178627: train-step 1490, loss 0.340679, acc 0.92
2016-05-20T19:40:38.134211: train-step 1491, loss 0.361221, acc 0.89
2016-05-20T19:40:47.400890: train-step 1492, loss 0.368526, acc 0.85
2016-05-20T19:40:56.583373: train-step 1493, loss 0.324116, acc 0.91
2016-05-20T19:41:05.941936: train-step 1494, loss 0.333719, acc 0.89
2016-05-20T19:41:14.964093: train-step 1495, loss 0.360346, acc 0.88
2016-05-20T19:41:24.077802: train-step 1496, loss 0.341537, acc 0.92
2016-05-20T19:41:33.131846: train-step 1497, loss 0.35575, acc 0.88
2016-05-20T19:41:41.941363: train-step 1498, loss 0.285107, acc 0.94
2016-05-20T19:41:50.188893: train-step 1499, loss 0.316504, acc 0.91
2016-05-20T19:41:59.647137: train-step 1500, loss 0.365772, acc 0.86
epoch number is: 6
2016-05-20T19:42:07.711956: train-step 1501, loss 0.305592, acc 0.93
2016-05-20T19:42:15.528216: train-step 1502, loss 0.387419, acc 0.85
2016-05-20T19:42:24.183549: train-step 1503, loss 0.286661, acc 0.95
2016-05-20T19:42:31.958516: train-step 1504, loss 0.340022, acc 0.89
2016-05-20T19:42:39.866816: train-step 1505, loss 0.338303, acc 0.92
2016-05-20T19:42:47.649778: train-step 1506, loss 0.330146, acc 0.89
2016-05-20T19:42:56.056214: train-step 1507, loss 0.313338, acc 0.89
2016-05-20T19:43:04.807477: train-step 1508, loss 0.364904, acc 0.9
2016-05-20T19:43:13.981220: train-step 1509, loss 0.35055, acc 0.84
2016-05-20T19:43:23.246143: train-step 1510, loss 0.333056, acc 0.88
2016-05-20T19:43:32.689896: train-step 1511, loss 0.330537, acc 0.91
2016-05-20T19:43:42.001582: train-step 1512, loss 0.311827, acc 0.9
2016-05-20T19:43:51.343850: train-step 1513, loss 0.397553, acc 0.87
2016-05-20T19:44:00.318179: train-step 1514, loss 0.313677, acc 0.92
2016-05-20T19:44:09.554506: train-step 1515, loss 0.427911, acc 0.85
2016-05-20T19:44:18.868741: train-step 1516, loss 0.303234, acc 0.89
2016-05-20T19:44:27.831190: train-step 1517, loss 0.387294, acc 0.87
2016-05-20T19:44:37.143995: train-step 1518, loss 0.380225, acc 0.89
2016-05-20T19:44:46.321249: train-step 1519, loss 0.389643, acc 0.87
2016-05-20T19:44:55.525774: train-step 1520, loss 0.380487, acc 0.86
2016-05-20T19:45:04.093251: train-step 1521, loss 0.351521, acc 0.88
2016-05-20T19:45:12.339170: train-step 1522, loss 0.329329, acc 0.9
2016-05-20T19:45:21.370290: train-step 1523, loss 0.353515, acc 0.89
2016-05-20T19:45:29.263042: train-step 1524, loss 0.293695, acc 0.93
2016-05-20T19:45:37.139305: train-step 1525, loss 0.364082, acc 0.88
2016-05-20T19:45:45.243864: train-step 1526, loss 0.398721, acc 0.87
2016-05-20T19:45:53.013696: train-step 1527, loss 0.428171, acc 0.83
2016-05-20T19:46:00.825047: train-step 1528, loss 0.348804, acc 0.88
2016-05-20T19:46:09.186057: train-step 1529, loss 0.297984, acc 0.95
2016-05-20T19:46:18.095988: train-step 1530, loss 0.318569, acc 0.89
2016-05-20T19:46:27.263082: train-step 1531, loss 0.367, acc 0.83
2016-05-20T19:46:36.490374: train-step 1532, loss 0.318543, acc 0.91
2016-05-20T19:46:45.901029: train-step 1533, loss 0.426614, acc 0.83
2016-05-20T19:46:55.038157: train-step 1534, loss 0.382997, acc 0.85
2016-05-20T19:47:04.161381: train-step 1535, loss 0.236959, acc 0.98
2016-05-20T19:47:13.760892: train-step 1536, loss 0.261003, acc 0.96
2016-05-20T19:47:23.033947: train-step 1537, loss 0.42326, acc 0.83
2016-05-20T19:47:32.369868: train-step 1538, loss 0.326246, acc 0.92
2016-05-20T19:47:41.725867: train-step 1539, loss 0.351889, acc 0.89
2016-05-20T19:47:50.855644: train-step 1540, loss 0.37551, acc 0.87
2016-05-20T19:47:59.882375: train-step 1541, loss 0.308008, acc 0.92
2016-05-20T19:48:08.566485: train-step 1542, loss 0.347673, acc 0.9
2016-05-20T19:48:16.959257: train-step 1543, loss 0.337247, acc 0.87
2016-05-20T19:48:26.253346: train-step 1544, loss 0.346342, acc 0.92
2016-05-20T19:48:34.164865: train-step 1545, loss 0.417129, acc 0.85
2016-05-20T19:48:42.426921: train-step 1546, loss 0.346753, acc 0.89
2016-05-20T19:48:50.398546: train-step 1547, loss 0.419671, acc 0.86
2016-05-20T19:48:58.054018: train-step 1548, loss 0.323359, acc 0.91
2016-05-20T19:49:05.789034: train-step 1549, loss 0.434338, acc 0.84
2016-05-20T19:49:13.460507: train-step 1550, loss 0.33454, acc 0.88
2016-05-20T19:49:21.892254: train-step 1551, loss 0.310647, acc 0.91
2016-05-20T19:49:30.685340: train-step 1552, loss 0.382636, acc 0.85
2016-05-20T19:49:39.872450: train-step 1553, loss 0.342824, acc 0.91
2016-05-20T19:49:48.913398: train-step 1554, loss 0.344127, acc 0.88
2016-05-20T19:49:58.042951: train-step 1555, loss 0.308263, acc 0.91
2016-05-20T19:50:07.488883: train-step 1556, loss 0.323309, acc 0.92
2016-05-20T19:50:16.783184: train-step 1557, loss 0.396203, acc 0.87
2016-05-20T19:50:26.093953: train-step 1558, loss 0.337596, acc 0.89
2016-05-20T19:50:35.304270: train-step 1559, loss 0.353384, acc 0.91
2016-05-20T19:50:44.286277: train-step 1560, loss 0.362733, acc 0.88
2016-05-20T19:50:53.608389: train-step 1561, loss 0.418842, acc 0.86
2016-05-20T19:51:03.025391: train-step 1562, loss 0.377806, acc 0.85
2016-05-20T19:51:12.128933: train-step 1563, loss 0.34404, acc 0.9
2016-05-20T19:51:20.828736: train-step 1564, loss 0.359407, acc 0.87
2016-05-20T19:51:28.933234: train-step 1565, loss 0.365315, acc 0.86
2016-05-20T19:51:38.236886: train-step 1566, loss 0.385851, acc 0.85
2016-05-20T19:51:45.758133: train-step 1567, loss 0.302953, acc 0.94
2016-05-20T19:51:53.453392: train-step 1568, loss 0.398787, acc 0.86
2016-05-20T19:52:01.610643: train-step 1569, loss 0.31282, acc 0.92
2016-05-20T19:52:09.460757: train-step 1570, loss 0.370792, acc 0.88
2016-05-20T19:52:17.408986: train-step 1571, loss 0.386104, acc 0.88
2016-05-20T19:52:25.198201: train-step 1572, loss 0.356164, acc 0.89
2016-05-20T19:52:34.185286: train-step 1573, loss 0.296371, acc 0.93
2016-05-20T19:52:43.401220: train-step 1574, loss 0.329254, acc 0.91
2016-05-20T19:52:52.694765: train-step 1575, loss 0.313108, acc 0.91
2016-05-20T19:53:01.891542: train-step 1576, loss 0.265373, acc 0.94
2016-05-20T19:53:11.195856: train-step 1577, loss 0.347541, acc 0.88
2016-05-20T19:53:20.730005: train-step 1578, loss 0.276636, acc 0.93
2016-05-20T19:53:29.823590: train-step 1579, loss 0.347082, acc 0.88
2016-05-20T19:53:39.117564: train-step 1580, loss 0.329725, acc 0.95
2016-05-20T19:53:48.347355: train-step 1581, loss 0.30927, acc 0.93
2016-05-20T19:53:57.478511: train-step 1582, loss 0.335179, acc 0.91
2016-05-20T19:54:06.592536: train-step 1583, loss 0.315883, acc 0.9
2016-05-20T19:54:15.671243: train-step 1584, loss 0.360168, acc 0.87
2016-05-20T19:54:24.805849: train-step 1585, loss 0.400606, acc 0.87
2016-05-20T19:54:33.723622: train-step 1586, loss 0.314808, acc 0.91
2016-05-20T19:54:41.788470: train-step 1587, loss 0.335614, acc 0.89
2016-05-20T19:54:51.222096: train-step 1588, loss 0.344582, acc 0.88
2016-05-20T19:54:58.773512: train-step 1589, loss 0.296329, acc 0.94
2016-05-20T19:55:06.861311: train-step 1590, loss 0.330298, acc 0.87
2016-05-20T19:55:14.857628: train-step 1591, loss 0.368051, acc 0.87
2016-05-20T19:55:22.673492: train-step 1592, loss 0.373276, acc 0.88
2016-05-20T19:55:30.698952: train-step 1593, loss 0.335076, acc 0.88
2016-05-20T19:55:38.703724: train-step 1594, loss 0.422708, acc 0.87
2016-05-20T19:55:47.176171: train-step 1595, loss 0.355298, acc 0.9
2016-05-20T19:55:55.986712: train-step 1596, loss 0.380452, acc 0.82
2016-05-20T19:56:05.318816: train-step 1597, loss 0.339745, acc 0.87
2016-05-20T19:56:14.280964: train-step 1598, loss 0.318285, acc 0.89
2016-05-20T19:56:23.547259: train-step 1599, loss 0.334005, acc 0.9
2016-05-20T19:56:32.958915: train-step 1600, loss 0.344202, acc 0.91
2016-05-20T19:56:42.295539: train-step 1601, loss 0.329577, acc 0.89
2016-05-20T19:56:51.815844: train-step 1602, loss 0.418063, acc 0.85
2016-05-20T19:57:01.104840: train-step 1603, loss 0.36627, acc 0.88
2016-05-20T19:57:10.227283: train-step 1604, loss 0.328887, acc 0.89
2016-05-20T19:57:19.341409: train-step 1605, loss 0.386428, acc 0.83
2016-05-20T19:57:28.437739: train-step 1606, loss 0.301136, acc 0.92
2016-05-20T19:57:37.669673: train-step 1607, loss 0.356458, acc 0.89
2016-05-20T19:57:46.575541: train-step 1608, loss 0.288402, acc 0.92
2016-05-20T19:57:54.661079: train-step 1609, loss 0.407394, acc 0.84
2016-05-20T19:58:04.390077: train-step 1610, loss 0.460613, acc 0.81
2016-05-20T19:58:11.948687: train-step 1611, loss 0.386823, acc 0.85
2016-05-20T19:58:19.853494: train-step 1612, loss 0.266223, acc 0.95
2016-05-20T19:58:27.582359: train-step 1613, loss 0.308245, acc 0.88
2016-05-20T19:58:35.216870: train-step 1614, loss 0.344456, acc 0.87
2016-05-20T19:58:42.823317: train-step 1615, loss 0.300575, acc 0.93
2016-05-20T19:58:50.713313: train-step 1616, loss 0.325944, acc 0.87
2016-05-20T19:58:58.997498: train-step 1617, loss 0.320543, acc 0.9
2016-05-20T19:59:07.663058: train-step 1618, loss 0.25549, acc 0.94
2016-05-20T19:59:16.947692: train-step 1619, loss 0.262352, acc 0.93
2016-05-20T19:59:26.071450: train-step 1620, loss 0.339199, acc 0.9
2016-05-20T19:59:35.402905: train-step 1621, loss 0.365516, acc 0.88
2016-05-20T19:59:44.787502: train-step 1622, loss 0.281481, acc 0.94
2016-05-20T19:59:53.701800: train-step 1623, loss 0.326192, acc 0.92
2016-05-20T20:00:03.203088: train-step 1624, loss 0.345396, acc 0.89
2016-05-20T20:00:12.531838: train-step 1625, loss 0.333892, acc 0.86
2016-05-20T20:00:21.589169: train-step 1626, loss 0.426991, acc 0.83
2016-05-20T20:00:30.744005: train-step 1627, loss 0.354289, acc 0.87
2016-05-20T20:00:39.975351: train-step 1628, loss 0.357743, acc 0.89
2016-05-20T20:00:49.139141: train-step 1629, loss 0.405912, acc 0.83
2016-05-20T20:00:57.473371: train-step 1630, loss 0.32772, acc 0.91
2016-05-20T20:01:05.416978: train-step 1631, loss 0.347118, acc 0.89
2016-05-20T20:01:14.517505: train-step 1632, loss 0.313188, acc 0.92
2016-05-20T20:01:22.164087: train-step 1633, loss 0.42812, acc 0.81
2016-05-20T20:01:29.888047: train-step 1634, loss 0.3133, acc 0.9
2016-05-20T20:01:38.628366: train-step 1635, loss 0.366499, acc 0.91
2016-05-20T20:01:46.918478: train-step 1636, loss 0.341376, acc 0.91
2016-05-20T20:01:54.836411: train-step 1637, loss 0.356526, acc 0.87
2016-05-20T20:02:02.672323: train-step 1638, loss 0.324628, acc 0.91
2016-05-20T20:02:10.872518: train-step 1639, loss 0.361941, acc 0.93
2016-05-20T20:02:19.613766: train-step 1640, loss 0.300654, acc 0.92
2016-05-20T20:02:28.803791: train-step 1641, loss 0.333278, acc 0.92
2016-05-20T20:02:38.038532: train-step 1642, loss 0.299152, acc 0.91
2016-05-20T20:02:47.263984: train-step 1643, loss 0.368681, acc 0.85
2016-05-20T20:02:56.560961: train-step 1644, loss 0.307, acc 0.91
2016-05-20T20:03:05.920600: train-step 1645, loss 0.376206, acc 0.88
2016-05-20T20:03:15.105963: train-step 1646, loss 0.359623, acc 0.89
2016-05-20T20:03:24.380899: train-step 1647, loss 0.331168, acc 0.91
2016-05-20T20:03:33.356283: train-step 1648, loss 0.31256, acc 0.94
2016-05-20T20:03:42.543811: train-step 1649, loss 0.34945, acc 0.87
2016-05-20T20:03:51.626097: train-step 1650, loss 0.312275, acc 0.9
2016-05-20T20:04:00.912232: train-step 1651, loss 0.411806, acc 0.86
2016-05-20T20:04:09.706859: train-step 1652, loss 0.314812, acc 0.9
2016-05-20T20:04:17.832147: train-step 1653, loss 0.381439, acc 0.86
2016-05-20T20:04:27.199957: train-step 1654, loss 0.359662, acc 0.86
2016-05-20T20:04:34.806954: train-step 1655, loss 0.315201, acc 0.93
2016-05-20T20:04:42.522166: train-step 1656, loss 0.424288, acc 0.86
2016-05-20T20:04:50.593175: train-step 1657, loss 0.309478, acc 0.91
2016-05-20T20:04:58.443506: train-step 1658, loss 0.338507, acc 0.9
2016-05-20T20:05:05.962010: train-step 1659, loss 0.312657, acc 0.89
2016-05-20T20:05:14.149484: train-step 1660, loss 0.286949, acc 0.96
2016-05-20T20:05:22.214446: train-step 1661, loss 0.347867, acc 0.91
2016-05-20T20:05:31.091357: train-step 1662, loss 0.329527, acc 0.92
2016-05-20T20:05:40.092584: train-step 1663, loss 0.413548, acc 0.87
2016-05-20T20:05:49.103437: train-step 1664, loss 0.413379, acc 0.83
2016-05-20T20:05:58.285593: train-step 1665, loss 0.301127, acc 0.88
2016-05-20T20:06:07.169389: train-step 1666, loss 0.390823, acc 0.86
2016-05-20T20:06:16.488385: train-step 1667, loss 0.331617, acc 0.92
2016-05-20T20:06:25.533236: train-step 1668, loss 0.326926, acc 0.89
2016-05-20T20:06:34.901731: train-step 1669, loss 0.362697, acc 0.88
2016-05-20T20:06:44.167988: train-step 1670, loss 0.327386, acc 0.9
2016-05-20T20:06:53.073481: train-step 1671, loss 0.387549, acc 0.87
2016-05-20T20:07:02.202523: train-step 1672, loss 0.30702, acc 0.93
2016-05-20T20:07:11.374228: train-step 1673, loss 0.351377, acc 0.91
2016-05-20T20:07:20.544880: train-step 1674, loss 0.290502, acc 0.92
2016-05-20T20:07:28.955460: train-step 1675, loss 0.368841, acc 0.86
2016-05-20T20:07:36.940072: train-step 1676, loss 0.342446, acc 0.9
2016-05-20T20:07:46.317635: train-step 1677, loss 0.310204, acc 0.91
2016-05-20T20:07:54.501784: train-step 1678, loss 0.326809, acc 0.91
2016-05-20T20:08:02.275250: train-step 1679, loss 0.334812, acc 0.88
2016-05-20T20:08:10.160841: train-step 1680, loss 0.267924, acc 0.96
2016-05-20T20:08:17.817241: train-step 1681, loss 0.385755, acc 0.9
2016-05-20T20:08:25.533204: train-step 1682, loss 0.362593, acc 0.85
2016-05-20T20:08:33.809294: train-step 1683, loss 0.342637, acc 0.89
2016-05-20T20:08:42.297280: train-step 1684, loss 0.439353, acc 0.81
2016-05-20T20:08:51.198001: train-step 1685, loss 0.395413, acc 0.82
2016-05-20T20:09:00.109598: train-step 1686, loss 0.323682, acc 0.93
2016-05-20T20:09:09.413414: train-step 1687, loss 0.323498, acc 0.87
2016-05-20T20:09:18.592457: train-step 1688, loss 0.347484, acc 0.87
2016-05-20T20:09:27.708974: train-step 1689, loss 0.343205, acc 0.88
2016-05-20T20:09:37.036632: train-step 1690, loss 0.398877, acc 0.86
2016-05-20T20:09:46.459504: train-step 1691, loss 0.38538, acc 0.83
2016-05-20T20:09:55.665678: train-step 1692, loss 0.330121, acc 0.91
2016-05-20T20:10:04.939576: train-step 1693, loss 0.299636, acc 0.92
2016-05-20T20:10:14.120059: train-step 1694, loss 0.302136, acc 0.92
2016-05-20T20:10:23.269130: train-step 1695, loss 0.319394, acc 0.9
2016-05-20T20:10:32.207376: train-step 1696, loss 0.373747, acc 0.85
2016-05-20T20:10:41.072486: train-step 1697, loss 0.369311, acc 0.86
2016-05-20T20:10:49.222935: train-step 1698, loss 0.307459, acc 0.91
2016-05-20T20:10:58.643130: train-step 1699, loss 0.280504, acc 0.93
2016-05-20T20:11:06.335723: train-step 1700, loss 0.352518, acc 0.87
2016-05-20T20:11:14.244262: train-step 1701, loss 0.344068, acc 0.89
2016-05-20T20:11:21.723085: train-step 1702, loss 0.347251, acc 0.91
2016-05-20T20:11:29.698531: train-step 1703, loss 0.286596, acc 0.92
2016-05-20T20:11:37.841181: train-step 1704, loss 0.352194, acc 0.92
2016-05-20T20:11:45.634322: train-step 1705, loss 0.363301, acc 0.88
2016-05-20T20:11:53.777380: train-step 1706, loss 0.289457, acc 0.92
2016-05-20T20:12:02.911243: train-step 1707, loss 0.426707, acc 0.8
2016-05-20T20:12:12.252506: train-step 1708, loss 0.351459, acc 0.89
2016-05-20T20:12:21.555598: train-step 1709, loss 0.334234, acc 0.91
2016-05-20T20:12:30.661311: train-step 1710, loss 0.296676, acc 0.93
2016-05-20T20:12:39.965872: train-step 1711, loss 0.292658, acc 0.93
2016-05-20T20:12:49.044670: train-step 1712, loss 0.347374, acc 0.9
2016-05-20T20:12:58.289447: train-step 1713, loss 0.276689, acc 0.95
2016-05-20T20:13:07.556627: train-step 1714, loss 0.346709, acc 0.9
2016-05-20T20:13:16.898323: train-step 1715, loss 0.362313, acc 0.86
2016-05-20T20:13:26.290628: train-step 1716, loss 0.419092, acc 0.82
2016-05-20T20:13:35.571523: train-step 1717, loss 0.315141, acc 0.89
2016-05-20T20:13:44.577901: train-step 1718, loss 0.290011, acc 0.93
2016-05-20T20:13:53.343963: train-step 1719, loss 0.390232, acc 0.85
2016-05-20T20:14:01.607594: train-step 1720, loss 0.388007, acc 0.82
2016-05-20T20:14:10.903354: train-step 1721, loss 0.311764, acc 0.89
2016-05-20T20:14:18.710654: train-step 1722, loss 0.491398, acc 0.8
2016-05-20T20:14:26.418319: train-step 1723, loss 0.372907, acc 0.85
2016-05-20T20:14:34.370927: train-step 1724, loss 0.235078, acc 0.96
2016-05-20T20:14:41.966779: train-step 1725, loss 0.38237, acc 0.87
2016-05-20T20:14:49.806052: train-step 1726, loss 0.31976, acc 0.92
2016-05-20T20:14:57.662875: train-step 1727, loss 0.346665, acc 0.89
2016-05-20T20:15:06.423454: train-step 1728, loss 0.315484, acc 0.92
2016-05-20T20:15:15.153859: train-step 1729, loss 0.30339, acc 0.89
2016-05-20T20:15:24.210336: train-step 1730, loss 0.384752, acc 0.87
2016-05-20T20:15:33.395586: train-step 1731, loss 0.330248, acc 0.93
2016-05-20T20:15:42.647370: train-step 1732, loss 0.3999, acc 0.87
2016-05-20T20:15:51.914207: train-step 1733, loss 0.340794, acc 0.9
2016-05-20T20:16:01.104022: train-step 1734, loss 0.424938, acc 0.84
2016-05-20T20:16:10.226559: train-step 1735, loss 0.356546, acc 0.85
2016-05-20T20:16:19.490607: train-step 1736, loss 0.309168, acc 0.91
2016-05-20T20:16:28.832877: train-step 1737, loss 0.372919, acc 0.88
2016-05-20T20:16:38.183503: train-step 1738, loss 0.328924, acc 0.92
2016-05-20T20:16:47.338860: train-step 1739, loss 0.337155, acc 0.85
2016-05-20T20:16:56.269014: train-step 1740, loss 0.431547, acc 0.84
2016-05-20T20:17:05.153426: train-step 1741, loss 0.343307, acc 0.86
2016-05-20T20:17:13.314152: train-step 1742, loss 0.312415, acc 0.92
2016-05-20T20:17:22.805053: train-step 1743, loss 0.342254, acc 0.84
2016-05-20T20:17:30.592010: train-step 1744, loss 0.387663, acc 0.88
2016-05-20T20:17:38.568284: train-step 1745, loss 0.298807, acc 0.89
2016-05-20T20:17:46.552495: train-step 1746, loss 0.31117, acc 0.89
2016-05-20T20:17:54.260335: train-step 1747, loss 0.344328, acc 0.9
2016-05-20T20:18:01.891160: train-step 1748, loss 0.306739, acc 0.89
2016-05-20T20:18:10.044792: train-step 1749, loss 0.333565, acc 0.91
2016-05-20T20:18:18.081332: train-step 1750, loss 0.324866, acc 0.9
epoch number is: 7
2016-05-20T20:18:26.895477: train-step 1751, loss 0.30334, acc 0.93
2016-05-20T20:18:35.904028: train-step 1752, loss 0.349333, acc 0.88
2016-05-20T20:18:45.277435: train-step 1753, loss 0.302521, acc 0.94
2016-05-20T20:18:54.286096: train-step 1754, loss 0.255112, acc 0.94
2016-05-20T20:19:03.527221: train-step 1755, loss 0.288899, acc 0.95
2016-05-20T20:19:12.869911: train-step 1756, loss 0.356922, acc 0.92
2016-05-20T20:19:22.061933: train-step 1757, loss 0.290652, acc 0.91
2016-05-20T20:19:31.157633: train-step 1758, loss 0.312081, acc 0.94
2016-05-20T20:19:40.271947: train-step 1759, loss 0.317081, acc 0.9
2016-05-20T20:19:49.091176: train-step 1760, loss 0.320286, acc 0.93
2016-05-20T20:19:58.284587: train-step 1761, loss 0.347329, acc 0.9
2016-05-20T20:20:07.524871: train-step 1762, loss 0.353461, acc 0.9
2016-05-20T20:20:16.562200: train-step 1763, loss 0.335839, acc 0.9
2016-05-20T20:20:24.787610: train-step 1764, loss 0.322363, acc 0.89
2016-05-20T20:20:32.975895: train-step 1765, loss 0.329166, acc 0.88
2016-05-20T20:20:43.173888: train-step 1766, loss 0.321905, acc 0.9
2016-05-20T20:20:51.054709: train-step 1767, loss 0.360673, acc 0.86
2016-05-20T20:20:58.832491: train-step 1768, loss 0.395407, acc 0.83
2016-05-20T20:21:06.389103: train-step 1769, loss 0.309322, acc 0.88
2016-05-20T20:21:14.022113: train-step 1770, loss 0.342896, acc 0.89
2016-05-20T20:21:21.709424: train-step 1771, loss 0.267679, acc 0.93
2016-05-20T20:21:29.668641: train-step 1772, loss 0.296688, acc 0.91
2016-05-20T20:21:37.950498: train-step 1773, loss 0.393316, acc 0.89
2016-05-20T20:21:46.893814: train-step 1774, loss 0.288063, acc 0.93
2016-05-20T20:21:56.306127: train-step 1775, loss 0.344777, acc 0.88
2016-05-20T20:22:05.522571: train-step 1776, loss 0.339981, acc 0.91
2016-05-20T20:22:14.533833: train-step 1777, loss 0.296572, acc 0.93
2016-05-20T20:22:23.851681: train-step 1778, loss 0.358505, acc 0.86
2016-05-20T20:22:33.071367: train-step 1779, loss 0.292124, acc 0.92
2016-05-20T20:22:42.214892: train-step 1780, loss 0.252152, acc 0.94
2016-05-20T20:22:51.220268: train-step 1781, loss 0.360462, acc 0.89
2016-05-20T20:23:00.787337: train-step 1782, loss 0.385236, acc 0.83
2016-05-20T20:23:10.286535: train-step 1783, loss 0.257072, acc 0.93
2016-05-20T20:23:19.459171: train-step 1784, loss 0.272141, acc 0.92
2016-05-20T20:23:28.602220: train-step 1785, loss 0.363497, acc 0.87
2016-05-20T20:23:37.381081: train-step 1786, loss 0.330779, acc 0.91
2016-05-20T20:23:45.467547: train-step 1787, loss 0.324123, acc 0.92
2016-05-20T20:23:54.754133: train-step 1788, loss 0.343488, acc 0.88
2016-05-20T20:24:02.229590: train-step 1789, loss 0.315855, acc 0.91
2016-05-20T20:24:10.127887: train-step 1790, loss 0.294041, acc 0.93
2016-05-20T20:24:18.120233: train-step 1791, loss 0.308913, acc 0.89
2016-05-20T20:24:25.964599: train-step 1792, loss 0.303789, acc 0.91
2016-05-20T20:24:33.772046: train-step 1793, loss 0.334411, acc 0.91
2016-05-20T20:24:41.759391: train-step 1794, loss 0.379763, acc 0.9
2016-05-20T20:24:50.202031: train-step 1795, loss 0.35129, acc 0.9
2016-05-20T20:24:59.112740: train-step 1796, loss 0.383922, acc 0.86
2016-05-20T20:25:08.304890: train-step 1797, loss 0.380525, acc 0.85
2016-05-20T20:25:17.762400: train-step 1798, loss 0.429872, acc 0.83
2016-05-20T20:25:27.314078: train-step 1799, loss 0.360876, acc 0.89
2016-05-20T20:25:36.554151: train-step 1800, loss 0.336819, acc 0.87
2016-05-20T20:25:45.664497: train-step 1801, loss 0.324553, acc 0.89
2016-05-20T20:25:55.078468: train-step 1802, loss 0.400143, acc 0.84
2016-05-20T20:26:04.579168: train-step 1803, loss 0.314639, acc 0.91
2016-05-20T20:26:14.045627: train-step 1804, loss 0.316226, acc 0.91
2016-05-20T20:26:23.200100: train-step 1805, loss 0.358094, acc 0.89
2016-05-20T20:26:32.409258: train-step 1806, loss 0.299116, acc 0.91
2016-05-20T20:26:41.508800: train-step 1807, loss 0.337738, acc 0.87
2016-05-20T20:26:49.983321: train-step 1808, loss 0.291976, acc 0.92
2016-05-20T20:26:57.988336: train-step 1809, loss 0.301956, acc 0.89
2016-05-20T20:27:07.216127: train-step 1810, loss 0.362935, acc 0.91
2016-05-20T20:27:15.315834: train-step 1811, loss 0.281873, acc 0.92
2016-05-20T20:27:23.096307: train-step 1812, loss 0.331645, acc 0.88
2016-05-20T20:27:31.287834: train-step 1813, loss 0.324985, acc 0.9
2016-05-20T20:27:39.055610: train-step 1814, loss 0.290527, acc 0.93
2016-05-20T20:27:46.864466: train-step 1815, loss 0.334965, acc 0.87
2016-05-20T20:27:54.699128: train-step 1816, loss 0.302344, acc 0.89
2016-05-20T20:28:03.634128: train-step 1817, loss 0.38669, acc 0.84
2016-05-20T20:28:12.625216: train-step 1818, loss 0.398494, acc 0.84
2016-05-20T20:28:21.763256: train-step 1819, loss 0.276869, acc 0.89
2016-05-20T20:28:31.115451: train-step 1820, loss 0.300394, acc 0.92
2016-05-20T20:28:40.612884: train-step 1821, loss 0.345664, acc 0.88
2016-05-20T20:28:49.966181: train-step 1822, loss 0.3176, acc 0.91
2016-05-20T20:28:59.348889: train-step 1823, loss 0.277729, acc 0.91
2016-05-20T20:29:08.594678: train-step 1824, loss 0.265569, acc 0.95
2016-05-20T20:29:18.006153: train-step 1825, loss 0.342401, acc 0.9
2016-05-20T20:29:27.208083: train-step 1826, loss 0.288542, acc 0.91
2016-05-20T20:29:36.496459: train-step 1827, loss 0.301145, acc 0.93
2016-05-20T20:29:45.485695: train-step 1828, loss 0.276835, acc 0.9
2016-05-20T20:29:54.472519: train-step 1829, loss 0.327811, acc 0.88
2016-05-20T20:30:02.760514: train-step 1830, loss 0.31866, acc 0.9
2016-05-20T20:30:11.952749: train-step 1831, loss 0.351944, acc 0.89
2016-05-20T20:30:19.619554: train-step 1832, loss 0.312439, acc 0.91
2016-05-20T20:30:27.880677: train-step 1833, loss 0.326788, acc 0.9
2016-05-20T20:30:35.317475: train-step 1834, loss 0.373539, acc 0.87
2016-05-20T20:30:43.041349: train-step 1835, loss 0.344479, acc 0.88
2016-05-20T20:30:50.647056: train-step 1836, loss 0.298534, acc 0.94
2016-05-20T20:30:58.549622: train-step 1837, loss 0.263931, acc 0.94
2016-05-20T20:31:06.774231: train-step 1838, loss 0.32926, acc 0.9
2016-05-20T20:31:15.563791: train-step 1839, loss 0.312313, acc 0.91
2016-05-20T20:31:24.375659: train-step 1840, loss 0.265123, acc 0.94
2016-05-20T20:31:33.702862: train-step 1841, loss 0.3512, acc 0.9
2016-05-20T20:31:43.136598: train-step 1842, loss 0.29437, acc 0.93
2016-05-20T20:31:52.388192: train-step 1843, loss 0.321578, acc 0.9
2016-05-20T20:32:01.855805: train-step 1844, loss 0.301646, acc 0.94
2016-05-20T20:32:11.577516: train-step 1845, loss 0.413416, acc 0.83
2016-05-20T20:32:20.918521: train-step 1846, loss 0.33359, acc 0.9
2016-05-20T20:32:30.053663: train-step 1847, loss 0.301944, acc 0.91
2016-05-20T20:32:38.971613: train-step 1848, loss 0.288658, acc 0.91
2016-05-20T20:32:48.168152: train-step 1849, loss 0.400604, acc 0.85
2016-05-20T20:32:57.343335: train-step 1850, loss 0.351205, acc 0.9
2016-05-20T20:33:06.185630: train-step 1851, loss 0.436297, acc 0.82
2016-05-20T20:33:14.422040: train-step 1852, loss 0.370462, acc 0.87
2016-05-20T20:33:22.622277: train-step 1853, loss 0.313246, acc 0.92
2016-05-20T20:33:31.396950: train-step 1854, loss 0.293701, acc 0.9
2016-05-20T20:33:39.124677: train-step 1855, loss 0.347166, acc 0.88
2016-05-20T20:33:47.099484: train-step 1856, loss 0.320513, acc 0.88
2016-05-20T20:33:54.968433: train-step 1857, loss 0.307337, acc 0.92
2016-05-20T20:34:02.685886: train-step 1858, loss 0.305945, acc 0.92
2016-05-20T20:34:10.366895: train-step 1859, loss 0.250392, acc 0.94
2016-05-20T20:34:18.495155: train-step 1860, loss 0.298844, acc 0.91
2016-05-20T20:34:27.404424: train-step 1861, loss 0.349811, acc 0.88
2016-05-20T20:34:36.231032: train-step 1862, loss 0.369896, acc 0.87
2016-05-20T20:34:45.311407: train-step 1863, loss 0.291447, acc 0.93
2016-05-20T20:34:54.671200: train-step 1864, loss 0.31289, acc 0.9
2016-05-20T20:35:03.853379: train-step 1865, loss 0.388285, acc 0.84
2016-05-20T20:35:13.097487: train-step 1866, loss 0.336113, acc 0.89
2016-05-20T20:35:22.313460: train-step 1867, loss 0.293241, acc 0.92
2016-05-20T20:35:31.612024: train-step 1868, loss 0.434132, acc 0.81
2016-05-20T20:35:40.771392: train-step 1869, loss 0.344107, acc 0.86
2016-05-20T20:35:50.242865: train-step 1870, loss 0.444342, acc 0.78
2016-05-20T20:35:59.396054: train-step 1871, loss 0.304508, acc 0.91
2016-05-20T20:36:08.502803: train-step 1872, loss 0.298622, acc 0.91
2016-05-20T20:36:17.435448: train-step 1873, loss 0.296339, acc 0.91
2016-05-20T20:36:26.083113: train-step 1874, loss 0.34131, acc 0.88
2016-05-20T20:36:34.215961: train-step 1875, loss 0.300051, acc 0.92
2016-05-20T20:36:42.890788: train-step 1876, loss 0.318012, acc 0.86
2016-05-20T20:36:50.387896: train-step 1877, loss 0.290228, acc 0.92
2016-05-20T20:36:58.632495: train-step 1878, loss 0.38741, acc 0.87
2016-05-20T20:37:06.631717: train-step 1879, loss 0.381871, acc 0.86
2016-05-20T20:37:14.267558: train-step 1880, loss 0.297691, acc 0.91
2016-05-20T20:37:22.243437: train-step 1881, loss 0.300012, acc 0.94
2016-05-20T20:37:30.548929: train-step 1882, loss 0.317503, acc 0.89
2016-05-20T20:37:39.190153: train-step 1883, loss 0.342555, acc 0.91
2016-05-20T20:37:48.244196: train-step 1884, loss 0.368252, acc 0.88
2016-05-20T20:37:57.237082: train-step 1885, loss 0.289649, acc 0.92
2016-05-20T20:38:06.553796: train-step 1886, loss 0.233885, acc 0.95
2016-05-20T20:38:16.273497: train-step 1887, loss 0.307536, acc 0.91
2016-05-20T20:38:25.704775: train-step 1888, loss 0.346645, acc 0.87
2016-05-20T20:38:34.765901: train-step 1889, loss 0.303422, acc 0.88
2016-05-20T20:38:43.917461: train-step 1890, loss 0.389553, acc 0.87
2016-05-20T20:38:53.223557: train-step 1891, loss 0.296175, acc 0.91
2016-05-20T20:39:02.421847: train-step 1892, loss 0.342722, acc 0.9
2016-05-20T20:39:11.457661: train-step 1893, loss 0.299861, acc 0.91
2016-05-20T20:39:20.761858: train-step 1894, loss 0.32381, acc 0.89
2016-05-20T20:39:29.580301: train-step 1895, loss 0.323478, acc 0.91
2016-05-20T20:39:37.948370: train-step 1896, loss 0.38836, acc 0.88
2016-05-20T20:39:46.349415: train-step 1897, loss 0.2492, acc 0.96
2016-05-20T20:39:54.930173: train-step 1898, loss 0.337295, acc 0.87
2016-05-20T20:40:02.836991: train-step 1899, loss 0.347204, acc 0.89
2016-05-20T20:40:10.894835: train-step 1900, loss 0.314774, acc 0.92
2016-05-20T20:40:18.833209: train-step 1901, loss 0.271537, acc 0.95
2016-05-20T20:40:26.650827: train-step 1902, loss 0.34053, acc 0.89
2016-05-20T20:40:34.682817: train-step 1903, loss 0.318544, acc 0.92
2016-05-20T20:40:42.507790: train-step 1904, loss 0.257719, acc 0.95
2016-05-20T20:40:50.993822: train-step 1905, loss 0.399547, acc 0.81
2016-05-20T20:41:00.089884: train-step 1906, loss 0.359669, acc 0.85
2016-05-20T20:41:09.128763: train-step 1907, loss 0.353972, acc 0.92
2016-05-20T20:41:18.235577: train-step 1908, loss 0.315829, acc 0.88
2016-05-20T20:41:27.568185: train-step 1909, loss 0.392739, acc 0.85
2016-05-20T20:41:36.698262: train-step 1910, loss 0.330053, acc 0.88
2016-05-20T20:41:45.769548: train-step 1911, loss 0.289943, acc 0.91
2016-05-20T20:41:54.829996: train-step 1912, loss 0.385889, acc 0.85
2016-05-20T20:42:04.053734: train-step 1913, loss 0.36116, acc 0.89
2016-05-20T20:42:13.299537: train-step 1914, loss 0.366794, acc 0.84
2016-05-20T20:42:22.392336: train-step 1915, loss 0.350075, acc 0.9
2016-05-20T20:42:31.477698: train-step 1916, loss 0.360085, acc 0.88
2016-05-20T20:42:40.450737: train-step 1917, loss 0.349512, acc 0.89
2016-05-20T20:42:49.190710: train-step 1918, loss 0.345172, acc 0.86
2016-05-20T20:42:57.197030: train-step 1919, loss 0.255022, acc 0.97
2016-05-20T20:43:06.781512: train-step 1920, loss 0.362606, acc 0.88
2016-05-20T20:43:14.397228: train-step 1921, loss 0.323846, acc 0.86
2016-05-20T20:43:22.897924: train-step 1922, loss 0.321877, acc 0.9
2016-05-20T20:43:31.388963: train-step 1923, loss 0.275181, acc 0.92
2016-05-20T20:43:39.424049: train-step 1924, loss 0.397851, acc 0.83
2016-05-20T20:43:47.278890: train-step 1925, loss 0.352399, acc 0.88
2016-05-20T20:43:55.316237: train-step 1926, loss 0.408317, acc 0.83
2016-05-20T20:44:04.032752: train-step 1927, loss 0.331143, acc 0.9
2016-05-20T20:44:13.024534: train-step 1928, loss 0.298911, acc 0.91
2016-05-20T20:44:22.465891: train-step 1929, loss 0.356388, acc 0.84
2016-05-20T20:44:31.563598: train-step 1930, loss 0.346533, acc 0.89
2016-05-20T20:44:40.744767: train-step 1931, loss 0.355558, acc 0.93
2016-05-20T20:44:49.995710: train-step 1932, loss 0.325368, acc 0.9
2016-05-20T20:44:59.082178: train-step 1933, loss 0.337414, acc 0.89
2016-05-20T20:45:08.180647: train-step 1934, loss 0.42255, acc 0.86
2016-05-20T20:45:17.548157: train-step 1935, loss 0.408736, acc 0.89
2016-05-20T20:45:26.574194: train-step 1936, loss 0.282944, acc 0.9
2016-05-20T20:45:35.877509: train-step 1937, loss 0.391792, acc 0.88
2016-05-20T20:45:45.087642: train-step 1938, loss 0.335946, acc 0.89
2016-05-20T20:45:53.876169: train-step 1939, loss 0.375795, acc 0.83
2016-05-20T20:46:02.588680: train-step 1940, loss 0.275807, acc 0.93
2016-05-20T20:46:10.783317: train-step 1941, loss 0.336917, acc 0.9
2016-05-20T20:46:20.412562: train-step 1942, loss 0.312892, acc 0.92
2016-05-20T20:46:28.035965: train-step 1943, loss 0.319826, acc 0.91
2016-05-20T20:46:35.742412: train-step 1944, loss 0.403319, acc 0.88
2016-05-20T20:46:43.733217: train-step 1945, loss 0.289814, acc 0.9
2016-05-20T20:46:51.666137: train-step 1946, loss 0.365001, acc 0.86
2016-05-20T20:46:59.296408: train-step 1947, loss 0.277959, acc 0.95
2016-05-20T20:47:07.374681: train-step 1948, loss 0.382411, acc 0.87
2016-05-20T20:47:15.636112: train-step 1949, loss 0.399888, acc 0.84
2016-05-20T20:47:24.936375: train-step 1950, loss 0.309229, acc 0.94
2016-05-20T20:47:34.132134: train-step 1951, loss 0.320397, acc 0.87
2016-05-20T20:47:43.042630: train-step 1952, loss 0.390945, acc 0.85
2016-05-20T20:47:52.115924: train-step 1953, loss 0.347205, acc 0.9
2016-05-20T20:48:01.226263: train-step 1954, loss 0.340305, acc 0.9
2016-05-20T20:48:10.564220: train-step 1955, loss 0.357178, acc 0.83
2016-05-20T20:48:19.867491: train-step 1956, loss 0.298265, acc 0.91
2016-05-20T20:48:29.209954: train-step 1957, loss 0.32511, acc 0.86
2016-05-20T20:48:38.590361: train-step 1958, loss 0.325326, acc 0.93
2016-05-20T20:48:47.785658: train-step 1959, loss 0.372433, acc 0.88
2016-05-20T20:48:57.027997: train-step 1960, loss 0.314917, acc 0.91
2016-05-20T20:49:06.234182: train-step 1961, loss 0.390482, acc 0.83
2016-05-20T20:49:15.071611: train-step 1962, loss 0.38216, acc 0.91
2016-05-20T20:49:23.570897: train-step 1963, loss 0.428824, acc 0.87
2016-05-20T20:49:32.819311: train-step 1964, loss 0.355481, acc 0.88
2016-05-20T20:49:41.067328: train-step 1965, loss 0.25456, acc 0.96
2016-05-20T20:49:49.412293: train-step 1966, loss 0.347931, acc 0.88
2016-05-20T20:49:57.145937: train-step 1967, loss 0.361408, acc 0.88
2016-05-20T20:50:05.110505: train-step 1968, loss 0.340008, acc 0.9
2016-05-20T20:50:12.714987: train-step 1969, loss 0.364968, acc 0.87
2016-05-20T20:50:21.112074: train-step 1970, loss 0.299157, acc 0.92
2016-05-20T20:50:29.349597: train-step 1971, loss 0.301315, acc 0.93
2016-05-20T20:50:38.225616: train-step 1972, loss 0.361921, acc 0.88
2016-05-20T20:50:47.215709: train-step 1973, loss 0.361168, acc 0.86
2016-05-20T20:50:56.391192: train-step 1974, loss 0.306428, acc 0.91
2016-05-20T20:51:05.674466: train-step 1975, loss 0.339798, acc 0.86
2016-05-20T20:51:15.011318: train-step 1976, loss 0.279951, acc 0.94
2016-05-20T20:51:23.997815: train-step 1977, loss 0.331288, acc 0.87
2016-05-20T20:51:33.442272: train-step 1978, loss 0.327547, acc 0.87
2016-05-20T20:51:42.585233: train-step 1979, loss 0.358969, acc 0.88
2016-05-20T20:51:51.797529: train-step 1980, loss 0.289753, acc 0.94
2016-05-20T20:52:00.760756: train-step 1981, loss 0.277335, acc 0.94
2016-05-20T20:52:09.974508: train-step 1982, loss 0.365076, acc 0.87
2016-05-20T20:52:19.111128: train-step 1983, loss 0.31155, acc 0.91
2016-05-20T20:52:28.298318: train-step 1984, loss 0.311875, acc 0.89
2016-05-20T20:52:36.833621: train-step 1985, loss 0.328496, acc 0.89
2016-05-20T20:52:46.159989: train-step 1986, loss 0.370791, acc 0.88
2016-05-20T20:52:54.256693: train-step 1987, loss 0.314593, acc 0.9
2016-05-20T20:53:01.972346: train-step 1988, loss 0.309027, acc 0.91
2016-05-20T20:53:09.831807: train-step 1989, loss 0.320988, acc 0.94
2016-05-20T20:53:17.817727: train-step 1990, loss 0.368469, acc 0.84
2016-05-20T20:53:25.696830: train-step 1991, loss 0.311035, acc 0.91
2016-05-20T20:53:33.833568: train-step 1992, loss 0.325626, acc 0.91
2016-05-20T20:53:41.553936: train-step 1993, loss 0.297518, acc 0.92
2016-05-20T20:53:49.929921: train-step 1994, loss 0.335594, acc 0.89
2016-05-20T20:53:59.087455: train-step 1995, loss 0.357642, acc 0.88
2016-05-20T20:54:08.279950: train-step 1996, loss 0.326657, acc 0.9
2016-05-20T20:54:17.413088: train-step 1997, loss 0.347961, acc 0.89
2016-05-20T20:54:26.874746: train-step 1998, loss 0.389976, acc 0.86
2016-05-20T20:54:36.069506: train-step 1999, loss 0.292889, acc 0.92
2016-05-20T20:54:45.399554: train-step 2000, loss 0.344676, acc 0.9
epoch number is: 8
2016-05-20T20:54:54.659252: train-step 2001, loss 0.354371, acc 0.88
2016-05-20T20:55:04.039365: train-step 2002, loss 0.297601, acc 0.92
2016-05-20T20:55:13.167118: train-step 2003, loss 0.290626, acc 0.92
2016-05-20T20:55:22.439505: train-step 2004, loss 0.315333, acc 0.9
2016-05-20T20:55:31.573114: train-step 2005, loss 0.288309, acc 0.93
2016-05-20T20:55:40.573042: train-step 2006, loss 0.315828, acc 0.91
2016-05-20T20:55:49.312684: train-step 2007, loss 0.312217, acc 0.93
2016-05-20T20:55:57.586981: train-step 2008, loss 0.296301, acc 0.92
2016-05-20T20:56:07.004447: train-step 2009, loss 0.348147, acc 0.85
2016-05-20T20:56:14.600464: train-step 2010, loss 0.266515, acc 0.92
2016-05-20T20:56:22.699269: train-step 2011, loss 0.358151, acc 0.89
2016-05-20T20:56:30.568188: train-step 2012, loss 0.303261, acc 0.92
2016-05-20T20:56:38.577218: train-step 2013, loss 0.34075, acc 0.9
2016-05-20T20:56:46.405302: train-step 2014, loss 0.362483, acc 0.86
2016-05-20T20:56:54.265152: train-step 2015, loss 0.263994, acc 0.94
2016-05-20T20:57:02.529255: train-step 2016, loss 0.350731, acc 0.89
2016-05-20T20:57:11.697131: train-step 2017, loss 0.273192, acc 0.9
2016-05-20T20:57:21.199112: train-step 2018, loss 0.389441, acc 0.91
2016-05-20T20:57:30.477004: train-step 2019, loss 0.304365, acc 0.89
2016-05-20T20:57:39.811756: train-step 2020, loss 0.327792, acc 0.88
2016-05-20T20:57:49.140818: train-step 2021, loss 0.338705, acc 0.91
2016-05-20T20:57:58.370805: train-step 2022, loss 0.358746, acc 0.88
2016-05-20T20:58:07.728496: train-step 2023, loss 0.354966, acc 0.85
2016-05-20T20:58:16.908313: train-step 2024, loss 0.319307, acc 0.89
2016-05-20T20:58:25.922889: train-step 2025, loss 0.355215, acc 0.91
2016-05-20T20:58:35.014591: train-step 2026, loss 0.234965, acc 0.95
2016-05-20T20:58:44.181469: train-step 2027, loss 0.293707, acc 0.95
2016-05-20T20:58:53.185906: train-step 2028, loss 0.326814, acc 0.9
2016-05-20T20:59:01.966692: train-step 2029, loss 0.310853, acc 0.92
2016-05-20T20:59:10.370688: train-step 2030, loss 0.320945, acc 0.86
2016-05-20T20:59:19.467604: train-step 2031, loss 0.36527, acc 0.85
2016-05-20T20:59:27.861190: train-step 2032, loss 0.319601, acc 0.92
2016-05-20T20:59:35.889848: train-step 2033, loss 0.30444, acc 0.91
2016-05-20T20:59:43.507629: train-step 2034, loss 0.284741, acc 0.93
2016-05-20T20:59:51.541891: train-step 2035, loss 0.312564, acc 0.87
2016-05-20T20:59:59.833957: train-step 2036, loss 0.313842, acc 0.91
2016-05-20T21:00:07.725677: train-step 2037, loss 0.283195, acc 0.92
2016-05-20T21:00:15.564737: train-step 2038, loss 0.337606, acc 0.86
2016-05-20T21:00:24.401676: train-step 2039, loss 0.310733, acc 0.89
2016-05-20T21:00:33.626434: train-step 2040, loss 0.329352, acc 0.91
2016-05-20T21:00:42.840616: train-step 2041, loss 0.332797, acc 0.89
2016-05-20T21:00:52.042961: train-step 2042, loss 0.30315, acc 0.9
2016-05-20T21:01:01.316464: train-step 2043, loss 0.294098, acc 0.9
2016-05-20T21:01:10.711617: train-step 2044, loss 0.344117, acc 0.84
2016-05-20T21:01:19.921138: train-step 2045, loss 0.366791, acc 0.86
2016-05-20T21:01:28.997742: train-step 2046, loss 0.365529, acc 0.91
2016-05-20T21:01:38.210776: train-step 2047, loss 0.338216, acc 0.88
2016-05-20T21:01:47.491322: train-step 2048, loss 0.307591, acc 0.91
2016-05-20T21:01:56.532004: train-step 2049, loss 0.323504, acc 0.92
2016-05-20T21:02:05.557670: train-step 2050, loss 0.441882, acc 0.86
2016-05-20T21:02:14.656999: train-step 2051, loss 0.328375, acc 0.89
2016-05-20T21:02:23.371573: train-step 2052, loss 0.343939, acc 0.89
2016-05-20T21:02:31.129837: train-step 2053, loss 0.280652, acc 0.94
2016-05-20T21:02:40.570237: train-step 2054, loss 0.318692, acc 0.88
2016-05-20T21:02:48.219381: train-step 2055, loss 0.342284, acc 0.86
2016-05-20T21:02:56.115943: train-step 2056, loss 0.310815, acc 0.9
2016-05-20T21:03:04.044594: train-step 2057, loss 0.352727, acc 0.88
2016-05-20T21:03:11.959315: train-step 2058, loss 0.337818, acc 0.88
2016-05-20T21:03:19.878804: train-step 2059, loss 0.308179, acc 0.91
2016-05-20T21:03:27.866247: train-step 2060, loss 0.327203, acc 0.91
2016-05-20T21:03:36.640030: train-step 2061, loss 0.352407, acc 0.89
2016-05-20T21:03:45.645639: train-step 2062, loss 0.383145, acc 0.91
2016-05-20T21:03:55.020183: train-step 2063, loss 0.299774, acc 0.93
2016-05-20T21:04:04.435433: train-step 2064, loss 0.315729, acc 0.9
2016-05-20T21:04:14.389692: train-step 2065, loss 0.379377, acc 0.85
2016-05-20T21:04:24.203105: train-step 2066, loss 0.266695, acc 0.92
2016-05-20T21:04:33.509498: train-step 2067, loss 0.302167, acc 0.89
2016-05-20T21:04:42.776039: train-step 2068, loss 0.326214, acc 0.89
2016-05-20T21:04:52.126159: train-step 2069, loss 0.400906, acc 0.81
2016-05-20T21:05:01.754902: train-step 2070, loss 0.404008, acc 0.83
2016-05-20T21:05:11.065006: train-step 2071, loss 0.351139, acc 0.88
2016-05-20T21:05:20.359975: train-step 2072, loss 0.321775, acc 0.91
2016-05-20T21:05:29.153844: train-step 2073, loss 0.298312, acc 0.91
2016-05-20T21:05:37.588614: train-step 2074, loss 0.289221, acc 0.93
2016-05-20T21:05:46.295548: train-step 2075, loss 0.40905, acc 0.87
2016-05-20T21:05:54.660011: train-step 2076, loss 0.2907, acc 0.92
2016-05-20T21:06:02.405443: train-step 2077, loss 0.309697, acc 0.89
2016-05-20T21:06:10.362204: train-step 2078, loss 0.267444, acc 0.93
2016-05-20T21:06:18.075077: train-step 2079, loss 0.316255, acc 0.89
2016-05-20T21:06:25.671288: train-step 2080, loss 0.304288, acc 0.94
2016-05-20T21:06:33.652810: train-step 2081, loss 0.325632, acc 0.87
2016-05-20T21:06:41.688571: train-step 2082, loss 0.378098, acc 0.87
2016-05-20T21:06:50.481683: train-step 2083, loss 0.365875, acc 0.88
2016-05-20T21:06:59.358657: train-step 2084, loss 0.355167, acc 0.9
2016-05-20T21:07:08.347212: train-step 2085, loss 0.310132, acc 0.9
2016-05-20T21:07:17.327729: train-step 2086, loss 0.386862, acc 0.87
2016-05-20T21:07:26.709358: train-step 2087, loss 0.206801, acc 0.98
2016-05-20T21:07:35.839797: train-step 2088, loss 0.274505, acc 0.93
2016-05-20T21:07:44.890949: train-step 2089, loss 0.394682, acc 0.84
2016-05-20T21:07:54.046456: train-step 2090, loss 0.309122, acc 0.89
2016-05-20T21:08:03.196518: train-step 2091, loss 0.317495, acc 0.89
2016-05-20T21:08:12.396617: train-step 2092, loss 0.276824, acc 0.93
2016-05-20T21:08:21.629707: train-step 2093, loss 0.261154, acc 0.96
2016-05-20T21:08:30.921426: train-step 2094, loss 0.349213, acc 0.91
2016-05-20T21:08:40.086528: train-step 2095, loss 0.313026, acc 0.89
2016-05-20T21:08:48.679145: train-step 2096, loss 0.221212, acc 0.97
2016-05-20T21:08:56.843126: train-step 2097, loss 0.363102, acc 0.87
2016-05-20T21:09:06.011472: train-step 2098, loss 0.330443, acc 0.9
2016-05-20T21:09:13.714348: train-step 2099, loss 0.342215, acc 0.86
2016-05-20T21:09:21.557429: train-step 2100, loss 0.288551, acc 0.92
2016-05-20T21:09:29.448763: train-step 2101, loss 0.305762, acc 0.91
2016-05-20T21:09:37.177481: train-step 2102, loss 0.329264, acc 0.88
2016-05-20T21:09:44.847479: train-step 2103, loss 0.32855, acc 0.92
2016-05-20T21:09:52.644132: train-step 2104, loss 0.257016, acc 0.95
2016-05-20T21:10:01.511359: train-step 2105, loss 0.298095, acc 0.9
2016-05-20T21:10:10.546486: train-step 2106, loss 0.362529, acc 0.86
2016-05-20T21:10:19.820617: train-step 2107, loss 0.282608, acc 0.91
2016-05-20T21:10:28.911657: train-step 2108, loss 0.296425, acc 0.91
2016-05-20T21:10:38.168032: train-step 2109, loss 0.288387, acc 0.92
2016-05-20T21:10:47.347117: train-step 2110, loss 0.368929, acc 0.85
2016-05-20T21:10:56.990598: train-step 2111, loss 0.284181, acc 0.94
2016-05-20T21:11:06.094994: train-step 2112, loss 0.286138, acc 0.91
2016-05-20T21:11:15.159137: train-step 2113, loss 0.326881, acc 0.9
2016-05-20T21:11:24.751058: train-step 2114, loss 0.25078, acc 0.95
2016-05-20T21:11:34.252446: train-step 2115, loss 0.314294, acc 0.94
2016-05-20T21:11:43.344488: train-step 2116, loss 0.240456, acc 0.93
2016-05-20T21:11:52.390657: train-step 2117, loss 0.371259, acc 0.88
2016-05-20T21:12:00.575140: train-step 2118, loss 0.266007, acc 0.93
2016-05-20T21:12:09.273728: train-step 2119, loss 0.348903, acc 0.9
2016-05-20T21:12:17.647527: train-step 2120, loss 0.261163, acc 0.94
2016-05-20T21:12:25.320112: train-step 2121, loss 0.302213, acc 0.93
2016-05-20T21:12:32.979360: train-step 2122, loss 0.281874, acc 0.94
2016-05-20T21:12:40.745461: train-step 2123, loss 0.348781, acc 0.9
2016-05-20T21:12:48.555207: train-step 2124, loss 0.30331, acc 0.9
2016-05-20T21:12:56.369327: train-step 2125, loss 0.315984, acc 0.9
2016-05-20T21:13:04.474489: train-step 2126, loss 0.402568, acc 0.86
2016-05-20T21:13:12.923734: train-step 2127, loss 0.332374, acc 0.9
2016-05-20T21:13:21.764147: train-step 2128, loss 0.299045, acc 0.9
2016-05-20T21:13:30.987230: train-step 2129, loss 0.3204, acc 0.91
2016-05-20T21:13:40.121789: train-step 2130, loss 0.319977, acc 0.89
2016-05-20T21:13:49.379650: train-step 2131, loss 0.359634, acc 0.87
2016-05-20T21:13:58.285643: train-step 2132, loss 0.355287, acc 0.88
2016-05-20T21:14:07.512301: train-step 2133, loss 0.331154, acc 0.91
2016-05-20T21:14:16.795808: train-step 2134, loss 0.365641, acc 0.88
2016-05-20T21:14:25.962633: train-step 2135, loss 0.330035, acc 0.92
2016-05-20T21:14:35.391712: train-step 2136, loss 0.32114, acc 0.89
2016-05-20T21:14:44.454707: train-step 2137, loss 0.368235, acc 0.86
2016-05-20T21:14:53.752696: train-step 2138, loss 0.286692, acc 0.91
2016-05-20T21:15:02.799519: train-step 2139, loss 0.328671, acc 0.91
2016-05-20T21:15:11.231284: train-step 2140, loss 0.266573, acc 0.95
2016-05-20T21:15:19.224902: train-step 2141, loss 0.295512, acc 0.93
2016-05-20T21:15:28.598369: train-step 2142, loss 0.292666, acc 0.9
2016-05-20T21:15:36.052942: train-step 2143, loss 0.338976, acc 0.89
2016-05-20T21:15:44.046551: train-step 2144, loss 0.282311, acc 0.91
2016-05-20T21:15:51.808919: train-step 2145, loss 0.316175, acc 0.88
2016-05-20T21:15:59.876941: train-step 2146, loss 0.334891, acc 0.88
2016-05-20T21:16:07.798675: train-step 2147, loss 0.323941, acc 0.91
2016-05-20T21:16:15.469877: train-step 2148, loss 0.28945, acc 0.94
2016-05-20T21:16:23.842990: train-step 2149, loss 0.326289, acc 0.89
2016-05-20T21:16:32.805369: train-step 2150, loss 0.290858, acc 0.91
2016-05-20T21:16:41.999420: train-step 2151, loss 0.324211, acc 0.85
2016-05-20T21:16:51.633088: train-step 2152, loss 0.275805, acc 0.89
2016-05-20T21:17:00.882657: train-step 2153, loss 0.289171, acc 0.92
2016-05-20T21:17:10.364424: train-step 2154, loss 0.285489, acc 0.93
2016-05-20T21:17:19.499557: train-step 2155, loss 0.402839, acc 0.91
2016-05-20T21:17:28.367250: train-step 2156, loss 0.285027, acc 0.93
2016-05-20T21:17:37.912518: train-step 2157, loss 0.314591, acc 0.91
2016-05-20T21:17:47.368243: train-step 2158, loss 0.289964, acc 0.9
2016-05-20T21:17:56.486431: train-step 2159, loss 0.323105, acc 0.91
2016-05-20T21:18:05.694236: train-step 2160, loss 0.379601, acc 0.87
2016-05-20T21:18:14.824357: train-step 2161, loss 0.264154, acc 0.92
2016-05-20T21:18:23.598626: train-step 2162, loss 0.331642, acc 0.91
2016-05-20T21:18:31.582474: train-step 2163, loss 0.322959, acc 0.9
2016-05-20T21:18:40.780168: train-step 2164, loss 0.300506, acc 0.9
2016-05-20T21:18:48.285919: train-step 2165, loss 0.305392, acc 0.9
2016-05-20T21:18:56.342008: train-step 2166, loss 0.344403, acc 0.88
2016-05-20T21:19:04.535524: train-step 2167, loss 0.390328, acc 0.86
2016-05-20T21:19:12.374047: train-step 2168, loss 0.346848, acc 0.89
2016-05-20T21:19:20.255468: train-step 2169, loss 0.257869, acc 0.97
2016-05-20T21:19:28.431565: train-step 2170, loss 0.362774, acc 0.87
2016-05-20T21:19:36.636979: train-step 2171, loss 0.294362, acc 0.93
2016-05-20T21:19:45.626742: train-step 2172, loss 0.36139, acc 0.85
2016-05-20T21:19:54.796362: train-step 2173, loss 0.279735, acc 0.9
2016-05-20T21:20:03.968205: train-step 2174, loss 0.420809, acc 0.81
2016-05-20T21:20:13.238047: train-step 2175, loss 0.290287, acc 0.89
2016-05-20T21:20:22.300877: train-step 2176, loss 0.325468, acc 0.92
2016-05-20T21:20:31.506127: train-step 2177, loss 0.311181, acc 0.92
2016-05-20T21:20:40.858350: train-step 2178, loss 0.29568, acc 0.94
2016-05-20T21:20:50.110859: train-step 2179, loss 0.311749, acc 0.9
2016-05-20T21:20:59.375393: train-step 2180, loss 0.33242, acc 0.88
2016-05-20T21:21:08.560571: train-step 2181, loss 0.383826, acc 0.89
2016-05-20T21:21:17.993082: train-step 2182, loss 0.315143, acc 0.92
2016-05-20T21:21:27.140649: train-step 2183, loss 0.303229, acc 0.89
2016-05-20T21:21:35.887907: train-step 2184, loss 0.310795, acc 0.88
2016-05-20T21:21:44.061269: train-step 2185, loss 0.273375, acc 0.95
2016-05-20T21:21:53.386714: train-step 2186, loss 0.32345, acc 0.92
2016-05-20T21:22:01.059530: train-step 2187, loss 0.383659, acc 0.87
2016-05-20T21:22:09.196229: train-step 2188, loss 0.363704, acc 0.88
2016-05-20T21:22:17.146989: train-step 2189, loss 0.254375, acc 0.92
2016-05-20T21:22:25.011723: train-step 2190, loss 0.423229, acc 0.85
2016-05-20T21:22:32.878540: train-step 2191, loss 0.287252, acc 0.91
2016-05-20T21:22:40.773111: train-step 2192, loss 0.294106, acc 0.92
2016-05-20T21:22:48.905145: train-step 2193, loss 0.30709, acc 0.9
2016-05-20T21:22:57.737861: train-step 2194, loss 0.329788, acc 0.89
2016-05-20T21:23:07.097132: train-step 2195, loss 0.345941, acc 0.89
2016-05-20T21:23:16.305712: train-step 2196, loss 0.29304, acc 0.91
2016-05-20T21:23:25.165154: train-step 2197, loss 0.290816, acc 0.94
2016-05-20T21:23:34.628213: train-step 2198, loss 0.320393, acc 0.91
2016-05-20T21:23:43.835468: train-step 2199, loss 0.287754, acc 0.89
2016-05-20T21:23:52.899871: train-step 2200, loss 0.294029, acc 0.94
2016-05-20T21:24:01.894293: train-step 2201, loss 0.376314, acc 0.87
2016-05-20T21:24:11.015865: train-step 2202, loss 0.29227, acc 0.89
2016-05-20T21:24:20.132548: train-step 2203, loss 0.326132, acc 0.94
2016-05-20T21:24:29.194235: train-step 2204, loss 0.285573, acc 0.95
2016-05-20T21:24:38.241573: train-step 2205, loss 0.310037, acc 0.91
2016-05-20T21:24:47.136050: train-step 2206, loss 0.372534, acc 0.85
2016-05-20T21:24:55.193794: train-step 2207, loss 0.344506, acc 0.9
2016-05-20T21:25:04.547152: train-step 2208, loss 0.293309, acc 0.91
2016-05-20T21:25:12.215805: train-step 2209, loss 0.357068, acc 0.89
2016-05-20T21:25:20.069072: train-step 2210, loss 0.298099, acc 0.95
2016-05-20T21:25:27.583041: train-step 2211, loss 0.298687, acc 0.93
2016-05-20T21:25:35.332466: train-step 2212, loss 0.297681, acc 0.92
2016-05-20T21:25:43.052038: train-step 2213, loss 0.27697, acc 0.93
2016-05-20T21:25:50.855327: train-step 2214, loss 0.386915, acc 0.88
2016-05-20T21:25:59.089137: train-step 2215, loss 0.342687, acc 0.87
2016-05-20T21:26:08.024278: train-step 2216, loss 0.286547, acc 0.94
2016-05-20T21:26:17.040774: train-step 2217, loss 0.419714, acc 0.83
2016-05-20T21:26:26.082957: train-step 2218, loss 0.320769, acc 0.9
2016-05-20T21:26:35.246892: train-step 2219, loss 0.332364, acc 0.89
2016-05-20T21:26:44.256865: train-step 2220, loss 0.329073, acc 0.87
2016-05-20T21:26:53.462943: train-step 2221, loss 0.352875, acc 0.88
2016-05-20T21:27:02.940979: train-step 2222, loss 0.281673, acc 0.91
2016-05-20T21:27:12.048484: train-step 2223, loss 0.290543, acc 0.92
2016-05-20T21:27:21.043508: train-step 2224, loss 0.416355, acc 0.84
2016-05-20T21:27:30.298758: train-step 2225, loss 0.300236, acc 0.91
2016-05-20T21:27:39.763308: train-step 2226, loss 0.359811, acc 0.89
2016-05-20T21:27:49.070888: train-step 2227, loss 0.356393, acc 0.91
2016-05-20T21:27:57.683487: train-step 2228, loss 0.30213, acc 0.91
2016-05-20T21:28:05.895763: train-step 2229, loss 0.322002, acc 0.89
2016-05-20T21:28:15.201097: train-step 2230, loss 0.278958, acc 0.9
2016-05-20T21:28:22.657838: train-step 2231, loss 0.305514, acc 0.9
2016-05-20T21:28:30.423171: train-step 2232, loss 0.316315, acc 0.9
2016-05-20T21:28:37.916997: train-step 2233, loss 0.292493, acc 0.89
2016-05-20T21:28:45.765142: train-step 2234, loss 0.317728, acc 0.89
2016-05-20T21:28:53.342595: train-step 2235, loss 0.344451, acc 0.88
2016-05-20T21:29:01.034886: train-step 2236, loss 0.255515, acc 0.93
2016-05-20T21:29:08.915816: train-step 2237, loss 0.262551, acc 0.92
2016-05-20T21:29:17.821257: train-step 2238, loss 0.307633, acc 0.89
2016-05-20T21:29:26.791838: train-step 2239, loss 0.318713, acc 0.93
2016-05-20T21:29:35.668473: train-step 2240, loss 0.357673, acc 0.9
2016-05-20T21:29:44.757122: train-step 2241, loss 0.297571, acc 0.91
2016-05-20T21:29:53.939768: train-step 2242, loss 0.268531, acc 0.92
2016-05-20T21:30:03.117910: train-step 2243, loss 0.340706, acc 0.92
2016-05-20T21:30:12.216178: train-step 2244, loss 0.277741, acc 0.9
2016-05-20T21:30:21.336696: train-step 2245, loss 0.344753, acc 0.88
2016-05-20T21:30:30.950821: train-step 2246, loss 0.401678, acc 0.87
2016-05-20T21:30:40.223089: train-step 2247, loss 0.365457, acc 0.87
2016-05-20T21:30:49.723355: train-step 2248, loss 0.252321, acc 0.96
2016-05-20T21:30:58.984669: train-step 2249, loss 0.287483, acc 0.9
2016-05-20T21:31:08.179179: train-step 2250, loss 0.337454, acc 0.85
epoch number is: 9
2016-05-20T21:31:17.030868: train-step 2251, loss 0.315395, acc 0.86
2016-05-20T21:31:25.122074: train-step 2252, loss 0.315165, acc 0.92
2016-05-20T21:31:34.031551: train-step 2253, loss 0.304986, acc 0.92
2016-05-20T21:31:41.764383: train-step 2254, loss 0.306003, acc 0.93
2016-05-20T21:31:49.467951: train-step 2255, loss 0.313677, acc 0.91
2016-05-20T21:31:57.314979: train-step 2256, loss 0.29717, acc 0.9
2016-05-20T21:32:05.422165: train-step 2257, loss 0.354718, acc 0.86
2016-05-20T21:32:13.456099: train-step 2258, loss 0.304228, acc 0.89
2016-05-20T21:32:21.406208: train-step 2259, loss 0.385289, acc 0.88
2016-05-20T21:32:30.362301: train-step 2260, loss 0.236915, acc 0.97
2016-05-20T21:32:39.312151: train-step 2261, loss 0.274902, acc 0.9
2016-05-20T21:32:48.787550: train-step 2262, loss 0.274159, acc 0.95
2016-05-20T21:32:58.059434: train-step 2263, loss 0.351927, acc 0.89
2016-05-20T21:33:07.471177: train-step 2264, loss 0.299501, acc 0.95
2016-05-20T21:33:16.791389: train-step 2265, loss 0.375188, acc 0.9
2016-05-20T21:33:25.908134: train-step 2266, loss 0.259104, acc 0.96
2016-05-20T21:33:35.193615: train-step 2267, loss 0.302308, acc 0.92
2016-05-20T21:33:44.603424: train-step 2268, loss 0.312643, acc 0.88
2016-05-20T21:33:53.655486: train-step 2269, loss 0.251453, acc 0.96
2016-05-20T21:34:02.798650: train-step 2270, loss 0.259444, acc 0.93
2016-05-20T21:34:11.619890: train-step 2271, loss 0.29099, acc 0.92
2016-05-20T21:34:20.600980: train-step 2272, loss 0.291195, acc 0.9
2016-05-20T21:34:29.114202: train-step 2273, loss 0.247278, acc 0.93
2016-05-20T21:34:37.201475: train-step 2274, loss 0.31242, acc 0.9
2016-05-20T21:34:46.719013: train-step 2275, loss 0.300807, acc 0.92
2016-05-20T21:34:54.400143: train-step 2276, loss 0.295506, acc 0.91
2016-05-20T21:35:02.335607: train-step 2277, loss 0.329123, acc 0.9
2016-05-20T21:35:09.818360: train-step 2278, loss 0.364437, acc 0.88
2016-05-20T21:35:17.627695: train-step 2279, loss 0.284143, acc 0.94
2016-05-20T21:35:25.495964: train-step 2280, loss 0.326572, acc 0.91
2016-05-20T21:35:33.241172: train-step 2281, loss 0.262622, acc 0.93
2016-05-20T21:35:41.694611: train-step 2282, loss 0.297297, acc 0.94
2016-05-20T21:35:50.871308: train-step 2283, loss 0.26092, acc 0.94
2016-05-20T21:35:59.964715: train-step 2284, loss 0.281178, acc 0.93
2016-05-20T21:36:09.217049: train-step 2285, loss 0.391226, acc 0.87
2016-05-20T21:36:18.388812: train-step 2286, loss 0.359342, acc 0.89
2016-05-20T21:36:27.521384: train-step 2287, loss 0.272517, acc 0.93
2016-05-20T21:36:36.739664: train-step 2288, loss 0.335666, acc 0.9
2016-05-20T21:36:46.121119: train-step 2289, loss 0.309262, acc 0.94
2016-05-20T21:36:55.337073: train-step 2290, loss 0.327393, acc 0.89
2016-05-20T21:37:04.491951: train-step 2291, loss 0.451895, acc 0.84
2016-05-20T21:37:13.644268: train-step 2292, loss 0.274079, acc 0.93
2016-05-20T21:37:22.696207: train-step 2293, loss 0.266832, acc 0.95
2016-05-20T21:37:31.712086: train-step 2294, loss 0.308316, acc 0.9
2016-05-20T21:37:40.664240: train-step 2295, loss 0.323434, acc 0.89
2016-05-20T21:37:48.690622: train-step 2296, loss 0.244445, acc 0.97
2016-05-20T21:37:58.206956: train-step 2297, loss 0.290553, acc 0.89
2016-05-20T21:38:06.231866: train-step 2298, loss 0.30802, acc 0.93
2016-05-20T21:38:14.300774: train-step 2299, loss 0.314017, acc 0.92
2016-05-20T21:38:22.040788: train-step 2300, loss 0.277027, acc 0.93
2016-05-20T21:38:29.843785: train-step 2301, loss 0.378367, acc 0.88
2016-05-20T21:38:37.471881: train-step 2302, loss 0.25996, acc 0.94
2016-05-20T21:38:45.360342: train-step 2303, loss 0.326161, acc 0.88
2016-05-20T21:38:53.937941: train-step 2304, loss 0.336877, acc 0.89
2016-05-20T21:39:02.909973: train-step 2305, loss 0.286227, acc 0.92
2016-05-20T21:39:12.190881: train-step 2306, loss 0.267752, acc 0.93
2016-05-20T21:39:21.219603: train-step 2307, loss 0.281711, acc 0.95
2016-05-20T21:39:30.347075: train-step 2308, loss 0.366433, acc 0.88
2016-05-20T21:39:39.777506: train-step 2309, loss 0.252082, acc 0.95
2016-05-20T21:39:48.942029: train-step 2310, loss 0.257609, acc 0.93
2016-05-20T21:39:58.164484: train-step 2311, loss 0.240576, acc 0.92
2016-05-20T21:40:07.661834: train-step 2312, loss 0.318477, acc 0.91
2016-05-20T21:40:16.809933: train-step 2313, loss 0.246285, acc 0.95
2016-05-20T21:40:25.986523: train-step 2314, loss 0.311994, acc 0.9
2016-05-20T21:40:34.901456: train-step 2315, loss 0.292095, acc 0.93
2016-05-20T21:40:44.034960: train-step 2316, loss 0.355763, acc 0.88
2016-05-20T21:40:52.856243: train-step 2317, loss 0.354178, acc 0.87
2016-05-20T21:41:01.364836: train-step 2318, loss 0.360652, acc 0.87
2016-05-20T21:41:09.208813: train-step 2319, loss 0.240946, acc 0.97
2016-05-20T21:41:18.365043: train-step 2320, loss 0.316731, acc 0.9
2016-05-20T21:41:26.577918: train-step 2321, loss 0.294532, acc 0.93
2016-05-20T21:41:34.390033: train-step 2322, loss 0.295177, acc 0.93
2016-05-20T21:41:42.122887: train-step 2323, loss 0.293852, acc 0.93
2016-05-20T21:41:49.745541: train-step 2324, loss 0.340749, acc 0.86
2016-05-20T21:41:57.625540: train-step 2325, loss 0.272757, acc 0.92
2016-05-20T21:42:05.459272: train-step 2326, loss 0.32281, acc 0.88
2016-05-20T21:42:14.290998: train-step 2327, loss 0.270969, acc 0.93
2016-05-20T21:42:23.158741: train-step 2328, loss 0.2864, acc 0.91
2016-05-20T21:42:32.285975: train-step 2329, loss 0.237654, acc 0.93
2016-05-20T21:42:41.532239: train-step 2330, loss 0.347543, acc 0.92
2016-05-20T21:42:51.015788: train-step 2331, loss 0.328377, acc 0.89
2016-05-20T21:42:59.831728: train-step 2332, loss 0.347554, acc 0.89
2016-05-20T21:43:09.298706: train-step 2333, loss 0.261524, acc 0.95
2016-05-20T21:43:18.822429: train-step 2334, loss 0.314672, acc 0.89
2016-05-20T21:43:27.885693: train-step 2335, loss 0.279034, acc 0.95
2016-05-20T21:43:37.079358: train-step 2336, loss 0.2792, acc 0.94
2016-05-20T21:43:46.387814: train-step 2337, loss 0.237947, acc 0.94
2016-05-20T21:43:55.798970: train-step 2338, loss 0.245123, acc 0.94
2016-05-20T21:44:04.679052: train-step 2339, loss 0.345663, acc 0.85
2016-05-20T21:44:12.938998: train-step 2340, loss 0.30874, acc 0.91
2016-05-20T21:44:21.512262: train-step 2341, loss 0.29437, acc 0.94
2016-05-20T21:44:29.995497: train-step 2342, loss 0.293166, acc 0.87
2016-05-20T21:44:38.034053: train-step 2343, loss 0.264412, acc 0.96
2016-05-20T21:44:45.961952: train-step 2344, loss 0.327637, acc 0.88
2016-05-20T21:44:54.078540: train-step 2345, loss 0.330655, acc 0.88
2016-05-20T21:45:01.934844: train-step 2346, loss 0.301821, acc 0.89
2016-05-20T21:45:09.981914: train-step 2347, loss 0.257672, acc 0.97
2016-05-20T21:45:18.291611: train-step 2348, loss 0.276666, acc 0.93
2016-05-20T21:45:26.945818: train-step 2349, loss 0.274188, acc 0.9
2016-05-20T21:45:36.066329: train-step 2350, loss 0.254903, acc 0.96
2016-05-20T21:45:45.164052: train-step 2351, loss 0.28504, acc 0.92
2016-05-20T21:45:54.568444: train-step 2352, loss 0.359986, acc 0.88
2016-05-20T21:46:03.825452: train-step 2353, loss 0.3446, acc 0.9
2016-05-20T21:46:13.265735: train-step 2354, loss 0.264791, acc 0.91
2016-05-20T21:46:22.713230: train-step 2355, loss 0.340802, acc 0.9
2016-05-20T21:46:31.994523: train-step 2356, loss 0.291866, acc 0.91
2016-05-20T21:46:41.137688: train-step 2357, loss 0.279657, acc 0.92
2016-05-20T21:46:50.568780: train-step 2358, loss 0.254452, acc 0.92
2016-05-20T21:46:59.628362: train-step 2359, loss 0.28918, acc 0.91
2016-05-20T21:47:09.059249: train-step 2360, loss 0.302322, acc 0.88
2016-05-20T21:47:17.777141: train-step 2361, loss 0.275545, acc 0.94
2016-05-20T21:47:25.876375: train-step 2362, loss 0.316026, acc 0.92
2016-05-20T21:47:35.239496: train-step 2363, loss 0.339103, acc 0.89
2016-05-20T21:47:43.061137: train-step 2364, loss 0.313786, acc 0.9
2016-05-20T21:47:51.143447: train-step 2365, loss 0.305927, acc 0.9
2016-05-20T21:47:58.997517: train-step 2366, loss 0.346748, acc 0.93
2016-05-20T21:48:06.701294: train-step 2367, loss 0.367495, acc 0.86
2016-05-20T21:48:14.567023: train-step 2368, loss 0.347285, acc 0.88
2016-05-20T21:48:22.644944: train-step 2369, loss 0.281427, acc 0.93
2016-05-20T21:48:30.509088: train-step 2370, loss 0.274762, acc 0.94
2016-05-20T21:48:39.572720: train-step 2371, loss 0.315216, acc 0.93
2016-05-20T21:48:48.777439: train-step 2372, loss 0.297694, acc 0.92
2016-05-20T21:48:57.761041: train-step 2373, loss 0.334622, acc 0.9
2016-05-20T21:49:06.948181: train-step 2374, loss 0.310031, acc 0.9
2016-05-20T21:49:15.979197: train-step 2375, loss 0.292305, acc 0.89
2016-05-20T21:49:25.267438: train-step 2376, loss 0.334543, acc 0.9
2016-05-20T21:49:34.501135: train-step 2377, loss 0.303293, acc 0.91
2016-05-20T21:49:43.440585: train-step 2378, loss 0.341756, acc 0.87
2016-05-20T21:49:52.780447: train-step 2379, loss 0.222876, acc 0.95
2016-05-20T21:50:01.831872: train-step 2380, loss 0.308464, acc 0.9
2016-05-20T21:50:10.929809: train-step 2381, loss 0.3298, acc 0.89
2016-05-20T21:50:19.819310: train-step 2382, loss 0.29909, acc 0.94
2016-05-20T21:50:28.948966: train-step 2383, loss 0.318845, acc 0.92
2016-05-20T21:50:37.250526: train-step 2384, loss 0.277857, acc 0.91
2016-05-20T21:50:45.614703: train-step 2385, loss 0.34234, acc 0.88
2016-05-20T21:50:54.149627: train-step 2386, loss 0.378038, acc 0.89
2016-05-20T21:51:02.328054: train-step 2387, loss 0.336406, acc 0.88
2016-05-20T21:51:10.142078: train-step 2388, loss 0.40459, acc 0.86
2016-05-20T21:51:17.989778: train-step 2389, loss 0.263706, acc 0.93
2016-05-20T21:51:25.992943: train-step 2390, loss 0.34619, acc 0.91
2016-05-20T21:51:33.973228: train-step 2391, loss 0.343544, acc 0.84
2016-05-20T21:51:42.297368: train-step 2392, loss 0.37675, acc 0.86
2016-05-20T21:51:50.959160: train-step 2393, loss 0.294036, acc 0.92
2016-05-20T21:51:59.837141: train-step 2394, loss 0.332812, acc 0.89
2016-05-20T21:52:08.954531: train-step 2395, loss 0.293184, acc 0.9
2016-05-20T21:52:18.446369: train-step 2396, loss 0.347292, acc 0.9
2016-05-20T21:52:27.614104: train-step 2397, loss 0.358737, acc 0.88
2016-05-20T21:52:36.846246: train-step 2398, loss 0.326887, acc 0.89
2016-05-20T21:52:46.098759: train-step 2399, loss 0.296714, acc 0.91
2016-05-20T21:52:55.399066: train-step 2400, loss 0.253949, acc 0.93
2016-05-20T21:53:04.534933: train-step 2401, loss 0.294341, acc 0.94
2016-05-20T21:53:13.606864: train-step 2402, loss 0.326699, acc 0.91
2016-05-20T21:53:22.632358: train-step 2403, loss 0.328825, acc 0.9
2016-05-20T21:53:31.623044: train-step 2404, loss 0.370979, acc 0.83
2016-05-20T21:53:40.702529: train-step 2405, loss 0.27256, acc 0.96
2016-05-20T21:53:49.207060: train-step 2406, loss 0.300478, acc 0.95
2016-05-20T21:53:57.128329: train-step 2407, loss 0.326775, acc 0.89
2016-05-20T21:54:06.755916: train-step 2408, loss 0.284742, acc 0.91
2016-05-20T21:54:15.095439: train-step 2409, loss 0.321525, acc 0.89
2016-05-20T21:54:23.243261: train-step 2410, loss 0.325173, acc 0.9
2016-05-20T21:54:31.465659: train-step 2411, loss 0.32198, acc 0.89
2016-05-20T21:54:39.183596: train-step 2412, loss 0.347044, acc 0.92
2016-05-20T21:54:47.276405: train-step 2413, loss 0.221525, acc 0.96
2016-05-20T21:54:55.338226: train-step 2414, loss 0.328023, acc 0.91
2016-05-20T21:55:03.996514: train-step 2415, loss 0.305748, acc 0.94
2016-05-20T21:55:13.126414: train-step 2416, loss 0.299831, acc 0.92
2016-05-20T21:55:22.110514: train-step 2417, loss 0.241888, acc 0.94
2016-05-20T21:55:31.573032: train-step 2418, loss 0.342058, acc 0.91
2016-05-20T21:55:40.913770: train-step 2419, loss 0.300647, acc 0.89
2016-05-20T21:55:50.156383: train-step 2420, loss 0.30992, acc 0.91
2016-05-20T21:55:59.418065: train-step 2421, loss 0.334779, acc 0.88
2016-05-20T21:56:08.401979: train-step 2422, loss 0.347114, acc 0.87
2016-05-20T21:56:18.004806: train-step 2423, loss 0.33461, acc 0.9
2016-05-20T21:56:27.321244: train-step 2424, loss 0.335226, acc 0.89
2016-05-20T21:56:36.691637: train-step 2425, loss 0.322616, acc 0.86
2016-05-20T21:56:45.643042: train-step 2426, loss 0.375389, acc 0.91
2016-05-20T21:56:54.539177: train-step 2427, loss 0.285955, acc 0.91
2016-05-20T21:57:03.263091: train-step 2428, loss 0.352423, acc 0.87
2016-05-20T21:57:11.075258: train-step 2429, loss 0.33541, acc 0.92
2016-05-20T21:57:20.546714: train-step 2430, loss 0.295312, acc 0.88
2016-05-20T21:57:28.049769: train-step 2431, loss 0.287851, acc 0.93
2016-05-20T21:57:35.776889: train-step 2432, loss 0.292038, acc 0.88
2016-05-20T21:57:43.521402: train-step 2433, loss 0.254503, acc 0.93
2016-05-20T21:57:51.287428: train-step 2434, loss 0.324732, acc 0.9
2016-05-20T21:57:58.994216: train-step 2435, loss 0.254259, acc 0.95
2016-05-20T21:58:07.340232: train-step 2436, loss 0.269382, acc 0.9
2016-05-20T21:58:16.124105: train-step 2437, loss 0.309628, acc 0.87
2016-05-20T21:58:25.733491: train-step 2438, loss 0.306186, acc 0.91
2016-05-20T21:58:34.911075: train-step 2439, loss 0.262136, acc 0.95
2016-05-20T21:58:44.057444: train-step 2440, loss 0.349785, acc 0.88
2016-05-20T21:58:52.902906: train-step 2441, loss 0.320746, acc 0.89
2016-05-20T21:59:01.780685: train-step 2442, loss 0.301901, acc 0.91
2016-05-20T21:59:11.020399: train-step 2443, loss 0.337876, acc 0.9
2016-05-20T21:59:20.208495: train-step 2444, loss 0.380031, acc 0.86
2016-05-20T21:59:29.407169: train-step 2445, loss 0.31199, acc 0.89
2016-05-20T21:59:38.731160: train-step 2446, loss 0.319857, acc 0.91
2016-05-20T21:59:47.988909: train-step 2447, loss 0.358606, acc 0.91
2016-05-20T21:59:57.167853: train-step 2448, loss 0.342518, acc 0.87
2016-05-20T22:00:06.670577: train-step 2449, loss 0.39731, acc 0.83
2016-05-20T22:00:15.633245: train-step 2450, loss 0.36883, acc 0.85
2016-05-20T22:00:23.904941: train-step 2451, loss 0.279673, acc 0.92
2016-05-20T22:00:33.159766: train-step 2452, loss 0.293242, acc 0.94
2016-05-20T22:00:41.321374: train-step 2453, loss 0.355555, acc 0.86
2016-05-20T22:00:49.464206: train-step 2454, loss 0.297522, acc 0.91
2016-05-20T22:00:57.254930: train-step 2455, loss 0.385299, acc 0.85
2016-05-20T22:01:04.990791: train-step 2456, loss 0.292082, acc 0.91
2016-05-20T22:01:12.895957: train-step 2457, loss 0.268379, acc 0.94
2016-05-20T22:01:20.807365: train-step 2458, loss 0.289513, acc 0.93
2016-05-20T22:01:29.024762: train-step 2459, loss 0.28264, acc 0.93
2016-05-20T22:01:38.111186: train-step 2460, loss 0.27104, acc 0.92
2016-05-20T22:01:47.184686: train-step 2461, loss 0.364585, acc 0.87
2016-05-20T22:01:56.752507: train-step 2462, loss 0.297627, acc 0.9
2016-05-20T22:02:06.083197: train-step 2463, loss 0.263467, acc 0.93
2016-05-20T22:02:15.240956: train-step 2464, loss 0.281598, acc 0.93
2016-05-20T22:02:24.417627: train-step 2465, loss 0.322047, acc 0.92
2016-05-20T22:02:34.180268: train-step 2466, loss 0.303667, acc 0.9
2016-05-20T22:02:43.386559: train-step 2467, loss 0.265822, acc 0.92
2016-05-20T22:02:52.626481: train-step 2468, loss 0.292428, acc 0.9
2016-05-20T22:03:01.848778: train-step 2469, loss 0.289536, acc 0.93
2016-05-20T22:03:10.928567: train-step 2470, loss 0.298757, acc 0.92
2016-05-20T22:03:19.809219: train-step 2471, loss 0.291191, acc 0.9
2016-05-20T22:03:28.515214: train-step 2472, loss 0.30615, acc 0.93
2016-05-20T22:03:36.432960: train-step 2473, loss 0.309564, acc 0.91
2016-05-20T22:03:45.615722: train-step 2474, loss 0.268573, acc 0.95
2016-05-20T22:03:53.382096: train-step 2475, loss 0.259012, acc 0.94
2016-05-20T22:04:01.188434: train-step 2476, loss 0.350794, acc 0.92
2016-05-20T22:04:09.127612: train-step 2477, loss 0.307427, acc 0.93
2016-05-20T22:04:16.892091: train-step 2478, loss 0.269734, acc 0.96
2016-05-20T22:04:24.593000: train-step 2479, loss 0.221842, acc 0.95
2016-05-20T22:04:32.370425: train-step 2480, loss 0.308742, acc 0.88
2016-05-20T22:04:40.569916: train-step 2481, loss 0.316376, acc 0.89
2016-05-20T22:04:49.250493: train-step 2482, loss 0.325087, acc 0.87
2016-05-20T22:04:57.972721: train-step 2483, loss 0.274963, acc 0.92
2016-05-20T22:05:07.610835: train-step 2484, loss 0.407676, acc 0.83
2016-05-20T22:05:16.965746: train-step 2485, loss 0.277695, acc 0.93
2016-05-20T22:05:26.542998: train-step 2486, loss 0.297529, acc 0.93
2016-05-20T22:05:35.606439: train-step 2487, loss 0.294333, acc 0.92
2016-05-20T22:05:44.715184: train-step 2488, loss 0.334839, acc 0.89
2016-05-20T22:05:54.277730: train-step 2489, loss 0.320251, acc 0.92
2016-05-20T22:06:03.609055: train-step 2490, loss 0.249029, acc 0.93
2016-05-20T22:06:13.073862: train-step 2491, loss 0.284146, acc 0.9
2016-05-20T22:06:22.285553: train-step 2492, loss 0.298961, acc 0.89
2016-05-20T22:06:31.592201: train-step 2493, loss 0.271485, acc 0.92
2016-05-20T22:06:40.622355: train-step 2494, loss 0.248375, acc 0.96
2016-05-20T22:06:49.142409: train-step 2495, loss 0.380144, acc 0.86
2016-05-20T22:06:58.473649: train-step 2496, loss 0.296797, acc 0.91
2016-05-20T22:07:06.116866: train-step 2497, loss 0.37225, acc 0.88
2016-05-20T22:07:13.970435: train-step 2498, loss 0.314387, acc 0.88
2016-05-20T22:07:21.881535: train-step 2499, loss 0.283499, acc 0.92
2016-05-20T22:07:29.890880: train-step 2500, loss 0.262909, acc 0.91
2016-05-20T22:07:33.468196  dev-step: 1  acc: 0.87
2016-05-20T22:07:36.468873  dev-step: 2  acc: 0.92
2016-05-20T22:07:39.457221  dev-step: 3  acc: 0.92
2016-05-20T22:07:42.619459  dev-step: 4  acc: 0.9
2016-05-20T22:07:45.761966  dev-step: 5  acc: 0.87
2016-05-20T22:07:48.918186  dev-step: 6  acc: 0.94
2016-05-20T22:07:52.103918  dev-step: 7  acc: 0.9
2016-05-20T22:07:55.189434  dev-step: 8  acc: 0.92
2016-05-20T22:07:58.358576  dev-step: 9  acc: 0.93
2016-05-20T22:08:01.676557  dev-step: 10  acc: 0.96
2016-05-20T22:08:04.961822  dev-step: 11  acc: 0.9
2016-05-20T22:08:08.287585  dev-step: 12  acc: 0.93
2016-05-20T22:08:11.863604  dev-step: 13  acc: 0.91
2016-05-20T22:08:15.253826  dev-step: 14  acc: 0.85
2016-05-20T22:08:18.659553  dev-step: 15  acc: 0.88
2016-05-20T22:08:21.974640  dev-step: 16  acc: 0.92
2016-05-20T22:08:25.749104  dev-step: 17  acc: 0.86
2016-05-20T22:08:29.232311  dev-step: 18  acc: 0.94
2016-05-20T22:08:32.805631  dev-step: 19  acc: 0.89
2016-05-20T22:08:36.470195  dev-step: 20  acc: 0.89
2016-05-20T22:08:39.939344  dev-step: 21  acc: 0.92
2016-05-20T22:08:43.301631  dev-step: 22  acc: 0.87
2016-05-20T22:08:46.668464  dev-step: 23  acc: 0.88
2016-05-20T22:08:50.344561  dev-step: 24  acc: 0.91
2016-05-20T22:08:53.824158  dev-step: 25  acc: 0.82
2016-05-20T22:08:57.273309  dev-step: 26  acc: 0.88
2016-05-20T22:09:00.509453  dev-step: 27  acc: 0.81
2016-05-20T22:09:04.004584  dev-step: 28  acc: 0.89
2016-05-20T22:09:07.298678  dev-step: 29  acc: 0.93
2016-05-20T22:09:10.756859  dev-step: 30  acc: 0.84
2016-05-20T22:09:14.022756  dev-step: 31  acc: 0.89
2016-05-20T22:09:17.388031  dev-step: 32  acc: 0.77
2016-05-20T22:09:20.683135  dev-step: 33  acc: 0.92
2016-05-20T22:09:24.156158  dev-step: 34  acc: 0.91
2016-05-20T22:09:27.477807  dev-step: 35  acc: 0.89
2016-05-20T22:09:30.838425  dev-step: 36  acc: 0.86
2016-05-20T22:09:34.028762  dev-step: 37  acc: 0.89
2016-05-20T22:09:37.465572  dev-step: 38  acc: 0.87
2016-05-20T22:09:41.572841  dev-step: 39  acc: 0.89
2016-05-20T22:09:45.438619  dev-step: 40  acc: 0.91
2016-05-20T22:09:49.117196  dev-step: 41  acc: 0.92
2016-05-20T22:09:52.647745  dev-step: 42  acc: 0.88
2016-05-20T22:09:56.420700  dev-step: 43  acc: 0.86
2016-05-20T22:09:59.982737  dev-step: 44  acc: 0.88
2016-05-20T22:10:03.192142  dev-step: 45  acc: 0.89
2016-05-20T22:10:06.461270  dev-step: 46  acc: 0.82
2016-05-20T22:10:09.792411  dev-step: 47  acc: 0.82
2016-05-20T22:10:12.982940  dev-step: 48  acc: 0.93
2016-05-20T22:10:16.197605  dev-step: 49  acc: 0.94
2016-05-20T22:10:19.248157  dev-step: 50  acc: 0.92
2016-05-20T22:10:22.488478  dev-step: 51  acc: 0.94
2016-05-20T22:10:25.768780  dev-step: 52  acc: 0.9
2016-05-20T22:10:28.841614  dev-step: 53  acc: 0.89
2016-05-20T22:10:33.079961  dev-step: 54  acc: 0.97
2016-05-20T22:10:36.273483  dev-step: 55  acc: 0.89
2016-05-20T22:10:39.379016  dev-step: 56  acc: 0.86
2016-05-20T22:10:42.569714  dev-step: 57  acc: 0.89
2016-05-20T22:10:45.809556  dev-step: 58  acc: 0.9
2016-05-20T22:10:49.191806  dev-step: 59  acc: 0.91
2016-05-20T22:10:52.171979  dev-step: 60  acc: 0.88
2016-05-20T22:10:55.203538  dev-step: 61  acc: 0.83
2016-05-20T22:10:58.224735  dev-step: 62  acc: 0.85
2016-05-20T22:11:01.355413  dev-step: 63  acc: 0.85
2016-05-20T22:11:04.300809  dev-step: 64  acc: 0.95
2016-05-20T22:11:07.327614  dev-step: 65  acc: 0.79
2016-05-20T22:11:10.434760  dev-step: 66  acc: 0.83
2016-05-20T22:11:13.572134  dev-step: 67  acc: 0.86
2016-05-20T22:11:16.794689  dev-step: 68  acc: 0.9
2016-05-20T22:11:19.911506  dev-step: 69  acc: 0.92
2016-05-20T22:11:22.865511  dev-step: 70  acc: 0.94
2016-05-20T22:11:25.728783  dev-step: 71  acc: 0.92
2016-05-20T22:11:28.727093  dev-step: 72  acc: 0.93
2016-05-20T22:11:31.752833  dev-step: 73  acc: 0.91
2016-05-20T22:11:34.874962  dev-step: 74  acc: 0.91
2016-05-20T22:11:38.157051  dev-step: 75  acc: 0.93
2016-05-20T22:11:41.453726  dev-step: 76  acc: 0.88
2016-05-20T22:11:44.819418  dev-step: 77  acc: 0.85
2016-05-20T22:11:48.105609  dev-step: 78  acc: 0.87
2016-05-20T22:11:51.496452  dev-step: 79  acc: 0.88
2016-05-20T22:11:54.679437  dev-step: 80  acc: 0.9
2016-05-20T22:11:58.195238  dev-step: 81  acc: 0.89
2016-05-20T22:12:01.637271  dev-step: 82  acc: 0.84
2016-05-20T22:12:04.882138  dev-step: 83  acc: 0.9
2016-05-20T22:12:08.343940  dev-step: 84  acc: 0.87
2016-05-20T22:12:11.913588  dev-step: 85  acc: 0.94
2016-05-20T22:12:15.336653  dev-step: 86  acc: 0.95
2016-05-20T22:12:18.738134  dev-step: 87  acc: 0.87
2016-05-20T22:12:22.134742  dev-step: 88  acc: 0.92
2016-05-20T22:12:25.479111  dev-step: 89  acc: 0.88
2016-05-20T22:12:29.049353  dev-step: 90  acc: 0.9
2016-05-20T22:12:32.670354  dev-step: 91  acc: 0.83
2016-05-20T22:12:36.146899  dev-step: 92  acc: 0.88
2016-05-20T22:12:39.794905  dev-step: 93  acc: 0.9
2016-05-20T22:12:43.188624  dev-step: 94  acc: 0.94
2016-05-20T22:12:46.609107  dev-step: 95  acc: 0.97
2016-05-20T22:12:49.832733  dev-step: 96  acc: 0.87
2016-05-20T22:12:53.190836  dev-step: 97  acc: 0.84
2016-05-20T22:12:56.682320  dev-step: 98  acc: 0.87
2016-05-20T22:12:59.987916  dev-step: 99  acc: 0.82
2016-05-20T22:13:03.458980  dev-step: 100  acc: 0.95
2016-05-20T22:13:06.942755  dev-step: 101  acc: 0.93
2016-05-20T22:13:10.336718  dev-step: 102  acc: 0.91
2016-05-20T22:13:13.634612  dev-step: 103  acc: 0.93
2016-05-20T22:13:17.082497  dev-step: 104  acc: 0.87
2016-05-20T22:13:20.548757  dev-step: 105  acc: 0.91
2016-05-20T22:13:24.032612  dev-step: 106  acc: 0.92
2016-05-20T22:13:28.080085  dev-step: 107  acc: 0.87
2016-05-20T22:13:31.652733  dev-step: 108  acc: 0.93
2016-05-20T22:13:35.238477  dev-step: 109  acc: 0.82
2016-05-20T22:13:38.756026  dev-step: 110  acc: 0.86
2016-05-20T22:13:42.031921  dev-step: 111  acc: 0.89
2016-05-20T22:13:45.353310  dev-step: 112  acc: 0.88
2016-05-20T22:13:48.681986  dev-step: 113  acc: 0.93
2016-05-20T22:13:51.898377  dev-step: 114  acc: 0.87
2016-05-20T22:13:55.046358  dev-step: 115  acc: 0.82
2016-05-20T22:13:58.220648  dev-step: 116  acc: 0.91
2016-05-20T22:14:01.390006  dev-step: 117  acc: 0.91
2016-05-20T22:14:04.344939  dev-step: 118  acc: 0.87
2016-05-20T22:14:07.337327  dev-step: 119  acc: 0.79
2016-05-20T22:14:11.143168  dev-step: 120  acc: 0.86
2016-05-20T22:14:14.961358  dev-step: 121  acc: 0.86
2016-05-20T22:14:18.060034  dev-step: 122  acc: 0.79
2016-05-20T22:14:21.179595  dev-step: 123  acc: 0.94
2016-05-20T22:14:24.169822  dev-step: 124  acc: 0.87
2016-05-20T22:14:27.210033  dev-step: 125  acc: 0.93
2016-05-20T22:14:30.339939  dev-step: 126  acc: 0.92
2016-05-20T22:14:33.261206  dev-step: 127  acc: 0.93
2016-05-20T22:14:36.068763  dev-step: 128  acc: 0.91
2016-05-20T22:14:38.969119  dev-step: 129  acc: 0.88
2016-05-20T22:14:42.307620  dev-step: 130  acc: 0.92
2016-05-20T22:14:45.367497  dev-step: 131  acc: 0.95
2016-05-20T22:14:48.478102  dev-step: 132  acc: 0.96
2016-05-20T22:14:51.589795  dev-step: 133  acc: 0.95
2016-05-20T22:14:54.717376  dev-step: 134  acc: 0.85
2016-05-20T22:14:57.841055  dev-step: 135  acc: 0.97
2016-05-20T22:15:00.871800  dev-step: 136  acc: 0.93
2016-05-20T22:15:03.962535  dev-step: 137  acc: 0.95
2016-05-20T22:15:07.015873  dev-step: 138  acc: 0.93
2016-05-20T22:15:10.392690  dev-step: 139  acc: 0.86
2016-05-20T22:15:13.787716  dev-step: 140  acc: 0.87
2016-05-20T22:15:16.970298  dev-step: 141  acc: 0.9
2016-05-20T22:15:20.260477  dev-step: 142  acc: 0.89
2016-05-20T22:15:23.592458  dev-step: 143  acc: 0.91
2016-05-20T22:15:26.997373  dev-step: 144  acc: 0.88
2016-05-20T22:15:30.409459  dev-step: 145  acc: 0.84
2016-05-20T22:15:33.685026  dev-step: 146  acc: 0.96
2016-05-20T22:15:37.110943  dev-step: 147  acc: 0.87
2016-05-20T22:15:40.546605  dev-step: 148  acc: 0.91
2016-05-20T22:15:44.053489  dev-step: 149  acc: 0.84
2016-05-20T22:15:47.500784  dev-step: 150  acc: 0.91
2016-05-20T22:15:51.114430  dev-step: 151  acc: 0.9
2016-05-20T22:15:54.649040  dev-step: 152  acc: 0.87
2016-05-20T22:15:58.045988  dev-step: 153  acc: 0.93
2016-05-20T22:16:01.519721  dev-step: 154  acc: 0.92
2016-05-20T22:16:05.503684  dev-step: 155  acc: 0.9
2016-05-20T22:16:08.921569  dev-step: 156  acc: 0.87
2016-05-20T22:16:12.171119  dev-step: 157  acc: 0.92
2016-05-20T22:16:15.743151  dev-step: 158  acc: 0.89
2016-05-20T22:16:19.228405  dev-step: 159  acc: 0.9
2016-05-20T22:16:22.670855  dev-step: 160  acc: 0.91
2016-05-20T22:16:26.177187  dev-step: 161  acc: 0.96
2016-05-20T22:16:29.609651  dev-step: 162  acc: 0.95
2016-05-20T22:16:32.884423  dev-step: 163  acc: 0.95
2016-05-20T22:16:36.422449  dev-step: 164  acc: 0.88
2016-05-20T22:16:40.000720  dev-step: 165  acc: 0.94
2016-05-20T22:16:43.522298  dev-step: 166  acc: 0.86
2016-05-20T22:16:47.027534  dev-step: 167  acc: 0.86
2016-05-20T22:16:50.354986  dev-step: 168  acc: 0.91
2016-05-20T22:16:53.865134  dev-step: 169  acc: 0.9
2016-05-20T22:16:57.430649  dev-step: 170  acc: 0.91
2016-05-20T22:17:00.794274  dev-step: 171  acc: 0.91
2016-05-20T22:17:04.035941  dev-step: 172  acc: 0.88
2016-05-20T22:17:07.312693  dev-step: 173  acc: 0.94
2016-05-20T22:17:10.760871  dev-step: 174  acc: 0.96
2016-05-20T22:17:13.978036  dev-step: 175  acc: 0.92
2016-05-20T22:17:17.442607  dev-step: 176  acc: 0.84
2016-05-20T22:17:20.806247  dev-step: 177  acc: 0.92
2016-05-20T22:17:24.055739  dev-step: 178  acc: 0.91
2016-05-20T22:17:27.436406  dev-step: 179  acc: 0.85
2016-05-20T22:17:30.582759  dev-step: 180  acc: 0.9
2016-05-20T22:17:33.718768  dev-step: 181  acc: 0.89
2016-05-20T22:17:36.664606  dev-step: 182  acc: 0.96
2016-05-20T22:17:39.725296  dev-step: 183  acc: 0.92
2016-05-20T22:17:43.964095  dev-step: 184  acc: 0.93
2016-05-20T22:17:47.177561  dev-step: 185  acc: 0.96
2016-05-20T22:17:50.330395  dev-step: 186  acc: 0.85
2016-05-20T22:17:53.625230  dev-step: 187  acc: 0.91
2016-05-20T22:17:56.892421  dev-step: 188  acc: 0.9
2016-05-20T22:18:00.078934  dev-step: 189  acc: 0.9
2016-05-20T22:18:03.316060  dev-step: 190  acc: 0.88
2016-05-20T22:18:06.259352  dev-step: 191  acc: 0.89
2016-05-20T22:18:09.348274  dev-step: 192  acc: 0.94
2016-05-20T22:18:12.453195  dev-step: 193  acc: 0.87
2016-05-20T22:18:15.350115  dev-step: 194  acc: 0.95
2016-05-20T22:18:18.447632  dev-step: 195  acc: 0.92
2016-05-20T22:18:21.439237  dev-step: 196  acc: 0.91
2016-05-20T22:18:24.289467  dev-step: 197  acc: 0.92
2016-05-20T22:18:27.189783  dev-step: 198  acc: 0.93
2016-05-20T22:18:30.238834  dev-step: 199  acc: 0.91
2016-05-20T22:18:33.327505  dev-step: 200  acc: 0.89
2016-05-20T22:18:36.637025  dev-step: 201  acc: 0.9
2016-05-20T22:18:39.566365  dev-step: 202  acc: 0.93
2016-05-20T22:18:42.677190  dev-step: 203  acc: 0.93
2016-05-20T22:18:45.929421  dev-step: 204  acc: 0.89
2016-05-20T22:18:49.229392  dev-step: 205  acc: 0.9
2016-05-20T22:18:52.485351  dev-step: 206  acc: 0.93
2016-05-20T22:18:55.741973  dev-step: 207  acc: 0.89
2016-05-20T22:18:59.072664  dev-step: 208  acc: 0.9
2016-05-20T22:19:02.545103  dev-step: 209  acc: 0.89
2016-05-20T22:19:06.057543  dev-step: 210  acc: 0.92
2016-05-20T22:19:09.361769  dev-step: 211  acc: 0.89
2016-05-20T22:19:12.716683  dev-step: 212  acc: 0.9
2016-05-20T22:19:16.154359  dev-step: 213  acc: 0.89
2016-05-20T22:19:19.621716  dev-step: 214  acc: 0.88
2016-05-20T22:19:23.112756  dev-step: 215  acc: 0.84
2016-05-20T22:19:26.461580  dev-step: 216  acc: 0.89
2016-05-20T22:19:30.143117  dev-step: 217  acc: 0.88
2016-05-20T22:19:33.921367  dev-step: 218  acc: 0.82
2016-05-20T22:19:37.333996  dev-step: 219  acc: 0.85
2016-05-20T22:19:40.642655  dev-step: 220  acc: 0.94
2016-05-20T22:19:43.955925  dev-step: 221  acc: 0.93
2016-05-20T22:19:47.455943  dev-step: 222  acc: 0.94
2016-05-20T22:19:51.165413  dev-step: 223  acc: 0.95
2016-05-20T22:19:54.652793  dev-step: 224  acc: 0.86
2016-05-20T22:19:58.235677  dev-step: 225  acc: 0.92
2016-05-20T22:20:01.637697  dev-step: 226  acc: 0.9
2016-05-20T22:20:04.982478  dev-step: 227  acc: 0.92
2016-05-20T22:20:08.405148  dev-step: 228  acc: 0.87
2016-05-20T22:20:11.794832  dev-step: 229  acc: 0.91
2016-05-20T22:20:15.118497  dev-step: 230  acc: 0.91
2016-05-20T22:20:18.585935  dev-step: 231  acc: 0.91
2016-05-20T22:20:21.960111  dev-step: 232  acc: 0.88
2016-05-20T22:20:25.278622  dev-step: 233  acc: 0.89
2016-05-20T22:20:28.721941  dev-step: 234  acc: 0.94
2016-05-20T22:20:32.403847  dev-step: 235  acc: 0.93
2016-05-20T22:20:35.741607  dev-step: 236  acc: 0.88
2016-05-20T22:20:39.330485  dev-step: 237  acc: 0.9
2016-05-20T22:20:42.631163  dev-step: 238  acc: 0.92
2016-05-20T22:20:46.100909  dev-step: 239  acc: 0.94
2016-05-20T22:20:49.423809  dev-step: 240  acc: 0.86
2016-05-20T22:20:52.622913  dev-step: 241  acc: 0.86
2016-05-20T22:20:55.886593  dev-step: 242  acc: 0.87
2016-05-20T22:20:59.075842  dev-step: 243  acc: 0.86
2016-05-20T22:21:02.288976  dev-step: 244  acc: 0.9
2016-05-20T22:21:05.253445  dev-step: 245  acc: 0.82
2016-05-20T22:21:08.080117  dev-step: 246  acc: 0.89
2016-05-20T22:21:11.776705  dev-step: 247  acc: 0.94
2016-05-20T22:21:15.611579  dev-step: 248  acc: 0.88
2016-05-20T22:21:18.647429  dev-step: 249  acc: 0.83
2016-05-20T22:21:21.592933  dev-step: 250  acc: 0.89
avg_loss 0.325642, avg_acc 0.89592, real_acc 0.89592
epoch number is: 10
2016-05-20T22:21:29.651610: train-step 2501, loss 0.270465, acc 0.9
2016-05-20T22:21:37.315481: train-step 2502, loss 0.232556, acc 0.94
2016-05-20T22:21:45.083079: train-step 2503, loss 0.286611, acc 0.94
2016-05-20T22:21:52.939195: train-step 2504, loss 0.32194, acc 0.91
2016-05-20T22:22:01.022279: train-step 2505, loss 0.283189, acc 0.91
2016-05-20T22:22:09.074454: train-step 2506, loss 0.232641, acc 0.95
2016-05-20T22:22:17.726635: train-step 2507, loss 0.27998, acc 0.96
2016-05-20T22:22:26.593592: train-step 2508, loss 0.286843, acc 0.91
2016-05-20T22:22:36.056450: train-step 2509, loss 0.314783, acc 0.88
2016-05-20T22:22:45.371504: train-step 2510, loss 0.314024, acc 0.91
2016-05-20T22:22:54.480238: train-step 2511, loss 0.3092, acc 0.88
2016-05-20T22:23:03.468320: train-step 2512, loss 0.295981, acc 0.92
2016-05-20T22:23:12.432793: train-step 2513, loss 0.295172, acc 0.91
2016-05-20T22:23:21.626460: train-step 2514, loss 0.268216, acc 0.92
2016-05-20T22:23:30.685073: train-step 2515, loss 0.245319, acc 0.98
2016-05-20T22:23:39.626769: train-step 2516, loss 0.250427, acc 0.93
2016-05-20T22:23:48.755901: train-step 2517, loss 0.265607, acc 0.93
2016-05-20T22:23:57.944548: train-step 2518, loss 0.356843, acc 0.87
2016-05-20T22:24:06.793570: train-step 2519, loss 0.285875, acc 0.91
2016-05-20T22:24:15.021815: train-step 2520, loss 0.294476, acc 0.93
2016-05-20T22:24:24.356006: train-step 2521, loss 0.253901, acc 0.95
2016-05-20T22:24:31.955653: train-step 2522, loss 0.287263, acc 0.91
2016-05-20T22:24:40.071664: train-step 2523, loss 0.295067, acc 0.91
2016-05-20T22:24:48.028415: train-step 2524, loss 0.237435, acc 0.95
2016-05-20T22:24:56.206776: train-step 2525, loss 0.290854, acc 0.9
2016-05-20T22:25:04.309273: train-step 2526, loss 0.290321, acc 0.94
2016-05-20T22:25:12.236796: train-step 2527, loss 0.300101, acc 0.92
2016-05-20T22:25:21.017382: train-step 2528, loss 0.356906, acc 0.89
2016-05-20T22:25:30.114057: train-step 2529, loss 0.264088, acc 0.95
2016-05-20T22:25:39.471905: train-step 2530, loss 0.28589, acc 0.93
2016-05-20T22:25:48.846209: train-step 2531, loss 0.294671, acc 0.94
2016-05-20T22:25:57.894796: train-step 2532, loss 0.285183, acc 0.93
2016-05-20T22:26:07.322948: train-step 2533, loss 0.264158, acc 0.94
2016-05-20T22:26:16.505267: train-step 2534, loss 0.30057, acc 0.92
2016-05-20T22:26:25.773225: train-step 2535, loss 0.272696, acc 0.94
2016-05-20T22:26:34.547424: train-step 2536, loss 0.302794, acc 0.9
2016-05-20T22:26:43.780289: train-step 2537, loss 0.241168, acc 0.96
2016-05-20T22:26:52.927027: train-step 2538, loss 0.226539, acc 0.95
2016-05-20T22:27:02.281290: train-step 2539, loss 0.278497, acc 0.92
2016-05-20T22:27:11.386746: train-step 2540, loss 0.298296, acc 0.91
2016-05-20T22:27:20.108974: train-step 2541, loss 0.323033, acc 0.93
2016-05-20T22:27:28.176039: train-step 2542, loss 0.218041, acc 0.98
2016-05-20T22:27:37.360696: train-step 2543, loss 0.23394, acc 0.95
2016-05-20T22:27:44.840003: train-step 2544, loss 0.190582, acc 0.99
2016-05-20T22:27:52.758584: train-step 2545, loss 0.336518, acc 0.9
2016-05-20T22:28:00.733560: train-step 2546, loss 0.245219, acc 0.94
2016-05-20T22:28:08.456226: train-step 2547, loss 0.29047, acc 0.94
2016-05-20T22:28:16.192980: train-step 2548, loss 0.250689, acc 0.94
2016-05-20T22:28:24.037694: train-step 2549, loss 0.26002, acc 0.92
2016-05-20T22:28:32.236134: train-step 2550, loss 0.222414, acc 0.96
2016-05-20T22:28:41.520322: train-step 2551, loss 0.228622, acc 0.93
2016-05-20T22:28:50.706304: train-step 2552, loss 0.319609, acc 0.92
2016-05-20T22:28:59.981873: train-step 2553, loss 0.341798, acc 0.88
2016-05-20T22:29:09.277995: train-step 2554, loss 0.302256, acc 0.91
2016-05-20T22:29:18.663918: train-step 2555, loss 0.323701, acc 0.88
2016-05-20T22:29:27.753782: train-step 2556, loss 0.287066, acc 0.94
2016-05-20T22:29:36.975202: train-step 2557, loss 0.333346, acc 0.9
2016-05-20T22:29:45.928031: train-step 2558, loss 0.309644, acc 0.89
2016-05-20T22:29:55.040714: train-step 2559, loss 0.305084, acc 0.88
2016-05-20T22:30:04.206673: train-step 2560, loss 0.243115, acc 0.96
2016-05-20T22:30:13.435602: train-step 2561, loss 0.30031, acc 0.92
2016-05-20T22:30:22.666423: train-step 2562, loss 0.319201, acc 0.9
2016-05-20T22:30:31.518640: train-step 2563, loss 0.289264, acc 0.88
2016-05-20T22:30:39.755263: train-step 2564, loss 0.326924, acc 0.89
2016-05-20T22:30:48.265107: train-step 2565, loss 0.35406, acc 0.89
2016-05-20T22:30:56.618159: train-step 2566, loss 0.279637, acc 0.92
2016-05-20T22:31:04.978146: train-step 2567, loss 0.246984, acc 0.97
2016-05-20T22:31:12.930489: train-step 2568, loss 0.245725, acc 0.94
2016-05-20T22:31:20.903789: train-step 2569, loss 0.316384, acc 0.87
2016-05-20T22:31:28.582520: train-step 2570, loss 0.270315, acc 0.92
2016-05-20T22:31:36.618039: train-step 2571, loss 0.272955, acc 0.93
2016-05-20T22:31:44.688485: train-step 2572, loss 0.312343, acc 0.93
2016-05-20T22:31:53.381763: train-step 2573, loss 0.317058, acc 0.94
2016-05-20T22:32:02.486981: train-step 2574, loss 0.297722, acc 0.9
2016-05-20T22:32:11.550824: train-step 2575, loss 0.302805, acc 0.9
2016-05-20T22:32:20.913653: train-step 2576, loss 0.283671, acc 0.91
2016-05-20T22:32:30.054576: train-step 2577, loss 0.305817, acc 0.91
2016-05-20T22:32:39.145971: train-step 2578, loss 0.314344, acc 0.87
2016-05-20T22:32:48.756373: train-step 2579, loss 0.281097, acc 0.92
2016-05-20T22:32:58.246596: train-step 2580, loss 0.306672, acc 0.86
2016-05-20T22:33:07.488297: train-step 2581, loss 0.248812, acc 0.93
2016-05-20T22:33:16.481131: train-step 2582, loss 0.294036, acc 0.89
2016-05-20T22:33:25.836756: train-step 2583, loss 0.333152, acc 0.88
2016-05-20T22:33:35.173783: train-step 2584, loss 0.281997, acc 0.89
2016-05-20T22:33:43.955400: train-step 2585, loss 0.293849, acc 0.93
2016-05-20T22:33:52.679163: train-step 2586, loss 0.330877, acc 0.91
2016-05-20T22:34:01.016186: train-step 2587, loss 0.26014, acc 0.93
2016-05-20T22:34:09.709856: train-step 2588, loss 0.274294, acc 0.93
2016-05-20T22:34:17.426580: train-step 2589, loss 0.293004, acc 0.93
2016-05-20T22:34:26.283276: train-step 2590, loss 0.293297, acc 0.93
2016-05-20T22:34:34.498707: train-step 2591, loss 0.282586, acc 0.94
2016-05-20T22:34:42.207284: train-step 2592, loss 0.249804, acc 0.95
2016-05-20T22:34:50.498706: train-step 2593, loss 0.274804, acc 0.93
2016-05-20T22:34:58.331632: train-step 2594, loss 0.263694, acc 0.94
2016-05-20T22:35:07.018925: train-step 2595, loss 0.345163, acc 0.88
2016-05-20T22:35:16.504334: train-step 2596, loss 0.301095, acc 0.93
2016-05-20T22:35:25.365936: train-step 2597, loss 0.354479, acc 0.88
2016-05-20T22:35:35.051275: train-step 2598, loss 0.346731, acc 0.88
2016-05-20T22:35:44.227952: train-step 2599, loss 0.280831, acc 0.93
2016-05-20T22:35:53.550405: train-step 2600, loss 0.284248, acc 0.91
2016-05-20T22:36:03.057080: train-step 2601, loss 0.347334, acc 0.89
2016-05-20T22:36:12.641987: train-step 2602, loss 0.305751, acc 0.9
2016-05-20T22:36:21.848519: train-step 2603, loss 0.305546, acc 0.91
2016-05-20T22:36:31.282421: train-step 2604, loss 0.24032, acc 0.96
2016-05-20T22:36:40.593302: train-step 2605, loss 0.337114, acc 0.88
2016-05-20T22:36:49.518808: train-step 2606, loss 0.279207, acc 0.92
2016-05-20T22:36:59.406713: train-step 2607, loss 0.436084, acc 0.87
2016-05-20T22:37:08.438225: train-step 2608, loss 0.255385, acc 0.95
2016-05-20T22:37:16.425931: train-step 2609, loss 0.258758, acc 0.93
2016-05-20T22:37:25.715789: train-step 2610, loss 0.346124, acc 0.87
2016-05-20T22:37:33.085868: train-step 2611, loss 0.314798, acc 0.89
2016-05-20T22:37:40.883965: train-step 2612, loss 0.30458, acc 0.9
2016-05-20T22:37:48.999343: train-step 2613, loss 0.308373, acc 0.92
2016-05-20T22:37:56.785260: train-step 2614, loss 0.309373, acc 0.9
2016-05-20T22:38:04.518520: train-step 2615, loss 0.338886, acc 0.85
2016-05-20T22:38:12.558577: train-step 2616, loss 0.34034, acc 0.87
2016-05-20T22:38:21.349345: train-step 2617, loss 0.241935, acc 0.96
2016-05-20T22:38:30.395221: train-step 2618, loss 0.294916, acc 0.92
2016-05-20T22:38:39.657469: train-step 2619, loss 0.291419, acc 0.9
2016-05-20T22:38:49.050328: train-step 2620, loss 0.296537, acc 0.91
2016-05-20T22:38:58.337581: train-step 2621, loss 0.279324, acc 0.92
2016-05-20T22:39:07.429289: train-step 2622, loss 0.242062, acc 0.95
2016-05-20T22:39:16.431538: train-step 2623, loss 0.286817, acc 0.92
2016-05-20T22:39:25.723503: train-step 2624, loss 0.351016, acc 0.9
2016-05-20T22:39:34.993417: train-step 2625, loss 0.268522, acc 0.89
2016-05-20T22:39:44.106480: train-step 2626, loss 0.340337, acc 0.89
2016-05-20T22:39:53.182940: train-step 2627, loss 0.238323, acc 0.94
2016-05-20T22:40:02.487558: train-step 2628, loss 0.359646, acc 0.89
2016-05-20T22:40:11.635207: train-step 2629, loss 0.291524, acc 0.91
2016-05-20T22:40:20.365956: train-step 2630, loss 0.303445, acc 0.93
2016-05-20T22:40:28.338471: train-step 2631, loss 0.306576, acc 0.92
2016-05-20T22:40:37.352074: train-step 2632, loss 0.285814, acc 0.88
2016-05-20T22:40:44.928242: train-step 2633, loss 0.291091, acc 0.91
2016-05-20T22:40:52.784525: train-step 2634, loss 0.3003, acc 0.89
2016-05-20T22:41:00.739225: train-step 2635, loss 0.321328, acc 0.9
2016-05-20T22:41:08.554979: train-step 2636, loss 0.33839, acc 0.89
2016-05-20T22:41:16.332699: train-step 2637, loss 0.292926, acc 0.92
2016-05-20T22:41:24.524919: train-step 2638, loss 0.282904, acc 0.92
2016-05-20T22:41:33.103958: train-step 2639, loss 0.257201, acc 0.96
2016-05-20T22:41:41.774377: train-step 2640, loss 0.338502, acc 0.88
2016-05-20T22:41:51.036173: train-step 2641, loss 0.323905, acc 0.93
2016-05-20T22:42:00.472605: train-step 2642, loss 0.274539, acc 0.95
2016-05-20T22:42:09.575362: train-step 2643, loss 0.369695, acc 0.83
2016-05-20T22:42:18.966218: train-step 2644, loss 0.267924, acc 0.92
2016-05-20T22:42:28.230581: train-step 2645, loss 0.290158, acc 0.92
2016-05-20T22:42:37.504721: train-step 2646, loss 0.319074, acc 0.9
2016-05-20T22:42:46.924068: train-step 2647, loss 0.255952, acc 0.95
2016-05-20T22:42:56.051557: train-step 2648, loss 0.32998, acc 0.89
2016-05-20T22:43:05.235533: train-step 2649, loss 0.40125, acc 0.85
2016-05-20T22:43:14.115958: train-step 2650, loss 0.334912, acc 0.86
2016-05-20T22:43:23.008123: train-step 2651, loss 0.261759, acc 0.91
2016-05-20T22:43:31.652884: train-step 2652, loss 0.319149, acc 0.91
2016-05-20T22:43:39.804320: train-step 2653, loss 0.283501, acc 0.94
2016-05-20T22:43:49.254792: train-step 2654, loss 0.286001, acc 0.91
2016-05-20T22:43:57.131812: train-step 2655, loss 0.309443, acc 0.9
2016-05-20T22:44:05.174961: train-step 2656, loss 0.293578, acc 0.93
2016-05-20T22:44:12.820128: train-step 2657, loss 0.284739, acc 0.91
2016-05-20T22:44:20.675603: train-step 2658, loss 0.310649, acc 0.94
2016-05-20T22:44:28.443318: train-step 2659, loss 0.303468, acc 0.92
2016-05-20T22:44:36.817415: train-step 2660, loss 0.293238, acc 0.92
2016-05-20T22:44:45.173696: train-step 2661, loss 0.250663, acc 0.96
2016-05-20T22:44:54.211895: train-step 2662, loss 0.281458, acc 0.93
2016-05-20T22:45:03.435596: train-step 2663, loss 0.272123, acc 0.93
2016-05-20T22:45:12.291322: train-step 2664, loss 0.330614, acc 0.87
2016-05-20T22:45:21.489957: train-step 2665, loss 0.300376, acc 0.91
2016-05-20T22:45:30.833450: train-step 2666, loss 0.313191, acc 0.89
2016-05-20T22:45:40.192000: train-step 2667, loss 0.270411, acc 0.89
2016-05-20T22:45:49.395009: train-step 2668, loss 0.299642, acc 0.92
2016-05-20T22:45:58.947244: train-step 2669, loss 0.268046, acc 0.93
2016-05-20T22:46:08.307913: train-step 2670, loss 0.240442, acc 0.97
2016-05-20T22:46:17.693784: train-step 2671, loss 0.318651, acc 0.9
2016-05-20T22:46:26.823567: train-step 2672, loss 0.350095, acc 0.89
2016-05-20T22:46:35.909552: train-step 2673, loss 0.310165, acc 0.92
2016-05-20T22:46:45.007379: train-step 2674, loss 0.318968, acc 0.89
2016-05-20T22:46:53.174303: train-step 2675, loss 0.28569, acc 0.9
2016-05-20T22:47:02.686187: train-step 2676, loss 0.27373, acc 0.93
2016-05-20T22:47:10.606504: train-step 2677, loss 0.3309, acc 0.89
2016-05-20T22:47:18.400103: train-step 2678, loss 0.325594, acc 0.88
2016-05-20T22:47:26.393341: train-step 2679, loss 0.241378, acc 0.92
2016-05-20T22:47:34.576963: train-step 2680, loss 0.270936, acc 0.94
2016-05-20T22:47:42.440195: train-step 2681, loss 0.279821, acc 0.93
2016-05-20T22:47:50.429044: train-step 2682, loss 0.253405, acc 0.94
2016-05-20T22:47:59.207201: train-step 2683, loss 0.307495, acc 0.92
2016-05-20T22:48:08.038808: train-step 2684, loss 0.29664, acc 0.88
2016-05-20T22:48:17.463714: train-step 2685, loss 0.313439, acc 0.9
2016-05-20T22:48:26.947749: train-step 2686, loss 0.308381, acc 0.91
2016-05-20T22:48:36.397674: train-step 2687, loss 0.313345, acc 0.91
2016-05-20T22:48:45.635252: train-step 2688, loss 0.238488, acc 0.97
2016-05-20T22:48:54.931428: train-step 2689, loss 0.231184, acc 0.97
2016-05-20T22:49:04.108887: train-step 2690, loss 0.308416, acc 0.9
2016-05-20T22:49:13.485641: train-step 2691, loss 0.275729, acc 0.9
2016-05-20T22:49:22.742867: train-step 2692, loss 0.328235, acc 0.9
2016-05-20T22:49:31.669032: train-step 2693, loss 0.23055, acc 0.97
2016-05-20T22:49:40.993858: train-step 2694, loss 0.350843, acc 0.89
2016-05-20T22:49:50.006078: train-step 2695, loss 0.319323, acc 0.92
2016-05-20T22:49:58.579250: train-step 2696, loss 0.288262, acc 0.91
2016-05-20T22:50:06.458219: train-step 2697, loss 0.292632, acc 0.92
2016-05-20T22:50:15.515326: train-step 2698, loss 0.282686, acc 0.93
2016-05-20T22:50:23.033108: train-step 2699, loss 0.306659, acc 0.91
2016-05-20T22:50:30.740898: train-step 2700, loss 0.280189, acc 0.92
2016-05-20T22:50:38.459889: train-step 2701, loss 0.250363, acc 0.95
2016-05-20T22:50:46.548801: train-step 2702, loss 0.308422, acc 0.91
2016-05-20T22:50:54.478465: train-step 2703, loss 0.384197, acc 0.86
2016-05-20T22:51:02.822844: train-step 2704, loss 0.285943, acc 0.91
2016-05-20T22:51:11.286002: train-step 2705, loss 0.333633, acc 0.87
2016-05-20T22:51:20.217811: train-step 2706, loss 0.303722, acc 0.91
2016-05-20T22:51:29.787375: train-step 2707, loss 0.310535, acc 0.88
2016-05-20T22:51:39.090080: train-step 2708, loss 0.318503, acc 0.88
2016-05-20T22:51:48.485726: train-step 2709, loss 0.349464, acc 0.88
2016-05-20T22:51:57.505912: train-step 2710, loss 0.305365, acc 0.92
2016-05-20T22:52:06.492217: train-step 2711, loss 0.290154, acc 0.92
2016-05-20T22:52:15.733362: train-step 2712, loss 0.30202, acc 0.89
2016-05-20T22:52:25.040675: train-step 2713, loss 0.347498, acc 0.9
2016-05-20T22:52:34.017084: train-step 2714, loss 0.305208, acc 0.91
2016-05-20T22:52:43.161423: train-step 2715, loss 0.306796, acc 0.91
2016-05-20T22:52:52.129645: train-step 2716, loss 0.252866, acc 0.95
2016-05-20T22:53:01.305750: train-step 2717, loss 0.205817, acc 0.96
2016-05-20T22:53:09.910172: train-step 2718, loss 0.31238, acc 0.91
2016-05-20T22:53:18.126818: train-step 2719, loss 0.315954, acc 0.92
2016-05-20T22:53:27.421747: train-step 2720, loss 0.254593, acc 0.94
2016-05-20T22:53:35.137271: train-step 2721, loss 0.261163, acc 0.92
2016-05-20T22:53:43.563839: train-step 2722, loss 0.283907, acc 0.91
2016-05-20T22:53:51.135965: train-step 2723, loss 0.33644, acc 0.87
2016-05-20T22:53:59.016961: train-step 2724, loss 0.285396, acc 0.91
2016-05-20T22:54:07.421821: train-step 2725, loss 0.2621, acc 0.93
2016-05-20T22:54:15.356185: train-step 2726, loss 0.275698, acc 0.92
2016-05-20T22:54:23.588788: train-step 2727, loss 0.260917, acc 0.95
2016-05-20T22:54:32.311970: train-step 2728, loss 0.298618, acc 0.94
2016-05-20T22:54:41.909640: train-step 2729, loss 0.345019, acc 0.88
2016-05-20T22:54:51.185263: train-step 2730, loss 0.301829, acc 0.94
2016-05-20T22:55:00.618317: train-step 2731, loss 0.25657, acc 0.92
2016-05-20T22:55:09.707890: train-step 2732, loss 0.311393, acc 0.89
2016-05-20T22:55:18.828237: train-step 2733, loss 0.306182, acc 0.92
2016-05-20T22:55:28.057364: train-step 2734, loss 0.252367, acc 0.94
2016-05-20T22:55:37.348587: train-step 2735, loss 0.281052, acc 0.93
2016-05-20T22:55:46.798050: train-step 2736, loss 0.364461, acc 0.9
2016-05-20T22:55:55.946197: train-step 2737, loss 0.28629, acc 0.89
2016-05-20T22:56:04.909754: train-step 2738, loss 0.309911, acc 0.89
2016-05-20T22:56:14.118693: train-step 2739, loss 0.278772, acc 0.92
2016-05-20T22:56:23.148754: train-step 2740, loss 0.288062, acc 0.92
2016-05-20T22:56:31.317942: train-step 2741, loss 0.210121, acc 0.99
2016-05-20T22:56:40.582363: train-step 2742, loss 0.300067, acc 0.92
2016-05-20T22:56:48.382561: train-step 2743, loss 0.32905, acc 0.89
2016-05-20T22:56:56.284942: train-step 2744, loss 0.256198, acc 0.96
2016-05-20T22:57:04.688741: train-step 2745, loss 0.247496, acc 0.92
2016-05-20T22:57:12.337543: train-step 2746, loss 0.202315, acc 0.97
2016-05-20T22:57:20.072371: train-step 2747, loss 0.271105, acc 0.93
2016-05-20T22:57:27.942336: train-step 2748, loss 0.293841, acc 0.93
2016-05-20T22:57:36.196259: train-step 2749, loss 0.251271, acc 0.94
2016-05-20T22:57:45.257993: train-step 2750, loss 0.318277, acc 0.9
epoch number is: 11
2016-05-20T22:57:54.563012: train-step 2751, loss 0.23911, acc 0.94
2016-05-20T22:58:03.788143: train-step 2752, loss 0.262374, acc 0.94
2016-05-20T22:58:13.151602: train-step 2753, loss 0.224606, acc 0.96
2016-05-20T22:58:22.581937: train-step 2754, loss 0.254454, acc 0.93
2016-05-20T22:58:31.758254: train-step 2755, loss 0.292414, acc 0.91
2016-05-20T22:58:40.885129: train-step 2756, loss 0.290108, acc 0.92
2016-05-20T22:58:50.048356: train-step 2757, loss 0.372764, acc 0.87
2016-05-20T22:58:59.123000: train-step 2758, loss 0.285581, acc 0.95
2016-05-20T22:59:08.293117: train-step 2759, loss 0.310132, acc 0.9
2016-05-20T22:59:17.204547: train-step 2760, loss 0.234994, acc 0.94
2016-05-20T22:59:26.291433: train-step 2761, loss 0.273801, acc 0.91
2016-05-20T22:59:34.991411: train-step 2762, loss 0.214425, acc 0.95
2016-05-20T22:59:43.116117: train-step 2763, loss 0.286294, acc 0.89
2016-05-20T22:59:53.158193: train-step 2764, loss 0.282458, acc 0.91
2016-05-20T23:00:01.489082: train-step 2765, loss 0.281974, acc 0.91
2016-05-20T23:00:09.590367: train-step 2766, loss 0.238974, acc 0.92
2016-05-20T23:00:17.305797: train-step 2767, loss 0.318659, acc 0.9
2016-05-20T23:00:25.195374: train-step 2768, loss 0.359526, acc 0.91
2016-05-20T23:00:32.773457: train-step 2769, loss 0.240098, acc 0.92
2016-05-20T23:00:40.567742: train-step 2770, loss 0.320874, acc 0.92
2016-05-20T23:00:48.793584: train-step 2771, loss 0.254593, acc 0.93
2016-05-20T23:00:57.502422: train-step 2772, loss 0.287526, acc 0.94
2016-05-20T23:01:06.762112: train-step 2773, loss 0.256901, acc 0.95
2016-05-20T23:01:15.859664: train-step 2774, loss 0.243904, acc 0.94
2016-05-20T23:01:25.032114: train-step 2775, loss 0.342183, acc 0.88
2016-05-20T23:01:34.468909: train-step 2776, loss 0.318252, acc 0.89
2016-05-20T23:01:43.839902: train-step 2777, loss 0.278953, acc 0.93
2016-05-20T23:01:52.943408: train-step 2778, loss 0.319585, acc 0.94
2016-05-20T23:02:02.200525: train-step 2779, loss 0.243047, acc 0.94
2016-05-20T23:02:11.562595: train-step 2780, loss 0.33684, acc 0.9
2016-05-20T23:02:20.818704: train-step 2781, loss 0.239805, acc 0.95
2016-05-20T23:02:30.011629: train-step 2782, loss 0.230521, acc 0.94
2016-05-20T23:02:38.876176: train-step 2783, loss 0.331855, acc 0.91
2016-05-20T23:02:47.695748: train-step 2784, loss 0.284674, acc 0.93
2016-05-20T23:02:55.784601: train-step 2785, loss 0.304308, acc 0.9
2016-05-20T23:03:05.016674: train-step 2786, loss 0.244104, acc 0.95
2016-05-20T23:03:12.792912: train-step 2787, loss 0.260157, acc 0.95
2016-05-20T23:03:21.891250: train-step 2788, loss 0.218295, acc 0.99
2016-05-20T23:03:29.913991: train-step 2789, loss 0.349407, acc 0.9
2016-05-20T23:03:37.778517: train-step 2790, loss 0.255391, acc 0.95
2016-05-20T23:03:45.557355: train-step 2791, loss 0.279062, acc 0.94
2016-05-20T23:03:53.530313: train-step 2792, loss 0.261585, acc 0.94
2016-05-20T23:04:01.742721: train-step 2793, loss 0.326023, acc 0.89
2016-05-20T23:04:10.475317: train-step 2794, loss 0.295822, acc 0.94
2016-05-20T23:04:19.576249: train-step 2795, loss 0.311464, acc 0.88
2016-05-20T23:04:28.881134: train-step 2796, loss 0.264449, acc 0.93
2016-05-20T23:04:37.951427: train-step 2797, loss 0.253668, acc 0.93
2016-05-20T23:04:47.304634: train-step 2798, loss 0.313111, acc 0.91
2016-05-20T23:04:56.499646: train-step 2799, loss 0.286649, acc 0.93
2016-05-20T23:05:05.669963: train-step 2800, loss 0.23492, acc 0.98
2016-05-20T23:05:14.827812: train-step 2801, loss 0.247241, acc 0.92
2016-05-20T23:05:24.325353: train-step 2802, loss 0.28281, acc 0.9
2016-05-20T23:05:33.627219: train-step 2803, loss 0.319913, acc 0.91
2016-05-20T23:05:42.755334: train-step 2804, loss 0.291064, acc 0.94
2016-05-20T23:05:51.584950: train-step 2805, loss 0.370477, acc 0.88
2016-05-20T23:06:00.428051: train-step 2806, loss 0.352514, acc 0.89
2016-05-20T23:06:08.721362: train-step 2807, loss 0.289585, acc 0.94
2016-05-20T23:06:18.215961: train-step 2808, loss 0.314605, acc 0.9
2016-05-20T23:06:25.942539: train-step 2809, loss 0.259349, acc 0.93
2016-05-20T23:06:33.967076: train-step 2810, loss 0.335628, acc 0.9
2016-05-20T23:06:42.440730: train-step 2811, loss 0.236303, acc 0.93
2016-05-20T23:06:50.485716: train-step 2812, loss 0.318764, acc 0.92
2016-05-20T23:06:58.194200: train-step 2813, loss 0.29498, acc 0.93
2016-05-20T23:07:06.181003: train-step 2814, loss 0.283993, acc 0.94
2016-05-20T23:07:14.293299: train-step 2815, loss 0.242129, acc 0.96
2016-05-20T23:07:22.968979: train-step 2816, loss 0.336104, acc 0.91
2016-05-20T23:07:32.081509: train-step 2817, loss 0.291421, acc 0.93
2016-05-20T23:07:41.433861: train-step 2818, loss 0.248708, acc 0.95
2016-05-20T23:07:50.872696: train-step 2819, loss 0.239489, acc 0.95
2016-05-20T23:08:00.111200: train-step 2820, loss 0.250662, acc 0.95
2016-05-20T23:08:09.442628: train-step 2821, loss 0.269434, acc 0.92
2016-05-20T23:08:18.486287: train-step 2822, loss 0.268555, acc 0.94
2016-05-20T23:08:27.501005: train-step 2823, loss 0.282513, acc 0.9
2016-05-20T23:08:36.503485: train-step 2824, loss 0.270314, acc 0.95
2016-05-20T23:08:45.625143: train-step 2825, loss 0.237042, acc 0.97
2016-05-20T23:08:55.049265: train-step 2826, loss 0.308182, acc 0.92
2016-05-20T23:09:04.208728: train-step 2827, loss 0.268089, acc 0.94
2016-05-20T23:09:13.157614: train-step 2828, loss 0.23703, acc 0.96
2016-05-20T23:09:21.494859: train-step 2829, loss 0.248518, acc 0.94
2016-05-20T23:09:29.939383: train-step 2830, loss 0.315722, acc 0.91
2016-05-20T23:09:38.913275: train-step 2831, loss 0.269749, acc 0.95
2016-05-20T23:09:47.105149: train-step 2832, loss 0.273113, acc 0.93
2016-05-20T23:09:54.938446: train-step 2833, loss 0.268738, acc 0.92
2016-05-20T23:10:02.988350: train-step 2834, loss 0.393531, acc 0.87
2016-05-20T23:10:10.748227: train-step 2835, loss 0.282521, acc 0.93
2016-05-20T23:10:18.477429: train-step 2836, loss 0.325605, acc 0.92
2016-05-20T23:10:26.684664: train-step 2837, loss 0.306874, acc 0.9
2016-05-20T23:10:35.157993: train-step 2838, loss 0.26541, acc 0.94
2016-05-20T23:10:44.492280: train-step 2839, loss 0.248064, acc 0.93
2016-05-20T23:10:53.702951: train-step 2840, loss 0.304803, acc 0.92
2016-05-20T23:11:03.164099: train-step 2841, loss 0.277128, acc 0.93
2016-05-20T23:11:12.275925: train-step 2842, loss 0.271445, acc 0.94
2016-05-20T23:11:21.285643: train-step 2843, loss 0.34506, acc 0.89
2016-05-20T23:11:30.435732: train-step 2844, loss 0.283109, acc 0.94
2016-05-20T23:11:39.329771: train-step 2845, loss 0.31498, acc 0.88
2016-05-20T23:11:48.645866: train-step 2846, loss 0.291803, acc 0.9
2016-05-20T23:11:57.662753: train-step 2847, loss 0.285815, acc 0.92
2016-05-20T23:12:06.848921: train-step 2848, loss 0.2529, acc 0.93
2016-05-20T23:12:15.991793: train-step 2849, loss 0.288767, acc 0.92
2016-05-20T23:12:24.842250: train-step 2850, loss 0.25558, acc 0.95
2016-05-20T23:12:33.430142: train-step 2851, loss 0.325309, acc 0.9
2016-05-20T23:12:41.584502: train-step 2852, loss 0.314248, acc 0.9
2016-05-20T23:12:50.759165: train-step 2853, loss 0.227329, acc 0.97
2016-05-20T23:12:58.339451: train-step 2854, loss 0.234989, acc 0.96
2016-05-20T23:13:06.792695: train-step 2855, loss 0.317823, acc 0.9
2016-05-20T23:13:15.021887: train-step 2856, loss 0.311565, acc 0.89
2016-05-20T23:13:22.872317: train-step 2857, loss 0.306005, acc 0.87
2016-05-20T23:13:30.577526: train-step 2858, loss 0.257178, acc 0.94
2016-05-20T23:13:38.825624: train-step 2859, loss 0.263711, acc 0.94
2016-05-20T23:13:47.381000: train-step 2860, loss 0.222795, acc 0.96
2016-05-20T23:13:56.356042: train-step 2861, loss 0.295189, acc 0.91
2016-05-20T23:14:05.396706: train-step 2862, loss 0.343189, acc 0.9
2016-05-20T23:14:14.671879: train-step 2863, loss 0.280825, acc 0.93
2016-05-20T23:14:24.065973: train-step 2864, loss 0.344841, acc 0.88
2016-05-20T23:14:33.274539: train-step 2865, loss 0.265852, acc 0.94
2016-05-20T23:14:42.578185: train-step 2866, loss 0.309941, acc 0.89
2016-05-20T23:14:52.213722: train-step 2867, loss 0.23902, acc 0.94
2016-05-20T23:15:01.245356: train-step 2868, loss 0.222866, acc 0.95
2016-05-20T23:15:10.481947: train-step 2869, loss 0.285166, acc 0.93
2016-05-20T23:15:19.889381: train-step 2870, loss 0.243515, acc 0.94
2016-05-20T23:15:29.089104: train-step 2871, loss 0.277901, acc 0.93
2016-05-20T23:15:38.421242: train-step 2872, loss 0.276044, acc 0.93
2016-05-20T23:15:46.835361: train-step 2873, loss 0.299125, acc 0.9
2016-05-20T23:15:54.857329: train-step 2874, loss 0.301902, acc 0.9
2016-05-20T23:16:04.260014: train-step 2875, loss 0.282774, acc 0.93
2016-05-20T23:16:12.174592: train-step 2876, loss 0.341837, acc 0.88
2016-05-20T23:16:20.168259: train-step 2877, loss 0.336627, acc 0.88
2016-05-20T23:16:28.187838: train-step 2878, loss 0.308799, acc 0.9
2016-05-20T23:16:35.914250: train-step 2879, loss 0.25534, acc 0.94
2016-05-20T23:16:43.598602: train-step 2880, loss 0.317142, acc 0.89
2016-05-20T23:16:51.370265: train-step 2881, loss 0.290594, acc 0.92
2016-05-20T23:16:59.790543: train-step 2882, loss 0.315169, acc 0.89
2016-05-20T23:17:08.548020: train-step 2883, loss 0.26043, acc 0.95
2016-05-20T23:17:17.688677: train-step 2884, loss 0.28457, acc 0.91
2016-05-20T23:17:26.976886: train-step 2885, loss 0.291024, acc 0.94
2016-05-20T23:17:36.017044: train-step 2886, loss 0.287781, acc 0.91
2016-05-20T23:17:45.164117: train-step 2887, loss 0.261289, acc 0.92
2016-05-20T23:17:54.846568: train-step 2888, loss 0.359792, acc 0.88
2016-05-20T23:18:04.211906: train-step 2889, loss 0.260958, acc 0.93
2016-05-20T23:18:13.427472: train-step 2890, loss 0.271041, acc 0.89
2016-05-20T23:18:22.792776: train-step 2891, loss 0.30887, acc 0.9
2016-05-20T23:18:32.018215: train-step 2892, loss 0.284653, acc 0.9
2016-05-20T23:18:41.091048: train-step 2893, loss 0.29781, acc 0.89
2016-05-20T23:18:50.728385: train-step 2894, loss 0.263156, acc 0.91
2016-05-20T23:18:59.659236: train-step 2895, loss 0.263775, acc 0.94
2016-05-20T23:19:07.674907: train-step 2896, loss 0.289924, acc 0.93
2016-05-20T23:19:17.014797: train-step 2897, loss 0.254469, acc 0.95
2016-05-20T23:19:24.531246: train-step 2898, loss 0.361765, acc 0.84
2016-05-20T23:19:32.857188: train-step 2899, loss 0.270428, acc 0.93
2016-05-20T23:19:40.738831: train-step 2900, loss 0.337497, acc 0.89
2016-05-20T23:19:48.397432: train-step 2901, loss 0.294526, acc 0.89
2016-05-20T23:19:56.353918: train-step 2902, loss 0.255733, acc 0.96
2016-05-20T23:20:04.382903: train-step 2903, loss 0.315115, acc 0.92
2016-05-20T23:20:12.845000: train-step 2904, loss 0.333759, acc 0.92
2016-05-20T23:20:21.829430: train-step 2905, loss 0.26136, acc 0.95
2016-05-20T23:20:30.790880: train-step 2906, loss 0.239698, acc 0.96
2016-05-20T23:20:40.029879: train-step 2907, loss 0.224364, acc 0.94
2016-05-20T23:20:49.294110: train-step 2908, loss 0.262284, acc 0.95
2016-05-20T23:20:58.641391: train-step 2909, loss 0.346508, acc 0.88
2016-05-20T23:21:07.960987: train-step 2910, loss 0.266347, acc 0.94
2016-05-20T23:21:17.007720: train-step 2911, loss 0.242038, acc 0.96
2016-05-20T23:21:26.086097: train-step 2912, loss 0.288695, acc 0.91
2016-05-20T23:21:35.337551: train-step 2913, loss 0.283215, acc 0.91
2016-05-20T23:21:44.332750: train-step 2914, loss 0.354383, acc 0.87
2016-05-20T23:21:53.399431: train-step 2915, loss 0.327238, acc 0.87
2016-05-20T23:22:02.390733: train-step 2916, loss 0.378844, acc 0.88
2016-05-20T23:22:11.090495: train-step 2917, loss 0.304029, acc 0.93
2016-05-20T23:22:19.350456: train-step 2918, loss 0.276572, acc 0.93
2016-05-20T23:22:28.855715: train-step 2919, loss 0.337845, acc 0.87
2016-05-20T23:22:36.521219: train-step 2920, loss 0.228479, acc 0.96
2016-05-20T23:22:44.392288: train-step 2921, loss 0.280457, acc 0.91
2016-05-20T23:22:52.240439: train-step 2922, loss 0.267977, acc 0.91
2016-05-20T23:23:00.235349: train-step 2923, loss 0.318944, acc 0.9
2016-05-20T23:23:07.876694: train-step 2924, loss 0.249268, acc 0.93
2016-05-20T23:23:15.671982: train-step 2925, loss 0.268391, acc 0.92
2016-05-20T23:23:23.865453: train-step 2926, loss 0.256278, acc 0.93
2016-05-20T23:23:32.565006: train-step 2927, loss 0.322744, acc 0.88
2016-05-20T23:23:41.670575: train-step 2928, loss 0.238, acc 0.97
2016-05-20T23:23:50.999722: train-step 2929, loss 0.326969, acc 0.89
2016-05-20T23:24:00.074086: train-step 2930, loss 0.264315, acc 0.94
2016-05-20T23:24:09.312883: train-step 2931, loss 0.265701, acc 0.93
2016-05-20T23:24:18.520411: train-step 2932, loss 0.321176, acc 0.9
2016-05-20T23:24:27.953401: train-step 2933, loss 0.295611, acc 0.91
2016-05-20T23:24:36.913896: train-step 2934, loss 0.300782, acc 0.92
2016-05-20T23:24:46.304286: train-step 2935, loss 0.376749, acc 0.84
2016-05-20T23:24:55.413361: train-step 2936, loss 0.30271, acc 0.9
2016-05-20T23:25:04.727198: train-step 2937, loss 0.328612, acc 0.88
2016-05-20T23:25:14.115040: train-step 2938, loss 0.289329, acc 0.92
2016-05-20T23:25:22.866856: train-step 2939, loss 0.284157, acc 0.9
2016-05-20T23:25:30.977577: train-step 2940, loss 0.252566, acc 0.93
2016-05-20T23:25:40.371828: train-step 2941, loss 0.268349, acc 0.94
2016-05-20T23:25:48.069972: train-step 2942, loss 0.259514, acc 0.95
2016-05-20T23:25:56.452219: train-step 2943, loss 0.234054, acc 0.95
2016-05-20T23:26:04.201116: train-step 2944, loss 0.27009, acc 0.93
2016-05-20T23:26:12.905718: train-step 2945, loss 0.281873, acc 0.93
2016-05-20T23:26:20.906513: train-step 2946, loss 0.284617, acc 0.91
2016-05-20T23:26:28.877256: train-step 2947, loss 0.280825, acc 0.93
2016-05-20T23:26:37.675693: train-step 2948, loss 0.269584, acc 0.93
2016-05-20T23:26:46.755169: train-step 2949, loss 0.300279, acc 0.9
2016-05-20T23:26:55.969481: train-step 2950, loss 0.298443, acc 0.92
2016-05-20T23:27:05.291652: train-step 2951, loss 0.340468, acc 0.87
2016-05-20T23:27:14.901627: train-step 2952, loss 0.273545, acc 0.94
2016-05-20T23:27:24.223921: train-step 2953, loss 0.256114, acc 0.95
2016-05-20T23:27:33.317096: train-step 2954, loss 0.327013, acc 0.89
2016-05-20T23:27:42.480367: train-step 2955, loss 0.309331, acc 0.9
2016-05-20T23:27:51.501991: train-step 2956, loss 0.305496, acc 0.9
2016-05-20T23:28:00.670332: train-step 2957, loss 0.275063, acc 0.93
2016-05-20T23:28:09.826367: train-step 2958, loss 0.290766, acc 0.89
2016-05-20T23:28:19.057711: train-step 2959, loss 0.264996, acc 0.9
2016-05-20T23:28:28.269478: train-step 2960, loss 0.223534, acc 0.98
2016-05-20T23:28:37.097363: train-step 2961, loss 0.260597, acc 0.95
2016-05-20T23:28:45.384703: train-step 2962, loss 0.271954, acc 0.93
2016-05-20T23:28:54.908843: train-step 2963, loss 0.284947, acc 0.94
2016-05-20T23:29:02.653742: train-step 2964, loss 0.265185, acc 0.93
2016-05-20T23:29:10.293342: train-step 2965, loss 0.302503, acc 0.89
2016-05-20T23:29:17.696570: train-step 2966, loss 0.27548, acc 0.94
2016-05-20T23:29:25.290298: train-step 2967, loss 0.241756, acc 0.94
2016-05-20T23:29:32.864567: train-step 2968, loss 0.35322, acc 0.84
2016-05-20T23:29:40.810036: train-step 2969, loss 0.335616, acc 0.88
2016-05-20T23:29:48.769364: train-step 2970, loss 0.331158, acc 0.9
2016-05-20T23:29:57.460197: train-step 2971, loss 0.244284, acc 0.94
2016-05-20T23:30:07.008829: train-step 2972, loss 0.299561, acc 0.92
2016-05-20T23:30:16.222246: train-step 2973, loss 0.260979, acc 0.93
2016-05-20T23:30:25.435589: train-step 2974, loss 0.294836, acc 0.9
2016-05-20T23:30:34.751497: train-step 2975, loss 0.248236, acc 0.96
2016-05-20T23:30:44.142652: train-step 2976, loss 0.295833, acc 0.91
2016-05-20T23:30:53.140561: train-step 2977, loss 0.304169, acc 0.9
2016-05-20T23:31:02.449260: train-step 2978, loss 0.308225, acc 0.89
2016-05-20T23:31:11.380011: train-step 2979, loss 0.307469, acc 0.92
2016-05-20T23:31:20.878593: train-step 2980, loss 0.293192, acc 0.89
2016-05-20T23:31:30.293524: train-step 2981, loss 0.24053, acc 0.94
2016-05-20T23:31:39.641101: train-step 2982, loss 0.323515, acc 0.91
2016-05-20T23:31:48.613307: train-step 2983, loss 0.348204, acc 0.89
2016-05-20T23:31:57.153612: train-step 2984, loss 0.299633, acc 0.91
2016-05-20T23:32:05.201651: train-step 2985, loss 0.260428, acc 0.93
2016-05-20T23:32:14.638768: train-step 2986, loss 0.268042, acc 0.95
2016-05-20T23:32:22.670836: train-step 2987, loss 0.260333, acc 0.95
2016-05-20T23:32:30.436435: train-step 2988, loss 0.27955, acc 0.92
2016-05-20T23:32:38.284470: train-step 2989, loss 0.246559, acc 0.93
2016-05-20T23:32:46.039739: train-step 2990, loss 0.343307, acc 0.86
2016-05-20T23:32:53.834718: train-step 2991, loss 0.339509, acc 0.89
2016-05-20T23:33:01.811352: train-step 2992, loss 0.287538, acc 0.93
2016-05-20T23:33:10.382470: train-step 2993, loss 0.269452, acc 0.95
2016-05-20T23:33:19.276210: train-step 2994, loss 0.270274, acc 0.91
2016-05-20T23:33:28.690475: train-step 2995, loss 0.25771, acc 0.93
2016-05-20T23:33:37.948798: train-step 2996, loss 0.311128, acc 0.89
2016-05-20T23:33:47.437927: train-step 2997, loss 0.304942, acc 0.87
2016-05-20T23:33:56.539601: train-step 2998, loss 0.2598, acc 0.94
2016-05-20T23:34:05.920903: train-step 2999, loss 0.286102, acc 0.93
2016-05-20T23:34:15.042998: train-step 3000, loss 0.294019, acc 0.91
epoch number is: 12
2016-05-20T23:34:24.439349: train-step 3001, loss 0.280083, acc 0.94
2016-05-20T23:34:33.949175: train-step 3002, loss 0.278778, acc 0.93
2016-05-20T23:34:43.130347: train-step 3003, loss 0.289931, acc 0.92
2016-05-20T23:34:52.311593: train-step 3004, loss 0.255821, acc 0.96
2016-05-20T23:35:01.435999: train-step 3005, loss 0.245425, acc 0.93
2016-05-20T23:35:09.873652: train-step 3006, loss 0.30064, acc 0.89
2016-05-20T23:35:18.008086: train-step 3007, loss 0.226454, acc 0.97
2016-05-20T23:35:27.369741: train-step 3008, loss 0.260887, acc 0.9
2016-05-20T23:35:34.793632: train-step 3009, loss 0.215021, acc 0.98
2016-05-20T23:35:42.512158: train-step 3010, loss 0.314516, acc 0.92
2016-05-20T23:35:50.293736: train-step 3011, loss 0.272196, acc 0.94
2016-05-20T23:35:57.956969: train-step 3012, loss 0.320773, acc 0.88
2016-05-20T23:36:05.819825: train-step 3013, loss 0.291112, acc 0.91
2016-05-20T23:36:14.057026: train-step 3014, loss 0.246932, acc 0.91
2016-05-20T23:36:22.572099: train-step 3015, loss 0.269179, acc 0.92
2016-05-20T23:36:31.737156: train-step 3016, loss 0.335082, acc 0.89
2016-05-20T23:36:40.997584: train-step 3017, loss 0.256535, acc 0.93
2016-05-20T23:36:50.328624: train-step 3018, loss 0.264272, acc 0.94
2016-05-20T23:36:59.583937: train-step 3019, loss 0.257519, acc 0.93
2016-05-20T23:37:08.756710: train-step 3020, loss 0.25548, acc 0.94
2016-05-20T23:37:18.103250: train-step 3021, loss 0.257162, acc 0.95
2016-05-20T23:37:27.329437: train-step 3022, loss 0.313287, acc 0.9
2016-05-20T23:37:36.627711: train-step 3023, loss 0.309953, acc 0.89
2016-05-20T23:37:46.294166: train-step 3024, loss 0.311445, acc 0.9
2016-05-20T23:37:55.866245: train-step 3025, loss 0.328112, acc 0.87
2016-05-20T23:38:04.840623: train-step 3026, loss 0.270899, acc 0.94
2016-05-20T23:38:14.177490: train-step 3027, loss 0.23601, acc 0.96
2016-05-20T23:38:22.869055: train-step 3028, loss 0.266748, acc 0.93
2016-05-20T23:38:30.650134: train-step 3029, loss 0.223254, acc 0.96
2016-05-20T23:38:39.837945: train-step 3030, loss 0.279546, acc 0.94
2016-05-20T23:38:47.468656: train-step 3031, loss 0.280876, acc 0.94
2016-05-20T23:38:55.796175: train-step 3032, loss 0.266792, acc 0.93
2016-05-20T23:39:04.008057: train-step 3033, loss 0.286471, acc 0.89
2016-05-20T23:39:11.665032: train-step 3034, loss 0.3219, acc 0.91
2016-05-20T23:39:19.589382: train-step 3035, loss 0.262875, acc 0.93
2016-05-20T23:39:27.373665: train-step 3036, loss 0.249491, acc 0.95
2016-05-20T23:39:35.794557: train-step 3037, loss 0.27531, acc 0.95
2016-05-20T23:39:44.556301: train-step 3038, loss 0.309896, acc 0.92
2016-05-20T23:39:53.664469: train-step 3039, loss 0.263477, acc 0.92
2016-05-20T23:40:02.953458: train-step 3040, loss 0.282173, acc 0.94
2016-05-20T23:40:12.156445: train-step 3041, loss 0.238167, acc 0.95
2016-05-20T23:40:21.328408: train-step 3042, loss 0.237448, acc 0.94
2016-05-20T23:40:30.391101: train-step 3043, loss 0.285501, acc 0.93
2016-05-20T23:40:39.573864: train-step 3044, loss 0.275462, acc 0.93
2016-05-20T23:40:49.006715: train-step 3045, loss 0.268541, acc 0.93
2016-05-20T23:40:57.895655: train-step 3046, loss 0.241716, acc 0.95
2016-05-20T23:41:07.213819: train-step 3047, loss 0.348653, acc 0.85
2016-05-20T23:41:16.194414: train-step 3048, loss 0.290089, acc 0.9
2016-05-20T23:41:25.059348: train-step 3049, loss 0.259452, acc 0.95
2016-05-20T23:41:33.793486: train-step 3050, loss 0.276297, acc 0.92
2016-05-20T23:41:42.094599: train-step 3051, loss 0.318495, acc 0.9
2016-05-20T23:41:51.574177: train-step 3052, loss 0.264684, acc 0.93
2016-05-20T23:41:59.395796: train-step 3053, loss 0.29724, acc 0.94
2016-05-20T23:42:07.356475: train-step 3054, loss 0.245177, acc 0.95
2016-05-20T23:42:14.945314: train-step 3055, loss 0.305031, acc 0.89
2016-05-20T23:42:23.294651: train-step 3056, loss 0.261045, acc 0.95
2016-05-20T23:42:31.165085: train-step 3057, loss 0.268566, acc 0.93
2016-05-20T23:42:39.567576: train-step 3058, loss 0.263587, acc 0.96
2016-05-20T23:42:47.935900: train-step 3059, loss 0.227357, acc 0.95
2016-05-20T23:42:56.943198: train-step 3060, loss 0.300238, acc 0.9
2016-05-20T23:43:06.150729: train-step 3061, loss 0.254897, acc 0.92
2016-05-20T23:43:15.545417: train-step 3062, loss 0.324897, acc 0.92
2016-05-20T23:43:24.812086: train-step 3063, loss 0.28013, acc 0.93
2016-05-20T23:43:34.297413: train-step 3064, loss 0.260021, acc 0.95
2016-05-20T23:43:43.674809: train-step 3065, loss 0.276663, acc 0.93
2016-05-20T23:43:53.403270: train-step 3066, loss 0.251816, acc 0.93
2016-05-20T23:44:02.979934: train-step 3067, loss 0.26052, acc 0.91
2016-05-20T23:44:12.267082: train-step 3068, loss 0.2587, acc 0.91
2016-05-20T23:44:21.634313: train-step 3069, loss 0.262371, acc 0.95
2016-05-20T23:44:30.818970: train-step 3070, loss 0.303245, acc 0.94
2016-05-20T23:44:39.630359: train-step 3071, loss 0.214446, acc 0.95
2016-05-20T23:44:48.190585: train-step 3072, loss 0.341908, acc 0.89
2016-05-20T23:44:56.348694: train-step 3073, loss 0.264282, acc 0.94
2016-05-20T23:45:05.445428: train-step 3074, loss 0.346309, acc 0.87
2016-05-20T23:45:13.282190: train-step 3075, loss 0.2786, acc 0.92
2016-05-20T23:45:21.228918: train-step 3076, loss 0.23754, acc 0.94
2016-05-20T23:45:29.350407: train-step 3077, loss 0.335899, acc 0.92
2016-05-20T23:45:37.046979: train-step 3078, loss 0.226299, acc 0.95
2016-05-20T23:45:45.074868: train-step 3079, loss 0.263056, acc 0.95
2016-05-20T23:45:52.934656: train-step 3080, loss 0.231772, acc 0.95
2016-05-20T23:46:01.598984: train-step 3081, loss 0.315508, acc 0.9
2016-05-20T23:46:10.457780: train-step 3082, loss 0.268562, acc 0.89
2016-05-20T23:46:19.523599: train-step 3083, loss 0.228801, acc 0.97
2016-05-20T23:46:28.825220: train-step 3084, loss 0.282752, acc 0.92
2016-05-20T23:46:37.951544: train-step 3085, loss 0.316497, acc 0.88
2016-05-20T23:46:47.042818: train-step 3086, loss 0.255408, acc 0.95
2016-05-20T23:46:56.182577: train-step 3087, loss 0.244224, acc 0.94
2016-05-20T23:47:05.222998: train-step 3088, loss 0.26009, acc 0.93
2016-05-20T23:47:14.673971: train-step 3089, loss 0.241092, acc 0.94
2016-05-20T23:47:23.860686: train-step 3090, loss 0.247767, acc 0.93
2016-05-20T23:47:33.207862: train-step 3091, loss 0.23746, acc 0.93
2016-05-20T23:47:42.313470: train-step 3092, loss 0.284238, acc 0.92
2016-05-20T23:47:51.349370: train-step 3093, loss 0.222692, acc 0.95
2016-05-20T23:47:59.875502: train-step 3094, loss 0.330913, acc 0.89
2016-05-20T23:48:07.629322: train-step 3095, loss 0.343082, acc 0.93
2016-05-20T23:48:17.028701: train-step 3096, loss 0.254316, acc 0.94
2016-05-20T23:48:24.523460: train-step 3097, loss 0.27724, acc 0.94
2016-05-20T23:48:32.867978: train-step 3098, loss 0.298289, acc 0.9
2016-05-20T23:48:41.103561: train-step 3099, loss 0.333447, acc 0.9
2016-05-20T23:48:48.757977: train-step 3100, loss 0.271144, acc 0.93
2016-05-20T23:48:56.631006: train-step 3101, loss 0.281255, acc 0.93
2016-05-20T23:49:05.094934: train-step 3102, loss 0.350099, acc 0.85
2016-05-20T23:49:13.914792: train-step 3103, loss 0.224013, acc 0.97
2016-05-20T23:49:23.574216: train-step 3104, loss 0.317183, acc 0.89
2016-05-20T23:49:33.004209: train-step 3105, loss 0.291834, acc 0.92
2016-05-20T23:49:42.256747: train-step 3106, loss 0.2645, acc 0.94
2016-05-20T23:49:51.460839: train-step 3107, loss 0.271871, acc 0.93
2016-05-20T23:50:00.615830: train-step 3108, loss 0.282142, acc 0.93
2016-05-20T23:50:09.404658: train-step 3109, loss 0.203198, acc 0.97
2016-05-20T23:50:18.436380: train-step 3110, loss 0.198636, acc 0.98
2016-05-20T23:50:27.740148: train-step 3111, loss 0.245353, acc 0.95
2016-05-20T23:50:36.819900: train-step 3112, loss 0.289088, acc 0.93
2016-05-20T23:50:46.071920: train-step 3113, loss 0.269154, acc 0.94
2016-05-20T23:50:55.203889: train-step 3114, loss 0.296177, acc 0.89
2016-05-20T23:51:04.091837: train-step 3115, loss 0.333786, acc 0.84
2016-05-20T23:51:12.570502: train-step 3116, loss 0.344033, acc 0.87
2016-05-20T23:51:20.647738: train-step 3117, loss 0.279725, acc 0.92
2016-05-20T23:51:29.528207: train-step 3118, loss 0.242882, acc 0.93
2016-05-20T23:51:37.137721: train-step 3119, loss 0.280526, acc 0.92
2016-05-20T23:51:45.131675: train-step 3120, loss 0.242438, acc 0.95
2016-05-20T23:51:53.330924: train-step 3121, loss 0.246704, acc 0.96
2016-05-20T23:52:01.009899: train-step 3122, loss 0.276659, acc 0.94
2016-05-20T23:52:08.886693: train-step 3123, loss 0.24349, acc 0.94
2016-05-20T23:52:17.144237: train-step 3124, loss 0.342209, acc 0.91
2016-05-20T23:52:25.743209: train-step 3125, loss 0.229544, acc 0.97
2016-05-20T23:52:34.679101: train-step 3126, loss 0.369806, acc 0.9
2016-05-20T23:52:43.732531: train-step 3127, loss 0.334245, acc 0.92
2016-05-20T23:52:53.152357: train-step 3128, loss 0.318815, acc 0.92
2016-05-20T23:53:02.318424: train-step 3129, loss 0.313518, acc 0.88
2016-05-20T23:53:11.330628: train-step 3130, loss 0.254317, acc 0.92
2016-05-20T23:53:20.421304: train-step 3131, loss 0.247399, acc 0.93
2016-05-20T23:53:29.344423: train-step 3132, loss 0.266341, acc 0.9
2016-05-20T23:53:38.556835: train-step 3133, loss 0.248605, acc 0.92
2016-05-20T23:53:48.103987: train-step 3134, loss 0.34862, acc 0.89
2016-05-20T23:53:57.513116: train-step 3135, loss 0.286245, acc 0.89
2016-05-20T23:54:06.574391: train-step 3136, loss 0.256424, acc 0.93
2016-05-20T23:54:15.670969: train-step 3137, loss 0.254482, acc 0.93
2016-05-20T23:54:24.075876: train-step 3138, loss 0.248077, acc 0.96
2016-05-20T23:54:32.448065: train-step 3139, loss 0.209395, acc 0.95
2016-05-20T23:54:41.373937: train-step 3140, loss 0.264222, acc 0.93
2016-05-20T23:54:50.128180: train-step 3141, loss 0.323946, acc 0.88
2016-05-20T23:54:58.115634: train-step 3142, loss 0.268866, acc 0.93
2016-05-20T23:55:06.002015: train-step 3143, loss 0.303227, acc 0.92
2016-05-20T23:55:14.208899: train-step 3144, loss 0.26954, acc 0.93
2016-05-20T23:55:22.233964: train-step 3145, loss 0.30987, acc 0.9
2016-05-20T23:55:30.413794: train-step 3146, loss 0.288259, acc 0.91
2016-05-20T23:55:39.457109: train-step 3147, loss 0.292298, acc 0.9
2016-05-20T23:55:48.611059: train-step 3148, loss 0.323631, acc 0.9
2016-05-20T23:55:57.771884: train-step 3149, loss 0.3457, acc 0.9
2016-05-20T23:56:07.109571: train-step 3150, loss 0.218677, acc 0.98
2016-05-20T23:56:16.422333: train-step 3151, loss 0.244058, acc 0.94
2016-05-20T23:56:25.760675: train-step 3152, loss 0.233372, acc 0.95
2016-05-20T23:56:35.144297: train-step 3153, loss 0.217342, acc 0.98
2016-05-20T23:56:44.162028: train-step 3154, loss 0.222519, acc 0.98
2016-05-20T23:56:53.870000: train-step 3155, loss 0.242206, acc 0.95
2016-05-20T23:57:03.197183: train-step 3156, loss 0.21849, acc 0.97
2016-05-20T23:57:12.285077: train-step 3157, loss 0.338374, acc 0.87
2016-05-20T23:57:21.355250: train-step 3158, loss 0.261106, acc 0.95
2016-05-20T23:57:30.641342: train-step 3159, loss 0.245625, acc 0.94
2016-05-20T23:57:39.203298: train-step 3160, loss 0.240058, acc 0.96
2016-05-20T23:57:47.090081: train-step 3161, loss 0.296044, acc 0.9
2016-05-20T23:57:56.308823: train-step 3162, loss 0.267663, acc 0.94
2016-05-20T23:58:03.796102: train-step 3163, loss 0.244718, acc 0.95
2016-05-20T23:58:11.592816: train-step 3164, loss 0.298172, acc 0.91
2016-05-20T23:58:19.369580: train-step 3165, loss 0.303347, acc 0.89
2016-05-20T23:58:26.756204: train-step 3166, loss 0.216808, acc 0.97
2016-05-20T23:58:34.514926: train-step 3167, loss 0.270328, acc 0.92
2016-05-20T23:58:42.319257: train-step 3168, loss 0.26004, acc 0.95
2016-05-20T23:58:50.849797: train-step 3169, loss 0.303298, acc 0.92
2016-05-20T23:58:59.867029: train-step 3170, loss 0.217591, acc 0.96
2016-05-20T23:59:09.142698: train-step 3171, loss 0.275066, acc 0.91
2016-05-20T23:59:18.477318: train-step 3172, loss 0.391203, acc 0.86
2016-05-20T23:59:27.917789: train-step 3173, loss 0.26451, acc 0.93
2016-05-20T23:59:37.077439: train-step 3174, loss 0.35577, acc 0.87
2016-05-20T23:59:46.429612: train-step 3175, loss 0.269855, acc 0.93
2016-05-20T23:59:55.506168: train-step 3176, loss 0.250942, acc 0.93
2016-05-21T00:00:04.614239: train-step 3177, loss 0.250175, acc 0.94
2016-05-21T00:00:13.698525: train-step 3178, loss 0.29061, acc 0.93
2016-05-21T00:00:22.593600: train-step 3179, loss 0.293392, acc 0.89
2016-05-21T00:00:31.616524: train-step 3180, loss 0.278274, acc 0.95
2016-05-21T00:00:40.690523: train-step 3181, loss 0.265889, acc 0.92
2016-05-21T00:00:49.229586: train-step 3182, loss 0.251749, acc 0.94
2016-05-21T00:00:57.153538: train-step 3183, loss 0.326356, acc 0.89
2016-05-21T00:01:06.472707: train-step 3184, loss 0.307922, acc 0.91
2016-05-21T00:01:14.081541: train-step 3185, loss 0.315698, acc 0.93
2016-05-21T00:01:21.776309: train-step 3186, loss 0.266956, acc 0.92
2016-05-21T00:01:29.673289: train-step 3187, loss 0.261845, acc 0.94
2016-05-21T00:01:37.423187: train-step 3188, loss 0.276834, acc 0.92
2016-05-21T00:01:45.225542: train-step 3189, loss 0.333746, acc 0.88
2016-05-21T00:01:53.155865: train-step 3190, loss 0.276038, acc 0.92
2016-05-21T00:02:01.867817: train-step 3191, loss 0.288294, acc 0.91
2016-05-21T00:02:10.849346: train-step 3192, loss 0.254718, acc 0.93
2016-05-21T00:02:20.135084: train-step 3193, loss 0.318193, acc 0.93
2016-05-21T00:02:29.544275: train-step 3194, loss 0.28591, acc 0.9
2016-05-21T00:02:38.580076: train-step 3195, loss 0.218379, acc 0.96
2016-05-21T00:02:47.987688: train-step 3196, loss 0.23425, acc 0.97
2016-05-21T00:02:57.384358: train-step 3197, loss 0.204986, acc 0.97
2016-05-21T00:03:06.842095: train-step 3198, loss 0.235865, acc 0.94
2016-05-21T00:03:15.980991: train-step 3199, loss 0.178854, acc 0.98
2016-05-21T00:03:25.041126: train-step 3200, loss 0.263587, acc 0.93
2016-05-21T00:03:34.210226: train-step 3201, loss 0.26553, acc 0.92
2016-05-21T00:03:43.387138: train-step 3202, loss 0.268766, acc 0.92
2016-05-21T00:03:52.343604: train-step 3203, loss 0.278934, acc 0.91
2016-05-21T00:04:00.862236: train-step 3204, loss 0.263102, acc 0.92
2016-05-21T00:04:09.086168: train-step 3205, loss 0.381612, acc 0.83
2016-05-21T00:04:18.293527: train-step 3206, loss 0.280905, acc 0.91
2016-05-21T00:04:26.599942: train-step 3207, loss 0.309443, acc 0.92
2016-05-21T00:04:34.377257: train-step 3208, loss 0.241458, acc 0.91
2016-05-21T00:04:42.187602: train-step 3209, loss 0.261525, acc 0.92
2016-05-21T00:04:50.152189: train-step 3210, loss 0.301181, acc 0.89
2016-05-21T00:04:58.022876: train-step 3211, loss 0.275671, acc 0.93
2016-05-21T00:05:05.768400: train-step 3212, loss 0.255373, acc 0.94
2016-05-21T00:05:14.328544: train-step 3213, loss 0.248082, acc 0.94
2016-05-21T00:05:23.328110: train-step 3214, loss 0.247936, acc 0.98
2016-05-21T00:05:32.531716: train-step 3215, loss 0.21458, acc 0.96
2016-05-21T00:05:41.697104: train-step 3216, loss 0.387845, acc 0.85
2016-05-21T00:05:51.049462: train-step 3217, loss 0.329944, acc 0.89
2016-05-21T00:06:00.604969: train-step 3218, loss 0.333745, acc 0.89
2016-05-21T00:06:09.682473: train-step 3219, loss 0.314998, acc 0.9
2016-05-21T00:06:18.721088: train-step 3220, loss 0.33616, acc 0.91
2016-05-21T00:06:27.916465: train-step 3221, loss 0.306219, acc 0.88
2016-05-21T00:06:37.264199: train-step 3222, loss 0.241095, acc 0.94
2016-05-21T00:06:46.471244: train-step 3223, loss 0.250381, acc 0.92
2016-05-21T00:06:55.781811: train-step 3224, loss 0.290051, acc 0.92
2016-05-21T00:07:04.513117: train-step 3225, loss 0.274685, acc 0.9
2016-05-21T00:07:13.305306: train-step 3226, loss 0.253931, acc 0.92
2016-05-21T00:07:21.531887: train-step 3227, loss 0.333472, acc 0.92
2016-05-21T00:07:31.209048: train-step 3228, loss 0.394266, acc 0.86
2016-05-21T00:07:39.006789: train-step 3229, loss 0.299129, acc 0.92
2016-05-21T00:07:47.248994: train-step 3230, loss 0.251884, acc 0.94
2016-05-21T00:07:55.372784: train-step 3231, loss 0.251698, acc 0.94
2016-05-21T00:08:03.036795: train-step 3232, loss 0.270256, acc 0.93
2016-05-21T00:08:11.164639: train-step 3233, loss 0.336054, acc 0.88
2016-05-21T00:08:18.725795: train-step 3234, loss 0.301837, acc 0.93
2016-05-21T00:08:26.726252: train-step 3235, loss 0.33094, acc 0.87
2016-05-21T00:08:35.576188: train-step 3236, loss 0.23632, acc 0.96
2016-05-21T00:08:44.551258: train-step 3237, loss 0.221411, acc 0.95
2016-05-21T00:08:53.821220: train-step 3238, loss 0.299101, acc 0.89
2016-05-21T00:09:02.809585: train-step 3239, loss 0.270935, acc 0.94
2016-05-21T00:09:11.867503: train-step 3240, loss 0.292405, acc 0.91
2016-05-21T00:09:20.850014: train-step 3241, loss 0.358505, acc 0.91
2016-05-21T00:09:30.257886: train-step 3242, loss 0.30277, acc 0.92
2016-05-21T00:09:39.450869: train-step 3243, loss 0.27472, acc 0.93
2016-05-21T00:09:48.682466: train-step 3244, loss 0.268095, acc 0.94
2016-05-21T00:09:57.808254: train-step 3245, loss 0.356673, acc 0.86
2016-05-21T00:10:06.758859: train-step 3246, loss 0.296762, acc 0.93
2016-05-21T00:10:16.183259: train-step 3247, loss 0.313659, acc 0.89
2016-05-21T00:10:25.101495: train-step 3248, loss 0.248174, acc 0.93
2016-05-21T00:10:33.749178: train-step 3249, loss 0.273083, acc 0.94
2016-05-21T00:10:41.526757: train-step 3250, loss 0.269696, acc 0.91
epoch number is: 13
2016-05-21T00:10:51.484242: train-step 3251, loss 0.246391, acc 0.92
2016-05-21T00:10:59.438857: train-step 3252, loss 0.279326, acc 0.94
2016-05-21T00:11:07.551329: train-step 3253, loss 0.213118, acc 0.97
2016-05-21T00:11:15.702096: train-step 3254, loss 0.296387, acc 0.92
2016-05-21T00:11:23.568877: train-step 3255, loss 0.312651, acc 0.91
2016-05-21T00:11:31.773193: train-step 3256, loss 0.279288, acc 0.9
2016-05-21T00:11:39.776929: train-step 3257, loss 0.288075, acc 0.89
2016-05-21T00:11:48.634921: train-step 3258, loss 0.263982, acc 0.94
2016-05-21T00:11:57.675038: train-step 3259, loss 0.264429, acc 0.94
2016-05-21T00:12:06.866253: train-step 3260, loss 0.253916, acc 0.93
2016-05-21T00:12:15.991487: train-step 3261, loss 0.31631, acc 0.9
2016-05-21T00:12:25.169926: train-step 3262, loss 0.263046, acc 0.94
2016-05-21T00:12:34.346592: train-step 3263, loss 0.233278, acc 0.96
2016-05-21T00:12:43.256902: train-step 3264, loss 0.261293, acc 0.94
2016-05-21T00:12:52.975367: train-step 3265, loss 0.193105, acc 0.98
2016-05-21T00:13:02.355446: train-step 3266, loss 0.270139, acc 0.93
2016-05-21T00:13:11.544927: train-step 3267, loss 0.244809, acc 0.97
2016-05-21T00:13:20.742698: train-step 3268, loss 0.350149, acc 0.9
2016-05-21T00:13:29.937675: train-step 3269, loss 0.206351, acc 0.96
2016-05-21T00:13:38.907264: train-step 3270, loss 0.206504, acc 0.95
2016-05-21T00:13:47.549123: train-step 3271, loss 0.260356, acc 0.93
2016-05-21T00:13:56.099448: train-step 3272, loss 0.246437, acc 0.95
2016-05-21T00:14:05.454717: train-step 3273, loss 0.239545, acc 0.93
2016-05-21T00:14:13.177634: train-step 3274, loss 0.233825, acc 0.95
2016-05-21T00:14:20.957654: train-step 3275, loss 0.284067, acc 0.92
2016-05-21T00:14:28.657376: train-step 3276, loss 0.344052, acc 0.9
2016-05-21T00:14:36.436773: train-step 3277, loss 0.265954, acc 0.9
2016-05-21T00:14:43.960193: train-step 3278, loss 0.29932, acc 0.91
2016-05-21T00:14:52.064562: train-step 3279, loss 0.210486, acc 0.96
2016-05-21T00:15:00.102665: train-step 3280, loss 0.286627, acc 0.92
2016-05-21T00:15:08.931207: train-step 3281, loss 0.232855, acc 0.96
2016-05-21T00:15:17.984771: train-step 3282, loss 0.295428, acc 0.91
2016-05-21T00:15:27.200771: train-step 3283, loss 0.21425, acc 0.96
2016-05-21T00:15:36.388358: train-step 3284, loss 0.237783, acc 0.96
2016-05-21T00:15:45.753095: train-step 3285, loss 0.260498, acc 0.93
2016-05-21T00:15:55.031998: train-step 3286, loss 0.248398, acc 0.95
2016-05-21T00:16:04.215343: train-step 3287, loss 0.216947, acc 0.95
2016-05-21T00:16:13.211067: train-step 3288, loss 0.271824, acc 0.91
2016-05-21T00:16:22.358030: train-step 3289, loss 0.229372, acc 0.96
2016-05-21T00:16:31.424126: train-step 3290, loss 0.271062, acc 0.92
2016-05-21T00:16:40.392169: train-step 3291, loss 0.221959, acc 0.98
2016-05-21T00:16:49.708465: train-step 3292, loss 0.198347, acc 0.98
2016-05-21T00:16:58.701448: train-step 3293, loss 0.291772, acc 0.9
2016-05-21T00:17:07.299345: train-step 3294, loss 0.252033, acc 0.9
2016-05-21T00:17:15.615059: train-step 3295, loss 0.295677, acc 0.9
2016-05-21T00:17:24.518814: train-step 3296, loss 0.291456, acc 0.89
2016-05-21T00:17:32.310669: train-step 3297, loss 0.226356, acc 0.96
2016-05-21T00:17:39.842193: train-step 3298, loss 0.259284, acc 0.9
2016-05-21T00:17:48.190639: train-step 3299, loss 0.238758, acc 0.93
2016-05-21T00:17:55.708687: train-step 3300, loss 0.21163, acc 0.94
2016-05-21T00:18:03.900873: train-step 3301, loss 0.295157, acc 0.88
2016-05-21T00:18:12.268088: train-step 3302, loss 0.291339, acc 0.9
2016-05-21T00:18:21.143484: train-step 3303, loss 0.311373, acc 0.88
2016-05-21T00:18:29.842288: train-step 3304, loss 0.216588, acc 0.97
2016-05-21T00:18:39.003408: train-step 3305, loss 0.290227, acc 0.93
2016-05-21T00:18:48.292101: train-step 3306, loss 0.255147, acc 0.9
2016-05-21T00:18:57.693651: train-step 3307, loss 0.273229, acc 0.96
2016-05-21T00:19:07.085781: train-step 3308, loss 0.262772, acc 0.93
2016-05-21T00:19:16.140115: train-step 3309, loss 0.23149, acc 0.96
2016-05-21T00:19:25.375248: train-step 3310, loss 0.255426, acc 0.93
2016-05-21T00:19:34.463609: train-step 3311, loss 0.288748, acc 0.92
2016-05-21T00:19:43.451604: train-step 3312, loss 0.307914, acc 0.9
2016-05-21T00:19:52.849127: train-step 3313, loss 0.251848, acc 0.92
2016-05-21T00:20:02.066122: train-step 3314, loss 0.25567, acc 0.94
2016-05-21T00:20:11.162758: train-step 3315, loss 0.317723, acc 0.88
2016-05-21T00:20:19.574721: train-step 3316, loss 0.250156, acc 0.96
2016-05-21T00:20:27.559552: train-step 3317, loss 0.319962, acc 0.91
2016-05-21T00:20:37.013429: train-step 3318, loss 0.269319, acc 0.92
2016-05-21T00:20:44.664913: train-step 3319, loss 0.327137, acc 0.92
2016-05-21T00:20:52.556696: train-step 3320, loss 0.265054, acc 0.94
2016-05-21T00:21:00.412078: train-step 3321, loss 0.313805, acc 0.89
2016-05-21T00:21:08.522186: train-step 3322, loss 0.275891, acc 0.93
2016-05-21T00:21:16.518235: train-step 3323, loss 0.299213, acc 0.88
2016-05-21T00:21:24.491043: train-step 3324, loss 0.367913, acc 0.88
2016-05-21T00:21:32.937754: train-step 3325, loss 0.249634, acc 0.92
2016-05-21T00:21:41.869567: train-step 3326, loss 0.240253, acc 0.93
2016-05-21T00:21:51.238256: train-step 3327, loss 0.255117, acc 0.92
2016-05-21T00:22:00.348222: train-step 3328, loss 0.284529, acc 0.91
2016-05-21T00:22:09.373315: train-step 3329, loss 0.310308, acc 0.91
2016-05-21T00:22:18.314135: train-step 3330, loss 0.339244, acc 0.88
2016-05-21T00:22:27.522956: train-step 3331, loss 0.34538, acc 0.87
2016-05-21T00:22:36.866823: train-step 3332, loss 0.30637, acc 0.89
2016-05-21T00:22:46.074254: train-step 3333, loss 0.268944, acc 0.95
2016-05-21T00:22:55.251915: train-step 3334, loss 0.225837, acc 0.96
2016-05-21T00:23:04.309975: train-step 3335, loss 0.255737, acc 0.93
2016-05-21T00:23:13.724297: train-step 3336, loss 0.226638, acc 0.94
2016-05-21T00:23:22.716303: train-step 3337, loss 0.257277, acc 0.94
2016-05-21T00:23:31.560124: train-step 3338, loss 0.272058, acc 0.95
2016-05-21T00:23:40.196755: train-step 3339, loss 0.274092, acc 0.91
2016-05-21T00:23:48.892246: train-step 3340, loss 0.223906, acc 0.96
2016-05-21T00:23:57.660761: train-step 3341, loss 0.264716, acc 0.93
2016-05-21T00:24:05.540981: train-step 3342, loss 0.251928, acc 0.96
2016-05-21T00:24:13.418753: train-step 3343, loss 0.249063, acc 0.93
2016-05-21T00:24:21.257268: train-step 3344, loss 0.247013, acc 0.94
2016-05-21T00:24:29.255453: train-step 3345, loss 0.222915, acc 0.95
2016-05-21T00:24:37.281206: train-step 3346, loss 0.269847, acc 0.9
2016-05-21T00:24:45.061335: train-step 3347, loss 0.25385, acc 0.98
2016-05-21T00:24:53.722886: train-step 3348, loss 0.287975, acc 0.9
2016-05-21T00:25:02.908971: train-step 3349, loss 0.218639, acc 0.95
2016-05-21T00:25:12.674991: train-step 3350, loss 0.270175, acc 0.9
2016-05-21T00:25:21.726165: train-step 3351, loss 0.220097, acc 0.97
2016-05-21T00:25:31.171960: train-step 3352, loss 0.287372, acc 0.94
2016-05-21T00:25:40.262271: train-step 3353, loss 0.296922, acc 0.91
2016-05-21T00:25:49.519989: train-step 3354, loss 0.303751, acc 0.91
2016-05-21T00:25:59.496923: train-step 3355, loss 0.228178, acc 0.94
2016-05-21T00:26:09.403902: train-step 3356, loss 0.262154, acc 0.93
2016-05-21T00:26:18.557080: train-step 3357, loss 0.238615, acc 0.95
2016-05-21T00:26:27.764274: train-step 3358, loss 0.24975, acc 0.96
2016-05-21T00:26:36.936381: train-step 3359, loss 0.259167, acc 0.92
2016-05-21T00:26:45.973047: train-step 3360, loss 0.300235, acc 0.89
2016-05-21T00:26:54.350205: train-step 3361, loss 0.30823, acc 0.9
2016-05-21T00:27:02.475302: train-step 3362, loss 0.21557, acc 0.95
2016-05-21T00:27:11.533476: train-step 3363, loss 0.230063, acc 0.94
2016-05-21T00:27:19.313563: train-step 3364, loss 0.26877, acc 0.95
2016-05-21T00:27:26.885348: train-step 3365, loss 0.250444, acc 0.93
2016-05-21T00:27:34.982371: train-step 3366, loss 0.264004, acc 0.94
2016-05-21T00:27:42.700553: train-step 3367, loss 0.347092, acc 0.88
2016-05-21T00:27:50.754514: train-step 3368, loss 0.247669, acc 0.93
2016-05-21T00:27:58.608413: train-step 3369, loss 0.240282, acc 0.92
2016-05-21T00:28:07.428115: train-step 3370, loss 0.286638, acc 0.94
2016-05-21T00:28:16.306017: train-step 3371, loss 0.264313, acc 0.93
2016-05-21T00:28:25.154285: train-step 3372, loss 0.277359, acc 0.9
2016-05-21T00:28:34.211268: train-step 3373, loss 0.289049, acc 0.92
2016-05-21T00:28:43.256260: train-step 3374, loss 0.280641, acc 0.92
2016-05-21T00:28:52.473849: train-step 3375, loss 0.214621, acc 0.96
2016-05-21T00:29:01.642953: train-step 3376, loss 0.246527, acc 0.94
2016-05-21T00:29:10.994517: train-step 3377, loss 0.240689, acc 0.95
2016-05-21T00:29:20.321463: train-step 3378, loss 0.254694, acc 0.93
2016-05-21T00:29:29.768371: train-step 3379, loss 0.321277, acc 0.89
2016-05-21T00:29:38.870156: train-step 3380, loss 0.324479, acc 0.93
2016-05-21T00:29:47.602752: train-step 3381, loss 0.237038, acc 0.95
2016-05-21T00:29:56.629523: train-step 3382, loss 0.223611, acc 0.93
2016-05-21T00:30:05.387684: train-step 3383, loss 0.287655, acc 0.92
2016-05-21T00:30:13.603356: train-step 3384, loss 0.287253, acc 0.93
2016-05-21T00:30:22.853483: train-step 3385, loss 0.266478, acc 0.92
2016-05-21T00:30:30.329553: train-step 3386, loss 0.228138, acc 0.98
2016-05-21T00:30:38.223423: train-step 3387, loss 0.252308, acc 0.95
2016-05-21T00:30:46.145679: train-step 3388, loss 0.306018, acc 0.91
2016-05-21T00:30:54.018059: train-step 3389, loss 0.270515, acc 0.94
2016-05-21T00:31:01.579598: train-step 3390, loss 0.265363, acc 0.94
2016-05-21T00:31:09.744898: train-step 3391, loss 0.273523, acc 0.93
2016-05-21T00:31:18.001628: train-step 3392, loss 0.223828, acc 0.97
2016-05-21T00:31:26.838539: train-step 3393, loss 0.311262, acc 0.9
2016-05-21T00:31:36.001393: train-step 3394, loss 0.295754, acc 0.93
2016-05-21T00:31:45.349555: train-step 3395, loss 0.324641, acc 0.89
2016-05-21T00:31:54.744055: train-step 3396, loss 0.257613, acc 0.93
2016-05-21T00:32:03.897628: train-step 3397, loss 0.273377, acc 0.92
2016-05-21T00:32:12.982230: train-step 3398, loss 0.184335, acc 0.98
2016-05-21T00:32:22.126056: train-step 3399, loss 0.23184, acc 0.95
2016-05-21T00:32:31.233987: train-step 3400, loss 0.30622, acc 0.88
2016-05-21T00:32:40.351466: train-step 3401, loss 0.312215, acc 0.87
2016-05-21T00:32:49.493446: train-step 3402, loss 0.305041, acc 0.93
2016-05-21T00:32:58.452617: train-step 3403, loss 0.251766, acc 0.95
2016-05-21T00:33:07.413219: train-step 3404, loss 0.290158, acc 0.9
2016-05-21T00:33:16.238350: train-step 3405, loss 0.32507, acc 0.9
2016-05-21T00:33:24.909469: train-step 3406, loss 0.258129, acc 0.96
2016-05-21T00:33:34.554912: train-step 3407, loss 0.231289, acc 0.97
2016-05-21T00:33:42.222434: train-step 3408, loss 0.258029, acc 0.93
2016-05-21T00:33:50.298224: train-step 3409, loss 0.252488, acc 0.92
2016-05-21T00:33:57.913350: train-step 3410, loss 0.324751, acc 0.89
2016-05-21T00:34:06.125172: train-step 3411, loss 0.324614, acc 0.89
2016-05-21T00:34:13.801179: train-step 3412, loss 0.258982, acc 0.94
2016-05-21T00:34:21.680545: train-step 3413, loss 0.269803, acc 0.9
2016-05-21T00:34:29.681701: train-step 3414, loss 0.285813, acc 0.95
2016-05-21T00:34:38.439626: train-step 3415, loss 0.208797, acc 0.97
2016-05-21T00:34:47.348495: train-step 3416, loss 0.248878, acc 0.94
2016-05-21T00:34:56.639520: train-step 3417, loss 0.266208, acc 0.89
2016-05-21T00:35:05.592695: train-step 3418, loss 0.283237, acc 0.92
2016-05-21T00:35:14.640590: train-step 3419, loss 0.21776, acc 0.97
2016-05-21T00:35:23.719879: train-step 3420, loss 0.257738, acc 0.95
2016-05-21T00:35:32.852103: train-step 3421, loss 0.256134, acc 0.92
2016-05-21T00:35:41.892217: train-step 3422, loss 0.22581, acc 0.93
2016-05-21T00:35:51.250305: train-step 3423, loss 0.23429, acc 0.96
2016-05-21T00:36:00.238342: train-step 3424, loss 0.279581, acc 0.91
2016-05-21T00:36:09.135208: train-step 3425, loss 0.37741, acc 0.9
2016-05-21T00:36:18.271936: train-step 3426, loss 0.239698, acc 0.94
2016-05-21T00:36:27.248504: train-step 3427, loss 0.240895, acc 0.91
2016-05-21T00:36:35.874074: train-step 3428, loss 0.313225, acc 0.89
2016-05-21T00:36:43.971621: train-step 3429, loss 0.275247, acc 0.91
2016-05-21T00:36:53.528099: train-step 3430, loss 0.206337, acc 0.95
2016-05-21T00:37:01.156609: train-step 3431, loss 0.245564, acc 0.92
2016-05-21T00:37:09.166358: train-step 3432, loss 0.297763, acc 0.93
2016-05-21T00:37:17.105286: train-step 3433, loss 0.269955, acc 0.94
2016-05-21T00:37:24.855032: train-step 3434, loss 0.236016, acc 0.98
2016-05-21T00:37:32.974442: train-step 3435, loss 0.292631, acc 0.89
2016-05-21T00:37:40.914907: train-step 3436, loss 0.290266, acc 0.92
2016-05-21T00:37:49.520989: train-step 3437, loss 0.258868, acc 0.93
2016-05-21T00:37:58.630477: train-step 3438, loss 0.236567, acc 0.95
2016-05-21T00:38:07.732207: train-step 3439, loss 0.34657, acc 0.86
2016-05-21T00:38:16.867174: train-step 3440, loss 0.222278, acc 0.96
2016-05-21T00:38:26.051171: train-step 3441, loss 0.204082, acc 0.96
2016-05-21T00:38:35.403448: train-step 3442, loss 0.291577, acc 0.89
2016-05-21T00:38:44.673921: train-step 3443, loss 0.329694, acc 0.85
2016-05-21T00:38:54.056520: train-step 3444, loss 0.26184, acc 0.93
2016-05-21T00:39:03.140284: train-step 3445, loss 0.26642, acc 0.91
2016-05-21T00:39:12.356806: train-step 3446, loss 0.288389, acc 0.91
2016-05-21T00:39:21.642437: train-step 3447, loss 0.185377, acc 0.98
2016-05-21T00:39:30.768776: train-step 3448, loss 0.23336, acc 0.93
2016-05-21T00:39:39.944421: train-step 3449, loss 0.27566, acc 0.95
2016-05-21T00:39:48.685290: train-step 3450, loss 0.253743, acc 0.91
2016-05-21T00:39:57.216777: train-step 3451, loss 0.242736, acc 0.9
2016-05-21T00:40:06.435503: train-step 3452, loss 0.260825, acc 0.94
2016-05-21T00:40:13.957313: train-step 3453, loss 0.32048, acc 0.9
2016-05-21T00:40:21.619092: train-step 3454, loss 0.235505, acc 0.95
2016-05-21T00:40:29.164287: train-step 3455, loss 0.297944, acc 0.92
2016-05-21T00:40:37.062571: train-step 3456, loss 0.226803, acc 0.97
2016-05-21T00:40:44.834201: train-step 3457, loss 0.279097, acc 0.92
2016-05-21T00:40:52.507033: train-step 3458, loss 0.349438, acc 0.86
2016-05-21T00:41:01.155393: train-step 3459, loss 0.266476, acc 0.94
2016-05-21T00:41:10.105664: train-step 3460, loss 0.270463, acc 0.94
2016-05-21T00:41:19.285553: train-step 3461, loss 0.255407, acc 0.93
2016-05-21T00:41:28.394429: train-step 3462, loss 0.261323, acc 0.92
2016-05-21T00:41:37.542120: train-step 3463, loss 0.231455, acc 0.98
2016-05-21T00:41:46.693948: train-step 3464, loss 0.245022, acc 0.94
2016-05-21T00:41:56.030138: train-step 3465, loss 0.233773, acc 0.95
2016-05-21T00:42:05.058075: train-step 3466, loss 0.222058, acc 0.96
2016-05-21T00:42:14.011245: train-step 3467, loss 0.303579, acc 0.92
2016-05-21T00:42:23.363530: train-step 3468, loss 0.279098, acc 0.9
2016-05-21T00:42:32.717473: train-step 3469, loss 0.254209, acc 0.92
2016-05-21T00:42:41.777901: train-step 3470, loss 0.207604, acc 0.98
2016-05-21T00:42:51.007725: train-step 3471, loss 0.289686, acc 0.92
2016-05-21T00:42:59.718527: train-step 3472, loss 0.268589, acc 0.94
2016-05-21T00:43:07.988060: train-step 3473, loss 0.275999, acc 0.91
2016-05-21T00:43:17.549186: train-step 3474, loss 0.228281, acc 0.93
2016-05-21T00:43:25.699313: train-step 3475, loss 0.303549, acc 0.92
2016-05-21T00:43:33.939097: train-step 3476, loss 0.268575, acc 0.92
2016-05-21T00:43:41.509500: train-step 3477, loss 0.256157, acc 0.91
2016-05-21T00:43:49.383720: train-step 3478, loss 0.339012, acc 0.84
2016-05-21T00:43:57.562269: train-step 3479, loss 0.283825, acc 0.91
2016-05-21T00:44:05.506422: train-step 3480, loss 0.276389, acc 0.94
2016-05-21T00:44:13.758181: train-step 3481, loss 0.260057, acc 0.91
2016-05-21T00:44:22.448268: train-step 3482, loss 0.305827, acc 0.9
2016-05-21T00:44:31.985610: train-step 3483, loss 0.296527, acc 0.9
2016-05-21T00:44:41.469831: train-step 3484, loss 0.270062, acc 0.93
2016-05-21T00:44:50.733228: train-step 3485, loss 0.232036, acc 0.95
2016-05-21T00:44:59.812398: train-step 3486, loss 0.271757, acc 0.92
2016-05-21T00:45:08.745354: train-step 3487, loss 0.226419, acc 0.96
2016-05-21T00:45:17.979691: train-step 3488, loss 0.227195, acc 0.94
2016-05-21T00:45:27.185607: train-step 3489, loss 0.22661, acc 0.94
2016-05-21T00:45:36.379548: train-step 3490, loss 0.280664, acc 0.93
2016-05-21T00:45:45.655911: train-step 3491, loss 0.271356, acc 0.93
2016-05-21T00:45:54.663314: train-step 3492, loss 0.212581, acc 0.97
2016-05-21T00:46:03.882493: train-step 3493, loss 0.271494, acc 0.92
2016-05-21T00:46:13.026549: train-step 3494, loss 0.272866, acc 0.93
2016-05-21T00:46:21.842113: train-step 3495, loss 0.278108, acc 0.9
2016-05-21T00:46:29.739778: train-step 3496, loss 0.2494, acc 0.95
2016-05-21T00:46:39.133334: train-step 3497, loss 0.247906, acc 0.93
2016-05-21T00:46:46.734774: train-step 3498, loss 0.209051, acc 0.97
2016-05-21T00:46:54.411169: train-step 3499, loss 0.312032, acc 0.89
2016-05-21T00:47:01.985559: train-step 3500, loss 0.331589, acc 0.91
epoch number is: 14
2016-05-21T00:47:09.877140: train-step 3501, loss 0.210961, acc 0.96
2016-05-21T00:47:17.690392: train-step 3502, loss 0.275294, acc 0.92
2016-05-21T00:47:25.601037: train-step 3503, loss 0.261111, acc 0.93
2016-05-21T00:47:33.822394: train-step 3504, loss 0.309923, acc 0.89
2016-05-21T00:47:42.555257: train-step 3505, loss 0.305928, acc 0.88
2016-05-21T00:47:51.468340: train-step 3506, loss 0.362879, acc 0.87
2016-05-21T00:48:00.719231: train-step 3507, loss 0.29668, acc 0.92
2016-05-21T00:48:09.820155: train-step 3508, loss 0.225617, acc 0.95
2016-05-21T00:48:19.172291: train-step 3509, loss 0.27573, acc 0.94
2016-05-21T00:48:28.316906: train-step 3510, loss 0.284686, acc 0.94
2016-05-21T00:48:37.673414: train-step 3511, loss 0.228908, acc 0.96
2016-05-21T00:48:47.019722: train-step 3512, loss 0.261928, acc 0.94
2016-05-21T00:48:56.199609: train-step 3513, loss 0.282148, acc 0.92
2016-05-21T00:49:05.487905: train-step 3514, loss 0.270838, acc 0.91
2016-05-21T00:49:14.926531: train-step 3515, loss 0.35159, acc 0.86
2016-05-21T00:49:24.008149: train-step 3516, loss 0.21879, acc 0.95
2016-05-21T00:49:32.872625: train-step 3517, loss 0.251768, acc 0.92
2016-05-21T00:49:41.185085: train-step 3518, loss 0.270984, acc 0.93
2016-05-21T00:49:50.685499: train-step 3519, loss 0.277364, acc 0.94
2016-05-21T00:49:58.439289: train-step 3520, loss 0.242079, acc 0.94
2016-05-21T00:50:06.394688: train-step 3521, loss 0.269037, acc 0.92
2016-05-21T00:50:13.887837: train-step 3522, loss 0.23033, acc 0.97
2016-05-21T00:50:21.880098: train-step 3523, loss 0.240724, acc 0.93
2016-05-21T00:50:29.655765: train-step 3524, loss 0.255054, acc 0.94
2016-05-21T00:50:37.515862: train-step 3525, loss 0.241419, acc 0.94
2016-05-21T00:50:45.907660: train-step 3526, loss 0.271527, acc 0.92
2016-05-21T00:50:54.667940: train-step 3527, loss 0.279758, acc 0.9
2016-05-21T00:51:03.607204: train-step 3528, loss 0.229846, acc 0.95
2016-05-21T00:51:12.816516: train-step 3529, loss 0.269493, acc 0.92
2016-05-21T00:51:22.086878: train-step 3530, loss 0.253018, acc 0.95
2016-05-21T00:51:31.514785: train-step 3531, loss 0.308798, acc 0.91
2016-05-21T00:51:40.830202: train-step 3532, loss 0.21626, acc 0.95
2016-05-21T00:51:50.065248: train-step 3533, loss 0.252319, acc 0.93
2016-05-21T00:51:59.086759: train-step 3534, loss 0.212648, acc 0.95
2016-05-21T00:52:08.468243: train-step 3535, loss 0.356246, acc 0.87
2016-05-21T00:52:17.528703: train-step 3536, loss 0.234179, acc 0.94
2016-05-21T00:52:26.802065: train-step 3537, loss 0.278698, acc 0.96
2016-05-21T00:52:35.763648: train-step 3538, loss 0.299821, acc 0.9
2016-05-21T00:52:44.671439: train-step 3539, loss 0.22984, acc 0.96
2016-05-21T00:52:52.904209: train-step 3540, loss 0.275831, acc 0.93
2016-05-21T00:53:01.032787: train-step 3541, loss 0.225825, acc 0.96
2016-05-21T00:53:10.630687: train-step 3542, loss 0.240863, acc 0.91
2016-05-21T00:53:18.400123: train-step 3543, loss 0.260141, acc 0.95
2016-05-21T00:53:26.391150: train-step 3544, loss 0.233641, acc 0.95
2016-05-21T00:53:34.441879: train-step 3545, loss 0.246921, acc 0.93
2016-05-21T00:53:42.627828: train-step 3546, loss 0.226933, acc 0.92
2016-05-21T00:53:50.559688: train-step 3547, loss 0.234175, acc 0.95
2016-05-21T00:53:58.408596: train-step 3548, loss 0.23744, acc 0.95
2016-05-21T00:54:06.978963: train-step 3549, loss 0.277087, acc 0.93
2016-05-21T00:54:16.014927: train-step 3550, loss 0.328848, acc 0.91
2016-05-21T00:54:25.251957: train-step 3551, loss 0.243975, acc 0.95
2016-05-21T00:54:34.293687: train-step 3552, loss 0.266402, acc 0.94
2016-05-21T00:54:43.779336: train-step 3553, loss 0.290345, acc 0.91
2016-05-21T00:54:52.777255: train-step 3554, loss 0.308894, acc 0.87
2016-05-21T00:55:01.975567: train-step 3555, loss 0.25114, acc 0.92
2016-05-21T00:55:11.953059: train-step 3556, loss 0.223263, acc 0.96
2016-05-21T00:55:21.263193: train-step 3557, loss 0.235277, acc 0.95
2016-05-21T00:55:30.474404: train-step 3558, loss 0.237415, acc 0.93
2016-05-21T00:55:39.656451: train-step 3559, loss 0.247152, acc 0.92
2016-05-21T00:55:48.677584: train-step 3560, loss 0.208079, acc 0.97
2016-05-21T00:55:57.829824: train-step 3561, loss 0.271966, acc 0.91
2016-05-21T00:56:06.894200: train-step 3562, loss 0.2011, acc 0.97
2016-05-21T00:56:15.431722: train-step 3563, loss 0.263301, acc 0.93
2016-05-21T00:56:24.822263: train-step 3564, loss 0.288868, acc 0.93
2016-05-21T00:56:32.983620: train-step 3565, loss 0.322816, acc 0.93
2016-05-21T00:56:40.933295: train-step 3566, loss 0.222689, acc 0.96
2016-05-21T00:56:49.024861: train-step 3567, loss 0.222669, acc 0.97
2016-05-21T00:56:56.630354: train-step 3568, loss 0.192303, acc 0.99
2016-05-21T00:57:04.725627: train-step 3569, loss 0.34159, acc 0.87
2016-05-21T00:57:12.990874: train-step 3570, loss 0.262501, acc 0.94
2016-05-21T00:57:21.670092: train-step 3571, loss 0.303188, acc 0.9
2016-05-21T00:57:30.760648: train-step 3572, loss 0.235464, acc 0.95
2016-05-21T00:57:40.396353: train-step 3573, loss 0.227912, acc 0.95
2016-05-21T00:57:49.783588: train-step 3574, loss 0.252061, acc 0.93
2016-05-21T00:57:59.562013: train-step 3575, loss 0.237197, acc 0.93
2016-05-21T00:58:08.684633: train-step 3576, loss 0.280836, acc 0.93
2016-05-21T00:58:17.888292: train-step 3577, loss 0.305865, acc 0.91
2016-05-21T00:58:27.174178: train-step 3578, loss 0.228224, acc 0.94
2016-05-21T00:58:36.716062: train-step 3579, loss 0.254016, acc 0.93
2016-05-21T00:58:45.766282: train-step 3580, loss 0.253765, acc 0.91
2016-05-21T00:58:54.950508: train-step 3581, loss 0.274964, acc 0.91
2016-05-21T00:59:03.923313: train-step 3582, loss 0.277305, acc 0.94
2016-05-21T00:59:12.719631: train-step 3583, loss 0.298072, acc 0.91
2016-05-21T00:59:21.559447: train-step 3584, loss 0.251438, acc 0.94
2016-05-21T00:59:29.511840: train-step 3585, loss 0.242109, acc 0.94
2016-05-21T00:59:39.396576: train-step 3586, loss 0.264445, acc 0.93
2016-05-21T00:59:46.964861: train-step 3587, loss 0.285415, acc 0.91
2016-05-21T00:59:55.176389: train-step 3588, loss 0.281735, acc 0.95
2016-05-21T01:00:02.978743: train-step 3589, loss 0.24897, acc 0.93
2016-05-21T01:00:10.609260: train-step 3590, loss 0.296907, acc 0.9
2016-05-21T01:00:18.520660: train-step 3591, loss 0.277906, acc 0.95
2016-05-21T01:00:26.412543: train-step 3592, loss 0.221824, acc 0.95
2016-05-21T01:00:35.082841: train-step 3593, loss 0.273877, acc 0.93
2016-05-21T01:00:44.060397: train-step 3594, loss 0.244861, acc 0.92
2016-05-21T01:00:53.027259: train-step 3595, loss 0.244748, acc 0.97
2016-05-21T01:01:02.034748: train-step 3596, loss 0.276, acc 0.93
2016-05-21T01:01:11.302019: train-step 3597, loss 0.322526, acc 0.88
2016-05-21T01:01:20.766271: train-step 3598, loss 0.280229, acc 0.93
2016-05-21T01:01:29.730437: train-step 3599, loss 0.229224, acc 0.96
2016-05-21T01:01:38.769653: train-step 3600, loss 0.290189, acc 0.92
2016-05-21T01:01:48.300526: train-step 3601, loss 0.284463, acc 0.94
2016-05-21T01:01:57.677060: train-step 3602, loss 0.285636, acc 0.94
2016-05-21T01:02:06.960696: train-step 3603, loss 0.306376, acc 0.86
2016-05-21T01:02:15.954837: train-step 3604, loss 0.191763, acc 0.99
2016-05-21T01:02:24.954161: train-step 3605, loss 0.271244, acc 0.94
2016-05-21T01:02:33.511430: train-step 3606, loss 0.208582, acc 0.97
2016-05-21T01:02:41.331858: train-step 3607, loss 0.289604, acc 0.91
2016-05-21T01:02:50.762623: train-step 3608, loss 0.229499, acc 0.95
2016-05-21T01:02:58.416339: train-step 3609, loss 0.237845, acc 0.93
2016-05-21T01:03:06.935557: train-step 3610, loss 0.273116, acc 0.93
2016-05-21T01:03:14.933113: train-step 3611, loss 0.247012, acc 0.93
2016-05-21T01:03:22.702774: train-step 3612, loss 0.273576, acc 0.95
2016-05-21T01:03:30.846366: train-step 3613, loss 0.226147, acc 0.97
2016-05-21T01:03:38.623966: train-step 3614, loss 0.211778, acc 0.95
2016-05-21T01:03:47.022122: train-step 3615, loss 0.233758, acc 0.95
2016-05-21T01:03:55.783027: train-step 3616, loss 0.284795, acc 0.93
2016-05-21T01:04:04.828908: train-step 3617, loss 0.248046, acc 0.95
2016-05-21T01:04:14.168204: train-step 3618, loss 0.265693, acc 0.93
2016-05-21T01:04:23.268888: train-step 3619, loss 0.232676, acc 0.96
2016-05-21T01:04:32.469995: train-step 3620, loss 0.248516, acc 0.95
2016-05-21T01:04:41.721270: train-step 3621, loss 0.284865, acc 0.93
2016-05-21T01:04:51.104849: train-step 3622, loss 0.27043, acc 0.95
2016-05-21T01:05:00.325369: train-step 3623, loss 0.287081, acc 0.92
2016-05-21T01:05:09.526869: train-step 3624, loss 0.284977, acc 0.91
2016-05-21T01:05:18.889921: train-step 3625, loss 0.267798, acc 0.92
2016-05-21T01:05:28.270095: train-step 3626, loss 0.259407, acc 0.9
2016-05-21T01:05:37.172436: train-step 3627, loss 0.230736, acc 0.94
2016-05-21T01:05:45.730016: train-step 3628, loss 0.250224, acc 0.91
2016-05-21T01:05:53.753924: train-step 3629, loss 0.283631, acc 0.92
2016-05-21T01:06:03.168795: train-step 3630, loss 0.271146, acc 0.93
2016-05-21T01:06:10.957713: train-step 3631, loss 0.251608, acc 0.93
2016-05-21T01:06:18.724448: train-step 3632, loss 0.233384, acc 0.95
2016-05-21T01:06:27.148867: train-step 3633, loss 0.33959, acc 0.89
2016-05-21T01:06:34.772453: train-step 3634, loss 0.223419, acc 0.96
2016-05-21T01:06:42.851433: train-step 3635, loss 0.277061, acc 0.95
2016-05-21T01:06:51.138584: train-step 3636, loss 0.252217, acc 0.92
2016-05-21T01:06:59.690800: train-step 3637, loss 0.288648, acc 0.94
2016-05-21T01:07:08.717975: train-step 3638, loss 0.249365, acc 0.95
2016-05-21T01:07:17.840013: train-step 3639, loss 0.234315, acc 0.95
2016-05-21T01:07:27.075981: train-step 3640, loss 0.262287, acc 0.91
2016-05-21T01:07:36.302363: train-step 3641, loss 0.303795, acc 0.92
2016-05-21T01:07:45.320764: train-step 3642, loss 0.230424, acc 0.96
2016-05-21T01:07:54.493189: train-step 3643, loss 0.230802, acc 0.95
2016-05-21T01:08:03.464580: train-step 3644, loss 0.334719, acc 0.88
2016-05-21T01:08:12.592335: train-step 3645, loss 0.229874, acc 0.97
2016-05-21T01:08:21.801910: train-step 3646, loss 0.265266, acc 0.92
2016-05-21T01:08:31.363131: train-step 3647, loss 0.237523, acc 0.95
2016-05-21T01:08:40.670936: train-step 3648, loss 0.250702, acc 0.94
2016-05-21T01:08:49.838452: train-step 3649, loss 0.292753, acc 0.89
2016-05-21T01:08:58.452691: train-step 3650, loss 0.210995, acc 0.97
2016-05-21T01:09:06.572660: train-step 3651, loss 0.280018, acc 0.92
2016-05-21T01:09:16.141521: train-step 3652, loss 0.222137, acc 0.95
2016-05-21T01:09:23.804324: train-step 3653, loss 0.257036, acc 0.96
2016-05-21T01:09:31.661206: train-step 3654, loss 0.288259, acc 0.92
2016-05-21T01:09:39.500021: train-step 3655, loss 0.30285, acc 0.92
2016-05-21T01:09:46.999471: train-step 3656, loss 0.259138, acc 0.92
2016-05-21T01:09:54.923948: train-step 3657, loss 0.262942, acc 0.95
2016-05-21T01:10:02.829079: train-step 3658, loss 0.232643, acc 0.96
2016-05-21T01:10:11.493641: train-step 3659, loss 0.255679, acc 0.93
2016-05-21T01:10:20.470026: train-step 3660, loss 0.232939, acc 0.95
2016-05-21T01:10:29.566662: train-step 3661, loss 0.266622, acc 0.96
2016-05-21T01:10:38.742618: train-step 3662, loss 0.255553, acc 0.92
2016-05-21T01:10:48.167728: train-step 3663, loss 0.231568, acc 0.95
2016-05-21T01:10:57.327258: train-step 3664, loss 0.242933, acc 0.93
2016-05-21T01:11:06.304659: train-step 3665, loss 0.225109, acc 0.95
2016-05-21T01:11:15.423158: train-step 3666, loss 0.258924, acc 0.93
2016-05-21T01:11:24.437586: train-step 3667, loss 0.253944, acc 0.94
2016-05-21T01:11:33.488199: train-step 3668, loss 0.302066, acc 0.89
2016-05-21T01:11:42.602130: train-step 3669, loss 0.246987, acc 0.94
2016-05-21T01:11:51.906983: train-step 3670, loss 0.308785, acc 0.9
2016-05-21T01:12:00.891669: train-step 3671, loss 0.231753, acc 0.95
2016-05-21T01:12:09.600860: train-step 3672, loss 0.243426, acc 0.92
2016-05-21T01:12:17.676686: train-step 3673, loss 0.282899, acc 0.93
2016-05-21T01:12:27.206196: train-step 3674, loss 0.251774, acc 0.95
2016-05-21T01:12:35.032192: train-step 3675, loss 0.224242, acc 0.95
2016-05-21T01:12:42.861199: train-step 3676, loss 0.320487, acc 0.89
2016-05-21T01:12:51.223789: train-step 3677, loss 0.260223, acc 0.93
2016-05-21T01:12:58.966158: train-step 3678, loss 0.287999, acc 0.92
2016-05-21T01:13:06.708367: train-step 3679, loss 0.248292, acc 0.95
2016-05-21T01:13:14.760742: train-step 3680, loss 0.320502, acc 0.91
2016-05-21T01:13:23.127976: train-step 3681, loss 0.264414, acc 0.94
2016-05-21T01:13:31.958946: train-step 3682, loss 0.301642, acc 0.89
2016-05-21T01:13:41.403189: train-step 3683, loss 0.307649, acc 0.89
2016-05-21T01:13:50.700794: train-step 3684, loss 0.235876, acc 0.95
2016-05-21T01:14:00.093089: train-step 3685, loss 0.203086, acc 0.96
2016-05-21T01:14:09.332715: train-step 3686, loss 0.225234, acc 0.98
2016-05-21T01:14:18.652108: train-step 3687, loss 0.271379, acc 0.94
2016-05-21T01:14:28.178539: train-step 3688, loss 0.223446, acc 0.96
2016-05-21T01:14:37.337354: train-step 3689, loss 0.23504, acc 0.94
2016-05-21T01:14:46.721701: train-step 3690, loss 0.194167, acc 0.97
2016-05-21T01:14:55.588312: train-step 3691, loss 0.273887, acc 0.93
2016-05-21T01:15:04.739876: train-step 3692, loss 0.27007, acc 0.91
2016-05-21T01:15:13.711441: train-step 3693, loss 0.270367, acc 0.91
2016-05-21T01:15:22.662794: train-step 3694, loss 0.224488, acc 0.95
2016-05-21T01:15:30.925966: train-step 3695, loss 0.293888, acc 0.92
2016-05-21T01:15:40.632433: train-step 3696, loss 0.264029, acc 0.94
2016-05-21T01:15:48.186185: train-step 3697, loss 0.279267, acc 0.93
2016-05-21T01:15:56.016013: train-step 3698, loss 0.324136, acc 0.89
2016-05-21T01:16:03.652552: train-step 3699, loss 0.254824, acc 0.95
2016-05-21T01:16:12.019394: train-step 3700, loss 0.247254, acc 0.94
2016-05-21T01:16:19.710355: train-step 3701, loss 0.242518, acc 0.93
2016-05-21T01:16:28.039593: train-step 3702, loss 0.28674, acc 0.9
2016-05-21T01:16:36.361102: train-step 3703, loss 0.275045, acc 0.95
2016-05-21T01:16:45.038989: train-step 3704, loss 0.282624, acc 0.95
2016-05-21T01:16:54.157685: train-step 3705, loss 0.303301, acc 0.92
2016-05-21T01:17:03.231459: train-step 3706, loss 0.278245, acc 0.93
2016-05-21T01:17:12.179109: train-step 3707, loss 0.21684, acc 0.95
2016-05-21T01:17:21.483678: train-step 3708, loss 0.278251, acc 0.9
2016-05-21T01:17:30.616145: train-step 3709, loss 0.280247, acc 0.88
2016-05-21T01:17:39.934220: train-step 3710, loss 0.221594, acc 0.94
2016-05-21T01:17:49.241834: train-step 3711, loss 0.287223, acc 0.92
2016-05-21T01:17:58.711892: train-step 3712, loss 0.283286, acc 0.91
2016-05-21T01:18:07.968130: train-step 3713, loss 0.232246, acc 0.95
2016-05-21T01:18:17.370940: train-step 3714, loss 0.210945, acc 0.97
2016-05-21T01:18:26.342830: train-step 3715, loss 0.225381, acc 0.94
2016-05-21T01:18:35.079280: train-step 3716, loss 0.270467, acc 0.92
2016-05-21T01:18:43.275962: train-step 3717, loss 0.20912, acc 0.95
2016-05-21T01:18:52.437460: train-step 3718, loss 0.270954, acc 0.91
2016-05-21T01:18:59.982982: train-step 3719, loss 0.260855, acc 0.93
2016-05-21T01:19:07.745352: train-step 3720, loss 0.291935, acc 0.89
2016-05-21T01:19:15.252411: train-step 3721, loss 0.283473, acc 0.93
2016-05-21T01:19:23.152630: train-step 3722, loss 0.28259, acc 0.92
2016-05-21T01:19:31.349056: train-step 3723, loss 0.222181, acc 0.98
2016-05-21T01:19:39.073630: train-step 3724, loss 0.265435, acc 0.91
2016-05-21T01:19:47.410964: train-step 3725, loss 0.285023, acc 0.91
2016-05-21T01:19:56.284468: train-step 3726, loss 0.291209, acc 0.93
2016-05-21T01:20:05.157918: train-step 3727, loss 0.268916, acc 0.93
2016-05-21T01:20:14.341413: train-step 3728, loss 0.267552, acc 0.9
2016-05-21T01:20:23.534751: train-step 3729, loss 0.263403, acc 0.92
2016-05-21T01:20:33.070466: train-step 3730, loss 0.237124, acc 0.97
2016-05-21T01:20:42.817387: train-step 3731, loss 0.27039, acc 0.93
2016-05-21T01:20:51.844093: train-step 3732, loss 0.225157, acc 0.93
2016-05-21T01:21:01.394516: train-step 3733, loss 0.215335, acc 0.97
2016-05-21T01:21:10.948828: train-step 3734, loss 0.203507, acc 0.96
2016-05-21T01:21:20.465559: train-step 3735, loss 0.254557, acc 0.96
2016-05-21T01:21:29.936145: train-step 3736, loss 0.302244, acc 0.9
2016-05-21T01:21:38.827286: train-step 3737, loss 0.2439, acc 0.96
2016-05-21T01:21:47.812732: train-step 3738, loss 0.242087, acc 0.94
2016-05-21T01:21:56.248073: train-step 3739, loss 0.251976, acc 0.92
2016-05-21T01:22:05.112108: train-step 3740, loss 0.243907, acc 0.95
2016-05-21T01:22:13.708540: train-step 3741, loss 0.262601, acc 0.96
2016-05-21T01:22:21.537790: train-step 3742, loss 0.289961, acc 0.91
2016-05-21T01:22:29.080731: train-step 3743, loss 0.311713, acc 0.87
2016-05-21T01:22:36.928458: train-step 3744, loss 0.260982, acc 0.93
2016-05-21T01:22:44.633334: train-step 3745, loss 0.231212, acc 0.95
2016-05-21T01:22:52.999051: train-step 3746, loss 0.214229, acc 0.96
2016-05-21T01:23:00.829283: train-step 3747, loss 0.298484, acc 0.91
2016-05-21T01:23:09.585537: train-step 3748, loss 0.247779, acc 0.93
2016-05-21T01:23:18.957630: train-step 3749, loss 0.312258, acc 0.9
2016-05-21T01:23:28.092184: train-step 3750, loss 0.261601, acc 0.92
epoch number is: 15
2016-05-21T01:23:37.825201: train-step 3751, loss 0.228695, acc 0.93
2016-05-21T01:23:47.566223: train-step 3752, loss 0.201767, acc 0.99
2016-05-21T01:23:56.799270: train-step 3753, loss 0.257158, acc 0.95
2016-05-21T01:24:06.088802: train-step 3754, loss 0.239197, acc 0.96
2016-05-21T01:24:15.315901: train-step 3755, loss 0.261834, acc 0.95
2016-05-21T01:24:24.374864: train-step 3756, loss 0.297284, acc 0.9
2016-05-21T01:24:33.354127: train-step 3757, loss 0.206755, acc 0.99
2016-05-21T01:24:42.569102: train-step 3758, loss 0.253417, acc 0.94
2016-05-21T01:24:51.672019: train-step 3759, loss 0.242856, acc 0.95
2016-05-21T01:25:00.560374: train-step 3760, loss 0.264391, acc 0.94
2016-05-21T01:25:09.154441: train-step 3761, loss 0.230818, acc 0.93
2016-05-21T01:25:17.855129: train-step 3762, loss 0.240541, acc 0.95
2016-05-21T01:25:26.459086: train-step 3763, loss 0.268342, acc 0.93
2016-05-21T01:25:34.137270: train-step 3764, loss 0.240417, acc 0.93
2016-05-21T01:25:42.244297: train-step 3765, loss 0.245871, acc 0.96
2016-05-21T01:25:50.317292: train-step 3766, loss 0.306331, acc 0.9
2016-05-21T01:25:58.165901: train-step 3767, loss 0.244675, acc 0.93
2016-05-21T01:26:06.568144: train-step 3768, loss 0.23474, acc 0.95
2016-05-21T01:26:14.860955: train-step 3769, loss 0.247619, acc 0.96
2016-05-21T01:26:23.519366: train-step 3770, loss 0.219866, acc 0.95
2016-05-21T01:26:32.729602: train-step 3771, loss 0.280055, acc 0.92
2016-05-21T01:26:42.138672: train-step 3772, loss 0.284485, acc 0.91
2016-05-21T01:26:51.891099: train-step 3773, loss 0.257698, acc 0.93
2016-05-21T01:27:01.299237: train-step 3774, loss 0.267662, acc 0.9
2016-05-21T01:27:10.691195: train-step 3775, loss 0.271999, acc 0.93
2016-05-21T01:27:20.012986: train-step 3776, loss 0.265737, acc 0.93
2016-05-21T01:27:29.091737: train-step 3777, loss 0.218284, acc 0.96
2016-05-21T01:27:38.270471: train-step 3778, loss 0.292673, acc 0.92
2016-05-21T01:27:47.447365: train-step 3779, loss 0.20556, acc 0.99
2016-05-21T01:27:56.744230: train-step 3780, loss 0.211134, acc 0.98
2016-05-21T01:28:05.946416: train-step 3781, loss 0.235013, acc 0.93
2016-05-21T01:28:14.791300: train-step 3782, loss 0.221552, acc 0.94
2016-05-21T01:28:23.296796: train-step 3783, loss 0.234919, acc 0.96
2016-05-21T01:28:31.831035: train-step 3784, loss 0.259003, acc 0.95
2016-05-21T01:28:41.061006: train-step 3785, loss 0.275215, acc 0.94
2016-05-21T01:28:49.263735: train-step 3786, loss 0.229437, acc 0.94
2016-05-21T01:28:57.156939: train-step 3787, loss 0.271592, acc 0.92
2016-05-21T01:29:05.232772: train-step 3788, loss 0.222437, acc 0.96
2016-05-21T01:29:13.204558: train-step 3789, loss 0.277158, acc 0.94
2016-05-21T01:29:21.206091: train-step 3790, loss 0.256891, acc 0.95
2016-05-21T01:29:29.465239: train-step 3791, loss 0.274745, acc 0.93
2016-05-21T01:29:38.047295: train-step 3792, loss 0.256674, acc 0.92
2016-05-21T01:29:46.959159: train-step 3793, loss 0.239895, acc 0.98
2016-05-21T01:29:56.294015: train-step 3794, loss 0.300456, acc 0.89
2016-05-21T01:30:05.806425: train-step 3795, loss 0.25619, acc 0.94
2016-05-21T01:30:14.972841: train-step 3796, loss 0.278691, acc 0.9
2016-05-21T01:30:24.562819: train-step 3797, loss 0.234608, acc 0.95
2016-05-21T01:30:34.070061: train-step 3798, loss 0.234133, acc 0.92
2016-05-21T01:30:43.361004: train-step 3799, loss 0.243334, acc 0.96
2016-05-21T01:30:51.458237: train-step 3800, loss 0.226986, acc 0.94
2016-05-21T01:30:57.783313: train-step 3801, loss 0.272225, acc 0.92
2016-05-21T01:31:04.107220: train-step 3802, loss 0.339375, acc 0.86
2016-05-21T01:31:10.740821: train-step 3803, loss 0.261949, acc 0.91
2016-05-21T01:31:17.154203: train-step 3804, loss 0.299891, acc 0.89
2016-05-21T01:31:23.412704: train-step 3805, loss 0.172483, acc 0.98
2016-05-21T01:31:30.056249: train-step 3806, loss 0.26622, acc 0.92
2016-05-21T01:31:36.422784: train-step 3807, loss 0.250519, acc 0.96
2016-05-21T01:31:42.847684: train-step 3808, loss 0.249494, acc 0.94
2016-05-21T01:31:49.117449: train-step 3809, loss 0.27486, acc 0.92
2016-05-21T01:31:55.563118: train-step 3810, loss 0.261796, acc 0.92
2016-05-21T01:32:02.134558: train-step 3811, loss 0.231095, acc 0.94
2016-05-21T01:32:08.527146: train-step 3812, loss 0.264126, acc 0.93
2016-05-21T01:32:15.441664: train-step 3813, loss 0.311734, acc 0.92
2016-05-21T01:32:21.967145: train-step 3814, loss 0.235479, acc 0.96
2016-05-21T01:32:28.203491: train-step 3815, loss 0.246771, acc 0.97
2016-05-21T01:32:34.586644: train-step 3816, loss 0.252011, acc 0.95
2016-05-21T01:32:41.192316: train-step 3817, loss 0.238018, acc 0.95
2016-05-21T01:32:47.513058: train-step 3818, loss 0.185538, acc 0.97
2016-05-21T01:32:53.882232: train-step 3819, loss 0.230244, acc 0.94
2016-05-21T01:33:00.525570: train-step 3820, loss 0.265651, acc 0.92
2016-05-21T01:33:06.818792: train-step 3821, loss 0.309665, acc 0.89
2016-05-21T01:33:13.061023: train-step 3822, loss 0.303323, acc 0.88
2016-05-21T01:33:19.327860: train-step 3823, loss 0.194833, acc 0.98
2016-05-21T01:33:25.589296: train-step 3824, loss 0.244118, acc 0.96
2016-05-21T01:33:31.811614: train-step 3825, loss 0.250142, acc 0.94
2016-05-21T01:33:38.069216: train-step 3826, loss 0.220166, acc 0.96
2016-05-21T01:33:44.507227: train-step 3827, loss 0.233996, acc 0.93
2016-05-21T01:33:50.905114: train-step 3828, loss 0.262818, acc 0.93
2016-05-21T01:33:57.194377: train-step 3829, loss 0.232845, acc 0.96
2016-05-21T01:34:03.460954: train-step 3830, loss 0.240213, acc 0.95
2016-05-21T01:34:09.875578: train-step 3831, loss 0.315264, acc 0.89
2016-05-21T01:34:16.271281: train-step 3832, loss 0.286855, acc 0.91
2016-05-21T01:34:22.579558: train-step 3833, loss 0.281884, acc 0.94
2016-05-21T01:34:28.806814: train-step 3834, loss 0.192051, acc 0.98
2016-05-21T01:34:35.096129: train-step 3835, loss 0.226602, acc 0.95
2016-05-21T01:34:41.485411: train-step 3836, loss 0.241816, acc 0.95
2016-05-21T01:34:47.686151: train-step 3837, loss 0.209688, acc 0.96
2016-05-21T01:34:53.902025: train-step 3838, loss 0.287258, acc 0.92
2016-05-21T01:35:00.324110: train-step 3839, loss 0.266091, acc 0.92
2016-05-21T01:35:06.597750: train-step 3840, loss 0.220774, acc 0.95
2016-05-21T01:35:12.803780: train-step 3841, loss 0.274264, acc 0.92
2016-05-21T01:35:19.051700: train-step 3842, loss 0.334964, acc 0.89
2016-05-21T01:35:25.708055: train-step 3843, loss 0.237078, acc 0.94
2016-05-21T01:35:33.114804: train-step 3844, loss 0.236152, acc 0.92
2016-05-21T01:35:40.323719: train-step 3845, loss 0.276939, acc 0.93
2016-05-21T01:35:47.529096: train-step 3846, loss 0.252919, acc 0.94
2016-05-21T01:35:54.534167: train-step 3847, loss 0.252322, acc 0.94
2016-05-21T01:36:01.557435: train-step 3848, loss 0.245034, acc 0.93
2016-05-21T01:36:08.422610: train-step 3849, loss 0.219993, acc 0.94
2016-05-21T01:36:15.377209: train-step 3850, loss 0.287285, acc 0.92
2016-05-21T01:36:22.840224: train-step 3851, loss 0.334039, acc 0.87
2016-05-21T01:36:29.995364: train-step 3852, loss 0.262703, acc 0.92
2016-05-21T01:36:37.017421: train-step 3853, loss 0.215409, acc 0.96
2016-05-21T01:36:43.953207: train-step 3854, loss 0.234463, acc 0.95
2016-05-21T01:36:51.181199: train-step 3855, loss 0.230987, acc 0.94
2016-05-21T01:36:58.592166: train-step 3856, loss 0.234923, acc 0.93
2016-05-21T01:37:05.651739: train-step 3857, loss 0.240186, acc 0.94
2016-05-21T01:37:12.706681: train-step 3858, loss 0.275196, acc 0.92
2016-05-21T01:37:19.742897: train-step 3859, loss 0.269607, acc 0.93
2016-05-21T01:37:26.836951: train-step 3860, loss 0.215661, acc 0.95
2016-05-21T01:37:33.923846: train-step 3861, loss 0.235099, acc 0.96
2016-05-21T01:37:41.006769: train-step 3862, loss 0.23748, acc 0.96
2016-05-21T01:37:48.527569: train-step 3863, loss 0.253442, acc 0.95
2016-05-21T01:37:55.583562: train-step 3864, loss 0.228615, acc 0.94
2016-05-21T01:38:02.583479: train-step 3865, loss 0.256591, acc 0.94
2016-05-21T01:38:09.403260: train-step 3866, loss 0.278094, acc 0.91
2016-05-21T01:38:16.373396: train-step 3867, loss 0.230454, acc 0.95
2016-05-21T01:38:23.303475: train-step 3868, loss 0.292453, acc 0.93
2016-05-21T01:38:30.493591: train-step 3869, loss 0.222713, acc 0.95
2016-05-21T01:38:37.393854: train-step 3870, loss 0.236964, acc 0.97
2016-05-21T01:38:44.372292: train-step 3871, loss 0.255468, acc 0.93
2016-05-21T01:38:51.357483: train-step 3872, loss 0.26087, acc 0.91
2016-05-21T01:38:58.579089: train-step 3873, loss 0.283628, acc 0.91
2016-05-21T01:39:05.582077: train-step 3874, loss 0.244703, acc 0.94
2016-05-21T01:39:12.544537: train-step 3875, loss 0.274018, acc 0.92
2016-05-21T01:39:19.808683: train-step 3876, loss 0.266819, acc 0.95
2016-05-21T01:39:26.839359: train-step 3877, loss 0.267334, acc 0.92
2016-05-21T01:39:33.871544: train-step 3878, loss 0.248667, acc 0.93
2016-05-21T01:39:40.918866: train-step 3879, loss 0.207803, acc 0.98
2016-05-21T01:39:47.984206: train-step 3880, loss 0.230673, acc 0.95
2016-05-21T01:39:54.950959: train-step 3881, loss 0.306506, acc 0.9
2016-05-21T01:40:01.884042: train-step 3882, loss 0.26262, acc 0.94
2016-05-21T01:40:08.871604: train-step 3883, loss 0.216179, acc 0.94
2016-05-21T01:40:15.831287: train-step 3884, loss 0.318988, acc 0.91
2016-05-21T01:40:23.304646: train-step 3885, loss 0.252939, acc 0.92
2016-05-21T01:40:30.414101: train-step 3886, loss 0.254697, acc 0.94
2016-05-21T01:40:37.411004: train-step 3887, loss 0.198125, acc 0.96
2016-05-21T01:40:44.362727: train-step 3888, loss 0.27316, acc 0.92
2016-05-21T01:40:51.350869: train-step 3889, loss 0.294369, acc 0.89
2016-05-21T01:40:58.446293: train-step 3890, loss 0.256604, acc 0.95
2016-05-21T01:41:05.394656: train-step 3891, loss 0.280403, acc 0.91
2016-05-21T01:41:12.378213: train-step 3892, loss 0.280081, acc 0.91
2016-05-21T01:41:19.375260: train-step 3893, loss 0.249155, acc 0.94
2016-05-21T01:41:26.568670: train-step 3894, loss 0.277687, acc 0.93
2016-05-21T01:41:33.518063: train-step 3895, loss 0.294228, acc 0.93
2016-05-21T01:41:40.417094: train-step 3896, loss 0.322032, acc 0.88
2016-05-21T01:41:47.462506: train-step 3897, loss 0.202273, acc 0.97
2016-05-21T01:41:54.461018: train-step 3898, loss 0.190126, acc 0.95
2016-05-21T01:42:01.459287: train-step 3899, loss 0.227168, acc 0.95
2016-05-21T01:42:08.506035: train-step 3900, loss 0.257336, acc 0.9
2016-05-21T01:42:15.579590: train-step 3901, loss 0.269954, acc 0.93
2016-05-21T01:42:22.665210: train-step 3902, loss 0.231279, acc 0.97
2016-05-21T01:42:29.719300: train-step 3903, loss 0.277954, acc 0.93
2016-05-21T01:42:36.678848: train-step 3904, loss 0.217047, acc 0.96
2016-05-21T01:42:43.868714: train-step 3905, loss 0.251457, acc 0.92
2016-05-21T01:42:50.899071: train-step 3906, loss 0.275024, acc 0.93
2016-05-21T01:42:57.984885: train-step 3907, loss 0.258509, acc 0.96
2016-05-21T01:43:05.022243: train-step 3908, loss 0.233727, acc 0.96
2016-05-21T01:43:12.086387: train-step 3909, loss 0.258699, acc 0.93
2016-05-21T01:43:19.222010: train-step 3910, loss 0.227081, acc 0.96
2016-05-21T01:43:26.294752: train-step 3911, loss 0.26045, acc 0.95
2016-05-21T01:43:33.279776: train-step 3912, loss 0.263584, acc 0.9
2016-05-21T01:43:40.260065: train-step 3913, loss 0.295196, acc 0.91
2016-05-21T01:43:47.381869: train-step 3914, loss 0.224507, acc 0.97
2016-05-21T01:43:54.554565: train-step 3915, loss 0.268763, acc 0.92
2016-05-21T01:44:02.046783: train-step 3916, loss 0.254743, acc 0.93
2016-05-21T01:44:09.298097: train-step 3917, loss 0.250697, acc 0.94
2016-05-21T01:44:16.365155: train-step 3918, loss 0.210384, acc 0.97
2016-05-21T01:44:23.366447: train-step 3919, loss 0.225496, acc 0.95
2016-05-21T01:44:30.454305: train-step 3920, loss 0.264951, acc 0.92
2016-05-21T01:44:37.550254: train-step 3921, loss 0.251583, acc 0.93
2016-05-21T01:44:44.554774: train-step 3922, loss 0.302841, acc 0.91
2016-05-21T01:44:51.706223: train-step 3923, loss 0.221045, acc 0.94
2016-05-21T01:44:58.939376: train-step 3924, loss 0.298621, acc 0.91
2016-05-21T01:45:05.884038: train-step 3925, loss 0.312376, acc 0.89
2016-05-21T01:45:12.825765: train-step 3926, loss 0.278073, acc 0.9
2016-05-21T01:45:19.735686: train-step 3927, loss 0.241704, acc 0.96
2016-05-21T01:45:26.850544: train-step 3928, loss 0.284018, acc 0.9
2016-05-21T01:45:34.216130: train-step 3929, loss 0.257363, acc 0.95
2016-05-21T01:45:41.253735: train-step 3930, loss 0.263733, acc 0.93
2016-05-21T01:45:48.285592: train-step 3931, loss 0.319965, acc 0.89
2016-05-21T01:45:55.343510: train-step 3932, loss 0.248171, acc 0.95
2016-05-21T01:46:02.337428: train-step 3933, loss 0.216737, acc 0.95
2016-05-21T01:46:09.476178: train-step 3934, loss 0.234348, acc 0.94
2016-05-21T01:46:17.004110: train-step 3935, loss 0.244466, acc 0.94
2016-05-21T01:46:24.168912: train-step 3936, loss 0.186883, acc 0.97
2016-05-21T01:46:31.301595: train-step 3937, loss 0.262886, acc 0.91
2016-05-21T01:46:38.741384: train-step 3938, loss 0.267483, acc 0.91
2016-05-21T01:46:45.993808: train-step 3939, loss 0.247405, acc 0.94
2016-05-21T01:46:53.036951: train-step 3940, loss 0.267068, acc 0.91
2016-05-21T01:47:00.068879: train-step 3941, loss 0.231629, acc 0.96
2016-05-21T01:47:07.117814: train-step 3942, loss 0.193916, acc 0.97
2016-05-21T01:47:14.096709: train-step 3943, loss 0.236236, acc 0.96
2016-05-21T01:47:21.062738: train-step 3944, loss 0.231268, acc 0.94
2016-05-21T01:47:28.154035: train-step 3945, loss 0.272654, acc 0.92
2016-05-21T01:47:35.275466: train-step 3946, loss 0.219855, acc 0.96
2016-05-21T01:47:42.252220: train-step 3947, loss 0.230321, acc 0.94
2016-05-21T01:47:49.414073: train-step 3948, loss 0.291267, acc 0.93
2016-05-21T01:47:56.601667: train-step 3949, loss 0.268621, acc 0.92
2016-05-21T01:48:03.717938: train-step 3950, loss 0.271668, acc 0.93
2016-05-21T01:48:10.865474: train-step 3951, loss 0.262245, acc 0.92
2016-05-21T01:48:17.932318: train-step 3952, loss 0.252104, acc 0.92
2016-05-21T01:48:24.908657: train-step 3953, loss 0.238365, acc 0.96
2016-05-21T01:48:32.394694: train-step 3954, loss 0.227829, acc 0.97
2016-05-21T01:48:39.583766: train-step 3955, loss 0.249952, acc 0.94
2016-05-21T01:48:46.642524: train-step 3956, loss 0.22553, acc 0.94
2016-05-21T01:48:54.174858: train-step 3957, loss 0.244085, acc 0.96
2016-05-21T01:49:01.238677: train-step 3958, loss 0.27565, acc 0.91
2016-05-21T01:49:08.310232: train-step 3959, loss 0.232269, acc 0.93
2016-05-21T01:49:15.522831: train-step 3960, loss 0.268829, acc 0.96
2016-05-21T01:49:22.938864: train-step 3961, loss 0.23372, acc 0.95
2016-05-21T01:49:30.220283: train-step 3962, loss 0.250565, acc 0.94
2016-05-21T01:49:37.279965: train-step 3963, loss 0.224642, acc 0.93
2016-05-21T01:49:44.369432: train-step 3964, loss 0.289345, acc 0.9
2016-05-21T01:49:51.429475: train-step 3965, loss 0.301358, acc 0.91
2016-05-21T01:49:58.448725: train-step 3966, loss 0.262678, acc 0.9
2016-05-21T01:50:05.593481: train-step 3967, loss 0.253109, acc 0.94
2016-05-21T01:50:12.453648: train-step 3968, loss 0.225547, acc 0.94
2016-05-21T01:50:18.776550: train-step 3969, loss 0.187431, acc 0.99
2016-05-21T01:50:25.043282: train-step 3970, loss 0.285381, acc 0.95
2016-05-21T01:50:31.335300: train-step 3971, loss 0.261023, acc 0.95
2016-05-21T01:50:37.869364: train-step 3972, loss 0.238799, acc 0.95
2016-05-21T01:50:44.203597: train-step 3973, loss 0.238733, acc 0.95
2016-05-21T01:50:50.499377: train-step 3974, loss 0.228779, acc 0.92
2016-05-21T01:50:56.813776: train-step 3975, loss 0.288917, acc 0.91
2016-05-21T01:51:03.147651: train-step 3976, loss 0.266468, acc 0.93
2016-05-21T01:51:09.854984: train-step 3977, loss 0.197107, acc 0.97
2016-05-21T01:51:16.218912: train-step 3978, loss 0.220341, acc 0.94
2016-05-21T01:51:22.803864: train-step 3979, loss 0.190303, acc 0.96
2016-05-21T01:51:29.853896: train-step 3980, loss 0.264669, acc 0.95
2016-05-21T01:51:37.429034: train-step 3981, loss 0.21266, acc 0.98
2016-05-21T01:51:44.431791: train-step 3982, loss 0.221863, acc 0.97
2016-05-21T01:51:51.587543: train-step 3983, loss 0.216258, acc 0.98
2016-05-21T01:51:59.037541: train-step 3984, loss 0.280418, acc 0.92
2016-05-21T01:52:06.097614: train-step 3985, loss 0.351789, acc 0.87
2016-05-21T01:52:13.172606: train-step 3986, loss 0.289428, acc 0.94
2016-05-21T01:52:20.451894: train-step 3987, loss 0.26535, acc 0.95
2016-05-21T01:52:27.639865: train-step 3988, loss 0.206794, acc 0.95
2016-05-21T01:52:34.706869: train-step 3989, loss 0.247358, acc 0.94
2016-05-21T01:52:41.787813: train-step 3990, loss 0.257434, acc 0.91
2016-05-21T01:52:48.847738: train-step 3991, loss 0.217079, acc 0.95
2016-05-21T01:52:56.055516: train-step 3992, loss 0.241582, acc 0.92
2016-05-21T01:53:03.547642: train-step 3993, loss 0.217381, acc 0.95
2016-05-21T01:53:10.598514: train-step 3994, loss 0.239971, acc 0.94
2016-05-21T01:53:17.986455: train-step 3995, loss 0.254939, acc 0.94
2016-05-21T01:53:25.207028: train-step 3996, loss 0.20364, acc 0.95
2016-05-21T01:53:32.289786: train-step 3997, loss 0.21897, acc 0.96
2016-05-21T01:53:39.281760: train-step 3998, loss 0.31228, acc 0.91
2016-05-21T01:53:46.289514: train-step 3999, loss 0.22845, acc 0.98
2016-05-21T01:53:53.308640: train-step 4000, loss 0.244533, acc 0.95
epoch number is: 16
2016-05-21T01:54:00.674746: train-step 4001, loss 0.320789, acc 0.89
2016-05-21T01:54:07.700064: train-step 4002, loss 0.258497, acc 0.9
2016-05-21T01:54:14.782524: train-step 4003, loss 0.214025, acc 0.97
2016-05-21T01:54:21.860279: train-step 4004, loss 0.258451, acc 0.9
2016-05-21T01:54:29.195317: train-step 4005, loss 0.195496, acc 0.99
2016-05-21T01:54:36.618977: train-step 4006, loss 0.208698, acc 0.98
2016-05-21T01:54:43.671973: train-step 4007, loss 0.208562, acc 0.96
2016-05-21T01:54:50.750649: train-step 4008, loss 0.282503, acc 0.91
2016-05-21T01:54:57.840071: train-step 4009, loss 0.227961, acc 0.95
2016-05-21T01:55:05.025125: train-step 4010, loss 0.225323, acc 0.94
2016-05-21T01:55:12.131143: train-step 4011, loss 0.272901, acc 0.92
2016-05-21T01:55:19.196477: train-step 4012, loss 0.250664, acc 0.93
2016-05-21T01:55:26.242248: train-step 4013, loss 0.256044, acc 0.93
2016-05-21T01:55:33.333695: train-step 4014, loss 0.265382, acc 0.92
2016-05-21T01:55:40.867141: train-step 4015, loss 0.210639, acc 0.94
2016-05-21T01:55:48.107085: train-step 4016, loss 0.261357, acc 0.95
2016-05-21T01:55:55.288744: train-step 4017, loss 0.247712, acc 0.93
2016-05-21T01:56:02.392250: train-step 4018, loss 0.235148, acc 0.94
2016-05-21T01:56:09.386769: train-step 4019, loss 0.218299, acc 0.94
2016-05-21T01:56:16.401372: train-step 4020, loss 0.251637, acc 0.94
2016-05-21T01:56:23.476608: train-step 4021, loss 0.207083, acc 0.97
2016-05-21T01:56:30.853604: train-step 4022, loss 0.269477, acc 0.93
2016-05-21T01:56:38.094876: train-step 4023, loss 0.255967, acc 0.93
2016-05-21T01:56:45.389912: train-step 4024, loss 0.208612, acc 0.96
2016-05-21T01:56:52.426776: train-step 4025, loss 0.22185, acc 0.97
2016-05-21T01:56:59.483384: train-step 4026, loss 0.300168, acc 0.92
2016-05-21T01:57:06.888255: train-step 4027, loss 0.259706, acc 0.97
2016-05-21T01:57:14.039826: train-step 4028, loss 0.292978, acc 0.9
2016-05-21T01:57:21.045568: train-step 4029, loss 0.25664, acc 0.92
2016-05-21T01:57:28.439116: train-step 4030, loss 0.211062, acc 0.96
2016-05-21T01:57:35.632244: train-step 4031, loss 0.208223, acc 0.95
2016-05-21T01:57:42.502871: train-step 4032, loss 0.212992, acc 0.97
2016-05-21T01:57:49.352985: train-step 4033, loss 0.214527, acc 0.98
2016-05-21T01:57:56.375413: train-step 4034, loss 0.29133, acc 0.89
2016-05-21T01:58:03.366130: train-step 4035, loss 0.184833, acc 0.99
2016-05-21T01:58:10.451093: train-step 4036, loss 0.181644, acc 0.97
2016-05-21T01:58:17.420101: train-step 4037, loss 0.195705, acc 0.96
2016-05-21T01:58:24.456789: train-step 4038, loss 0.269479, acc 0.9
2016-05-21T01:58:31.463711: train-step 4039, loss 0.271318, acc 0.93
2016-05-21T01:58:38.563831: train-step 4040, loss 0.254101, acc 0.96
2016-05-21T01:58:45.471371: train-step 4041, loss 0.233792, acc 0.97
2016-05-21T01:58:52.704713: train-step 4042, loss 0.268145, acc 0.94
2016-05-21T01:58:59.775565: train-step 4043, loss 0.197255, acc 0.96
2016-05-21T01:59:06.832966: train-step 4044, loss 0.246467, acc 0.94
2016-05-21T01:59:13.839150: train-step 4045, loss 0.221359, acc 0.94
2016-05-21T01:59:21.062822: train-step 4046, loss 0.256494, acc 0.92
2016-05-21T01:59:28.355605: train-step 4047, loss 0.277858, acc 0.91
2016-05-21T01:59:35.478990: train-step 4048, loss 0.223298, acc 0.96
2016-05-21T01:59:42.995614: train-step 4049, loss 0.273517, acc 0.94
2016-05-21T01:59:50.234622: train-step 4050, loss 0.221451, acc 0.94
2016-05-21T01:59:57.397001: train-step 4051, loss 0.230077, acc 0.94
2016-05-21T02:00:04.742326: train-step 4052, loss 0.234337, acc 0.97
2016-05-21T02:00:12.020327: train-step 4053, loss 0.250873, acc 0.94
2016-05-21T02:00:19.322907: train-step 4054, loss 0.228533, acc 0.96
2016-05-21T02:00:26.406334: train-step 4055, loss 0.294855, acc 0.92
2016-05-21T02:00:33.736243: train-step 4056, loss 0.223656, acc 0.95
2016-05-21T02:00:40.995392: train-step 4057, loss 0.282401, acc 0.92
2016-05-21T02:00:47.995520: train-step 4058, loss 0.19663, acc 0.97
2016-05-21T02:00:55.066813: train-step 4059, loss 0.242012, acc 0.96
2016-05-21T02:01:02.037912: train-step 4060, loss 0.178495, acc 0.98
2016-05-21T02:01:09.086227: train-step 4061, loss 0.250761, acc 0.94
2016-05-21T02:01:16.274451: train-step 4062, loss 0.247427, acc 0.93
2016-05-21T02:01:23.685250: train-step 4063, loss 0.29189, acc 0.91
2016-05-21T02:01:30.795355: train-step 4064, loss 0.253529, acc 0.94
2016-05-21T02:01:37.730658: train-step 4065, loss 0.224924, acc 0.99
2016-05-21T02:01:44.998481: train-step 4066, loss 0.210745, acc 0.97
2016-05-21T02:01:52.049614: train-step 4067, loss 0.211392, acc 0.97
2016-05-21T02:01:59.046899: train-step 4068, loss 0.227187, acc 0.95
2016-05-21T02:02:06.147534: train-step 4069, loss 0.228183, acc 0.96
2016-05-21T02:02:13.178250: train-step 4070, loss 0.253246, acc 0.94
2016-05-21T02:02:20.258274: train-step 4071, loss 0.190266, acc 0.97
2016-05-21T02:02:27.296514: train-step 4072, loss 0.255185, acc 0.93
2016-05-21T02:02:34.295412: train-step 4073, loss 0.21304, acc 0.96
2016-05-21T02:02:41.339331: train-step 4074, loss 0.255208, acc 0.94
2016-05-21T02:02:48.439849: train-step 4075, loss 0.25843, acc 0.94
2016-05-21T02:02:55.490745: train-step 4076, loss 0.290558, acc 0.92
2016-05-21T02:03:02.559854: train-step 4077, loss 0.218062, acc 0.95
2016-05-21T02:03:09.655552: train-step 4078, loss 0.233184, acc 0.94
2016-05-21T02:03:16.773328: train-step 4079, loss 0.253028, acc 0.95
2016-05-21T02:03:23.792430: train-step 4080, loss 0.249379, acc 0.92
2016-05-21T02:03:30.779407: train-step 4081, loss 0.268992, acc 0.93
2016-05-21T02:03:37.849822: train-step 4082, loss 0.208683, acc 0.96
2016-05-21T02:03:44.894051: train-step 4083, loss 0.202205, acc 0.98
2016-05-21T02:03:52.300249: train-step 4084, loss 0.227808, acc 0.96
2016-05-21T02:03:59.538131: train-step 4085, loss 0.265829, acc 0.93
2016-05-21T02:04:06.574436: train-step 4086, loss 0.261903, acc 0.94
2016-05-21T02:04:14.147581: train-step 4087, loss 0.26007, acc 0.93
2016-05-21T02:04:21.236800: train-step 4088, loss 0.286716, acc 0.92
2016-05-21T02:04:28.300785: train-step 4089, loss 0.234849, acc 0.94
2016-05-21T02:04:35.259144: train-step 4090, loss 0.253708, acc 0.95
2016-05-21T02:04:42.458971: train-step 4091, loss 0.263099, acc 0.93
2016-05-21T02:04:49.596473: train-step 4092, loss 0.261065, acc 0.95
2016-05-21T02:04:56.803827: train-step 4093, loss 0.229482, acc 0.97
2016-05-21T02:05:04.238000: train-step 4094, loss 0.215801, acc 0.96
2016-05-21T02:05:11.361644: train-step 4095, loss 0.238851, acc 0.94
2016-05-21T02:05:18.416700: train-step 4096, loss 0.274161, acc 0.9
2016-05-21T02:05:25.534160: train-step 4097, loss 0.281761, acc 0.91
2016-05-21T02:05:32.527874: train-step 4098, loss 0.232685, acc 0.95
2016-05-21T02:05:39.607037: train-step 4099, loss 0.264057, acc 0.93
2016-05-21T02:05:46.694851: train-step 4100, loss 0.221027, acc 0.94
2016-05-21T02:05:54.237926: train-step 4101, loss 0.220252, acc 0.95
2016-05-21T02:06:01.380521: train-step 4102, loss 0.322067, acc 0.86
2016-05-21T02:06:08.448285: train-step 4103, loss 0.193419, acc 0.97
2016-05-21T02:06:15.433342: train-step 4104, loss 0.222132, acc 0.95
2016-05-21T02:06:22.586027: train-step 4105, loss 0.265959, acc 0.93
2016-05-21T02:06:30.355605: train-step 4106, loss 0.233429, acc 0.93
2016-05-21T02:06:37.576410: train-step 4107, loss 0.29365, acc 0.93
2016-05-21T02:06:44.522759: train-step 4108, loss 0.247119, acc 0.91
2016-05-21T02:06:51.637325: train-step 4109, loss 0.239125, acc 0.94
2016-05-21T02:06:58.694902: train-step 4110, loss 0.253642, acc 0.93
2016-05-21T02:07:05.722007: train-step 4111, loss 0.199412, acc 0.98
2016-05-21T02:07:12.870233: train-step 4112, loss 0.215691, acc 0.94
2016-05-21T02:07:19.986066: train-step 4113, loss 0.313668, acc 0.9
2016-05-21T02:07:27.096394: train-step 4114, loss 0.234284, acc 0.93
2016-05-21T02:07:34.095418: train-step 4115, loss 0.335441, acc 0.9
2016-05-21T02:07:41.569700: train-step 4116, loss 0.21979, acc 0.94
2016-05-21T02:07:48.760735: train-step 4117, loss 0.279788, acc 0.94
2016-05-21T02:07:55.800654: train-step 4118, loss 0.280538, acc 0.95
2016-05-21T02:08:02.879847: train-step 4119, loss 0.261892, acc 0.94
2016-05-21T02:08:10.415654: train-step 4120, loss 0.219921, acc 0.95
2016-05-21T02:08:17.487430: train-step 4121, loss 0.286714, acc 0.92
2016-05-21T02:08:24.501692: train-step 4122, loss 0.281888, acc 0.93
2016-05-21T02:08:31.447710: train-step 4123, loss 0.239508, acc 0.96
2016-05-21T02:08:38.369923: train-step 4124, loss 0.222085, acc 0.96
2016-05-21T02:08:45.485916: train-step 4125, loss 0.295799, acc 0.91
2016-05-21T02:08:52.673353: train-step 4126, loss 0.211251, acc 0.95
2016-05-21T02:08:59.739361: train-step 4127, loss 0.247918, acc 0.96
2016-05-21T02:09:06.691430: train-step 4128, loss 0.250321, acc 0.95
2016-05-21T02:09:13.844765: train-step 4129, loss 0.238462, acc 0.96
2016-05-21T02:09:20.982666: train-step 4130, loss 0.200106, acc 0.96
2016-05-21T02:09:28.081482: train-step 4131, loss 0.201428, acc 0.95
2016-05-21T02:09:35.062900: train-step 4132, loss 0.264164, acc 0.92
2016-05-21T02:09:41.993113: train-step 4133, loss 0.285459, acc 0.89
2016-05-21T02:09:49.048817: train-step 4134, loss 0.212143, acc 0.97
2016-05-21T02:09:56.008109: train-step 4135, loss 0.237769, acc 0.92
2016-05-21T02:10:03.083123: train-step 4136, loss 0.216455, acc 0.95
2016-05-21T02:10:10.157158: train-step 4137, loss 0.234482, acc 0.95
2016-05-21T02:10:17.211962: train-step 4138, loss 0.210132, acc 0.97
2016-05-21T02:10:24.278683: train-step 4139, loss 0.228179, acc 0.95
2016-05-21T02:10:31.346612: train-step 4140, loss 0.237719, acc 0.95
2016-05-21T02:10:38.444865: train-step 4141, loss 0.287509, acc 0.9
2016-05-21T02:10:45.611452: train-step 4142, loss 0.241702, acc 0.96
2016-05-21T02:10:52.664587: train-step 4143, loss 0.237095, acc 0.96
2016-05-21T02:10:59.642926: train-step 4144, loss 0.238012, acc 0.94
2016-05-21T02:11:06.868551: train-step 4145, loss 0.24997, acc 0.92
2016-05-21T02:11:13.979611: train-step 4146, loss 0.246134, acc 0.93
2016-05-21T02:11:21.081530: train-step 4147, loss 0.31727, acc 0.93
2016-05-21T02:11:28.058936: train-step 4148, loss 0.272338, acc 0.88
2016-05-21T02:11:35.067538: train-step 4149, loss 0.191586, acc 0.95
2016-05-21T02:11:42.200038: train-step 4150, loss 0.291425, acc 0.93
2016-05-21T02:11:49.350697: train-step 4151, loss 0.25541, acc 0.93
2016-05-21T02:11:56.387578: train-step 4152, loss 0.238874, acc 0.93
2016-05-21T02:12:03.444151: train-step 4153, loss 0.207328, acc 0.96
2016-05-21T02:12:10.529254: train-step 4154, loss 0.223576, acc 0.95
2016-05-21T02:12:17.518054: train-step 4155, loss 0.236187, acc 0.96
2016-05-21T02:12:24.581855: train-step 4156, loss 0.244226, acc 0.94
2016-05-21T02:12:31.597936: train-step 4157, loss 0.240906, acc 0.95
2016-05-21T02:12:38.700541: train-step 4158, loss 0.349873, acc 0.88
2016-05-21T02:12:46.064748: train-step 4159, loss 0.287739, acc 0.93
2016-05-21T02:12:53.389224: train-step 4160, loss 0.2498, acc 0.95
2016-05-21T02:13:00.569989: train-step 4161, loss 0.232918, acc 0.95
2016-05-21T02:13:07.640462: train-step 4162, loss 0.263157, acc 0.93
2016-05-21T02:13:14.734926: train-step 4163, loss 0.290304, acc 0.93
2016-05-21T02:13:21.774763: train-step 4164, loss 0.242747, acc 0.94
2016-05-21T02:13:28.859757: train-step 4165, loss 0.256079, acc 0.94
2016-05-21T02:13:35.814239: train-step 4166, loss 0.259756, acc 0.94
2016-05-21T02:13:43.028519: train-step 4167, loss 0.266788, acc 0.94
2016-05-21T02:13:50.044662: train-step 4168, loss 0.235041, acc 0.95
2016-05-21T02:13:57.195004: train-step 4169, loss 0.255873, acc 0.92
2016-05-21T02:14:04.274516: train-step 4170, loss 0.265321, acc 0.92
2016-05-21T02:14:11.295042: train-step 4171, loss 0.246787, acc 0.92
2016-05-21T02:14:18.845769: train-step 4172, loss 0.24356, acc 0.93
2016-05-21T02:14:25.976268: train-step 4173, loss 0.213656, acc 0.96
2016-05-21T02:14:33.058308: train-step 4174, loss 0.281777, acc 0.92
2016-05-21T02:14:40.047464: train-step 4175, loss 0.267766, acc 0.94
2016-05-21T02:14:47.140634: train-step 4176, loss 0.249335, acc 0.96
2016-05-21T02:14:54.246446: train-step 4177, loss 0.301885, acc 0.89
2016-05-21T02:15:01.325440: train-step 4178, loss 0.302816, acc 0.88
2016-05-21T02:15:08.358868: train-step 4179, loss 0.210774, acc 0.95
2016-05-21T02:15:15.394703: train-step 4180, loss 0.202468, acc 0.98
2016-05-21T02:15:22.433944: train-step 4181, loss 0.254559, acc 0.92
2016-05-21T02:15:29.401425: train-step 4182, loss 0.239313, acc 0.94
2016-05-21T02:15:36.941460: train-step 4183, loss 0.31333, acc 0.89
2016-05-21T02:15:44.036405: train-step 4184, loss 0.226931, acc 0.95
2016-05-21T02:15:51.083055: train-step 4185, loss 0.233583, acc 0.95
2016-05-21T02:15:58.610119: train-step 4186, loss 0.187846, acc 0.96
2016-05-21T02:16:05.748931: train-step 4187, loss 0.227363, acc 0.96
2016-05-21T02:16:12.945736: train-step 4188, loss 0.225006, acc 0.94
2016-05-21T02:16:20.360989: train-step 4189, loss 0.275694, acc 0.93
2016-05-21T02:16:27.379682: train-step 4190, loss 0.342861, acc 0.91
2016-05-21T02:16:34.405382: train-step 4191, loss 0.29193, acc 0.91
2016-05-21T02:16:41.572894: train-step 4192, loss 0.227517, acc 0.94
2016-05-21T02:16:48.619997: train-step 4193, loss 0.26708, acc 0.91
2016-05-21T02:16:55.737800: train-step 4194, loss 0.196612, acc 0.96
2016-05-21T02:17:02.722139: train-step 4195, loss 0.278296, acc 0.93
2016-05-21T02:17:09.755321: train-step 4196, loss 0.247932, acc 0.95
2016-05-21T02:17:16.946707: train-step 4197, loss 0.249772, acc 0.95
2016-05-21T02:17:23.968014: train-step 4198, loss 0.243505, acc 0.93
2016-05-21T02:17:31.128348: train-step 4199, loss 0.246123, acc 0.94
2016-05-21T02:17:38.207963: train-step 4200, loss 0.267138, acc 0.94
2016-05-21T02:17:45.205034: train-step 4201, loss 0.26464, acc 0.92
2016-05-21T02:17:52.263880: train-step 4202, loss 0.184734, acc 0.96
2016-05-21T02:17:59.294911: train-step 4203, loss 0.205915, acc 0.96
2016-05-21T02:18:06.758424: train-step 4204, loss 0.271529, acc 0.92
2016-05-21T02:18:13.877611: train-step 4205, loss 0.26176, acc 0.89
2016-05-21T02:18:20.922572: train-step 4206, loss 0.208108, acc 0.94
2016-05-21T02:18:27.875573: train-step 4207, loss 0.174967, acc 0.99
2016-05-21T02:18:34.946811: train-step 4208, loss 0.247112, acc 0.96
2016-05-21T02:18:42.035237: train-step 4209, loss 0.221596, acc 0.95
2016-05-21T02:18:49.075944: train-step 4210, loss 0.248861, acc 0.93
2016-05-21T02:18:55.970434: train-step 4211, loss 0.249956, acc 0.95
2016-05-21T02:19:02.933364: train-step 4212, loss 0.265154, acc 0.93
2016-05-21T02:19:09.929093: train-step 4213, loss 0.20963, acc 0.96
2016-05-21T02:19:17.154120: train-step 4214, loss 0.228505, acc 0.94
2016-05-21T02:19:23.970743: train-step 4215, loss 0.321999, acc 0.87
2016-05-21T02:19:30.930269: train-step 4216, loss 0.246231, acc 0.9
2016-05-21T02:19:37.923161: train-step 4217, loss 0.215561, acc 0.96
2016-05-21T02:19:44.977850: train-step 4218, loss 0.267988, acc 0.89
2016-05-21T02:19:51.878040: train-step 4219, loss 0.297588, acc 0.9
2016-05-21T02:19:58.828845: train-step 4220, loss 0.272202, acc 0.93
2016-05-21T02:20:05.889987: train-step 4221, loss 0.231766, acc 0.95
2016-05-21T02:20:12.957913: train-step 4222, loss 0.262913, acc 0.9
2016-05-21T02:20:19.963419: train-step 4223, loss 0.263153, acc 0.96
2016-05-21T02:20:26.977986: train-step 4224, loss 0.267985, acc 0.93
2016-05-21T02:20:33.983190: train-step 4225, loss 0.280861, acc 0.93
2016-05-21T02:20:40.997065: train-step 4226, loss 0.291557, acc 0.92
2016-05-21T02:20:48.041722: train-step 4227, loss 0.195901, acc 0.97
2016-05-21T02:20:54.983406: train-step 4228, loss 0.267759, acc 0.92
2016-05-21T02:21:02.177364: train-step 4229, loss 0.231331, acc 0.94
2016-05-21T02:21:09.267237: train-step 4230, loss 0.265216, acc 0.92
2016-05-21T02:21:16.350269: train-step 4231, loss 0.286996, acc 0.92
2016-05-21T02:21:23.417405: train-step 4232, loss 0.231781, acc 0.97
2016-05-21T02:21:30.520009: train-step 4233, loss 0.215903, acc 0.96
2016-05-21T02:21:37.627562: train-step 4234, loss 0.239455, acc 0.94
2016-05-21T02:21:44.878670: train-step 4235, loss 0.213303, acc 0.94
2016-05-21T02:21:51.962652: train-step 4236, loss 0.206011, acc 0.95
2016-05-21T02:21:59.671145: train-step 4237, loss 0.24803, acc 0.94
2016-05-21T02:22:07.173491: train-step 4238, loss 0.279562, acc 0.94
2016-05-21T02:22:14.430146: train-step 4239, loss 0.193107, acc 0.99
2016-05-21T02:22:22.009969: train-step 4240, loss 0.236529, acc 0.95
2016-05-21T02:22:29.358117: train-step 4241, loss 0.228871, acc 0.94
2016-05-21T02:22:36.625736: train-step 4242, loss 0.225609, acc 0.96
2016-05-21T02:22:42.858558: train-step 4243, loss 0.272105, acc 0.92
2016-05-21T02:22:49.126056: train-step 4244, loss 0.274117, acc 0.93
2016-05-21T02:22:55.367437: train-step 4245, loss 0.246572, acc 0.96
2016-05-21T02:23:01.629963: train-step 4246, loss 0.233487, acc 0.95
2016-05-21T02:23:08.001886: train-step 4247, loss 0.248592, acc 0.95
2016-05-21T02:23:14.198883: train-step 4248, loss 0.2236, acc 0.94
2016-05-21T02:23:20.750070: train-step 4249, loss 0.265597, acc 0.93
2016-05-21T02:23:27.077596: train-step 4250, loss 0.312092, acc 0.88
epoch number is: 17
2016-05-21T02:23:33.646944: train-step 4251, loss 0.192916, acc 0.97
2016-05-21T02:23:39.969546: train-step 4252, loss 0.259749, acc 0.94
2016-05-21T02:23:46.145555: train-step 4253, loss 0.231745, acc 0.96
2016-05-21T02:23:52.504662: train-step 4254, loss 0.199566, acc 0.96
2016-05-21T02:23:58.862386: train-step 4255, loss 0.305732, acc 0.92
2016-05-21T02:24:05.218901: train-step 4256, loss 0.264698, acc 0.93
2016-05-21T02:24:11.475088: train-step 4257, loss 0.24794, acc 0.94
2016-05-21T02:24:17.750946: train-step 4258, loss 0.233576, acc 0.95
2016-05-21T02:24:24.139879: train-step 4259, loss 0.242951, acc 0.94
2016-05-21T02:24:30.404943: train-step 4260, loss 0.230824, acc 0.92
2016-05-21T02:24:37.005719: train-step 4261, loss 0.260113, acc 0.94
2016-05-21T02:24:43.386311: train-step 4262, loss 0.2152, acc 0.96
2016-05-21T02:24:49.836575: train-step 4263, loss 0.248233, acc 0.93
2016-05-21T02:24:56.303948: train-step 4264, loss 0.259449, acc 0.96
2016-05-21T02:25:02.731338: train-step 4265, loss 0.20552, acc 0.96
2016-05-21T02:25:09.511994: train-step 4266, loss 0.230339, acc 0.94
2016-05-21T02:25:16.543254: train-step 4267, loss 0.214167, acc 0.97
2016-05-21T02:25:24.030382: train-step 4268, loss 0.213738, acc 0.96
2016-05-21T02:25:31.686903: train-step 4269, loss 0.284108, acc 0.94
2016-05-21T02:25:38.383193: train-step 4270, loss 0.21834, acc 0.97
2016-05-21T02:25:44.779020: train-step 4271, loss 0.216574, acc 0.94
2016-05-21T02:25:51.077842: train-step 4272, loss 0.221582, acc 0.95
2016-05-21T02:25:57.679219: train-step 4273, loss 0.228767, acc 0.96
2016-05-21T02:26:04.121190: train-step 4274, loss 0.230433, acc 0.95
2016-05-21T02:26:10.401521: train-step 4275, loss 0.224651, acc 0.96
2016-05-21T02:26:16.698342: train-step 4276, loss 0.214903, acc 0.94
2016-05-21T02:26:23.197372: train-step 4277, loss 0.222604, acc 0.93
2016-05-21T02:26:29.751259: train-step 4278, loss 0.236626, acc 0.95
2016-05-21T02:26:36.095799: train-step 4279, loss 0.361758, acc 0.87
2016-05-21T02:26:42.445341: train-step 4280, loss 0.261489, acc 0.92
2016-05-21T02:26:48.898516: train-step 4281, loss 0.259242, acc 0.93
2016-05-21T02:26:55.379841: train-step 4282, loss 0.303388, acc 0.92
2016-05-21T02:27:01.836698: train-step 4283, loss 0.209697, acc 0.97
2016-05-21T02:27:08.225506: train-step 4284, loss 0.287392, acc 0.93
2016-05-21T02:27:14.720106: train-step 4285, loss 0.245769, acc 0.95
2016-05-21T02:27:21.155127: train-step 4286, loss 0.32801, acc 0.92
2016-05-21T02:27:27.446365: train-step 4287, loss 0.266688, acc 0.91
2016-05-21T02:27:33.669967: train-step 4288, loss 0.318057, acc 0.91
2016-05-21T02:27:40.087784: train-step 4289, loss 0.250673, acc 0.93
2016-05-21T02:27:46.511664: train-step 4290, loss 0.212789, acc 0.97
2016-05-21T02:27:53.154536: train-step 4291, loss 0.247057, acc 0.92
2016-05-21T02:27:59.672316: train-step 4292, loss 0.198029, acc 0.97
2016-05-21T02:28:06.204998: train-step 4293, loss 0.184032, acc 0.98
2016-05-21T02:28:12.607003: train-step 4294, loss 0.253093, acc 0.94
2016-05-21T02:28:19.554339: train-step 4295, loss 0.267031, acc 0.94
2016-05-21T02:28:26.420979: train-step 4296, loss 0.270756, acc 0.93
2016-05-21T02:28:32.951211: train-step 4297, loss 0.227487, acc 0.95
2016-05-21T02:28:39.738924: train-step 4298, loss 0.227448, acc 0.92
2016-05-21T02:28:46.732111: train-step 4299, loss 0.244082, acc 0.94
2016-05-21T02:28:53.774101: train-step 4300, loss 0.232969, acc 0.95
2016-05-21T02:29:00.642716: train-step 4301, loss 0.212509, acc 0.96
2016-05-21T02:29:07.696570: train-step 4302, loss 0.267384, acc 0.94
2016-05-21T02:29:14.795207: train-step 4303, loss 0.252781, acc 0.96
2016-05-21T02:29:21.896289: train-step 4304, loss 0.230247, acc 0.94
2016-05-21T02:29:28.965594: train-step 4305, loss 0.23731, acc 0.96
2016-05-21T02:29:35.987828: train-step 4306, loss 0.236962, acc 0.95
2016-05-21T02:29:42.983052: train-step 4307, loss 0.249462, acc 0.94
2016-05-21T02:29:50.089509: train-step 4308, loss 0.244466, acc 0.95
2016-05-21T02:29:57.066321: train-step 4309, loss 0.234642, acc 0.92
2016-05-21T02:30:04.151629: train-step 4310, loss 0.251193, acc 0.93
2016-05-21T02:30:11.712763: train-step 4311, loss 0.211936, acc 0.95
2016-05-21T02:30:18.777499: train-step 4312, loss 0.276986, acc 0.93
2016-05-21T02:30:25.844713: train-step 4313, loss 0.254047, acc 0.93
2016-05-21T02:30:32.802440: train-step 4314, loss 0.194365, acc 0.98
2016-05-21T02:30:39.973554: train-step 4315, loss 0.213299, acc 0.96
2016-05-21T02:30:47.050605: train-step 4316, loss 0.221853, acc 0.95
2016-05-21T02:30:54.011975: train-step 4317, loss 0.264503, acc 0.94
2016-05-21T02:31:01.104467: train-step 4318, loss 0.285346, acc 0.91
2016-05-21T02:31:08.209868: train-step 4319, loss 0.282026, acc 0.91
2016-05-21T02:31:15.689787: train-step 4320, loss 0.227114, acc 0.97
2016-05-21T02:31:22.923965: train-step 4321, loss 0.190093, acc 0.97
2016-05-21T02:31:30.123086: train-step 4322, loss 0.240229, acc 0.93
2016-05-21T02:31:37.421768: train-step 4323, loss 0.216313, acc 0.97
2016-05-21T02:31:44.666154: train-step 4324, loss 0.258896, acc 0.9
2016-05-21T02:31:51.734500: train-step 4325, loss 0.22829, acc 0.97
2016-05-21T02:31:59.195074: train-step 4326, loss 0.219553, acc 0.96
2016-05-21T02:32:06.359991: train-step 4327, loss 0.236194, acc 0.95
2016-05-21T02:32:13.558748: train-step 4328, loss 0.207903, acc 0.99
2016-05-21T02:32:21.083519: train-step 4329, loss 0.220604, acc 0.96
2016-05-21T02:32:28.286517: train-step 4330, loss 0.224702, acc 0.96
2016-05-21T02:32:35.356597: train-step 4331, loss 0.214521, acc 0.94
2016-05-21T02:32:42.597535: train-step 4332, loss 0.251836, acc 0.92
2016-05-21T02:32:50.000236: train-step 4333, loss 0.234461, acc 0.94
2016-05-21T02:32:56.981571: train-step 4334, loss 0.25008, acc 0.91
2016-05-21T02:33:03.949539: train-step 4335, loss 0.205805, acc 0.97
2016-05-21T02:33:11.084629: train-step 4336, loss 0.247483, acc 0.91
2016-05-21T02:33:18.126078: train-step 4337, loss 0.253054, acc 0.93
2016-05-21T02:33:25.181129: train-step 4338, loss 0.35792, acc 0.9
2016-05-21T02:33:32.204164: train-step 4339, loss 0.205152, acc 0.95
2016-05-21T02:33:39.803972: train-step 4340, loss 0.267818, acc 0.93
2016-05-21T02:33:46.906000: train-step 4341, loss 0.267389, acc 0.92
2016-05-21T02:33:53.903783: train-step 4342, loss 0.218279, acc 0.96
2016-05-21T02:34:00.806930: train-step 4343, loss 0.214787, acc 0.94
2016-05-21T02:34:07.704097: train-step 4344, loss 0.257472, acc 0.92
2016-05-21T02:34:14.841898: train-step 4345, loss 0.175305, acc 0.97
2016-05-21T02:34:22.203232: train-step 4346, loss 0.20436, acc 0.98
2016-05-21T02:34:29.212767: train-step 4347, loss 0.291167, acc 0.91
2016-05-21T02:34:36.268320: train-step 4348, loss 0.198072, acc 0.96
2016-05-21T02:34:43.424474: train-step 4349, loss 0.251295, acc 0.91
2016-05-21T02:34:50.675219: train-step 4350, loss 0.230042, acc 0.96
2016-05-21T02:34:57.809619: train-step 4351, loss 0.199313, acc 0.98
2016-05-21T02:35:04.889941: train-step 4352, loss 0.228835, acc 0.95
2016-05-21T02:35:11.786768: train-step 4353, loss 0.24771, acc 0.92
2016-05-21T02:35:18.989229: train-step 4354, loss 0.286295, acc 0.92
2016-05-21T02:35:25.955040: train-step 4355, loss 0.225287, acc 0.96
2016-05-21T02:35:32.958596: train-step 4356, loss 0.256688, acc 0.93
2016-05-21T02:35:39.880426: train-step 4357, loss 0.242502, acc 0.94
2016-05-21T02:35:47.049502: train-step 4358, loss 0.262189, acc 0.92
2016-05-21T02:35:54.095777: train-step 4359, loss 0.229348, acc 0.98
2016-05-21T02:36:01.181193: train-step 4360, loss 0.304564, acc 0.89
2016-05-21T02:36:08.201786: train-step 4361, loss 0.239718, acc 0.94
2016-05-21T02:36:15.300022: train-step 4362, loss 0.24178, acc 0.95
2016-05-21T02:36:22.518798: train-step 4363, loss 0.259666, acc 0.94
2016-05-21T02:36:29.936171: train-step 4364, loss 0.208322, acc 0.95
2016-05-21T02:36:36.933050: train-step 4365, loss 0.192547, acc 0.98
2016-05-21T02:36:43.844433: train-step 4366, loss 0.238623, acc 0.96
2016-05-21T02:36:50.900735: train-step 4367, loss 0.186127, acc 0.95
2016-05-21T02:36:57.835338: train-step 4368, loss 0.274727, acc 0.91
2016-05-21T02:37:05.227255: train-step 4369, loss 0.283039, acc 0.9
2016-05-21T02:37:12.407558: train-step 4370, loss 0.228141, acc 0.92
2016-05-21T02:37:19.736808: train-step 4371, loss 0.230148, acc 0.95
2016-05-21T02:37:26.693209: train-step 4372, loss 0.184602, acc 0.99
2016-05-21T02:37:34.151372: train-step 4373, loss 0.228726, acc 0.96
2016-05-21T02:37:41.347294: train-step 4374, loss 0.237612, acc 0.95
2016-05-21T02:37:48.436051: train-step 4375, loss 0.229927, acc 0.94
2016-05-21T02:37:55.875864: train-step 4376, loss 0.198354, acc 0.96
2016-05-21T02:38:03.013854: train-step 4377, loss 0.33001, acc 0.89
2016-05-21T02:38:10.040606: train-step 4378, loss 0.183126, acc 0.98
2016-05-21T02:38:17.003592: train-step 4379, loss 0.27853, acc 0.95
2016-05-21T02:38:23.964391: train-step 4380, loss 0.21207, acc 0.96
2016-05-21T02:38:30.943832: train-step 4381, loss 0.178878, acc 0.99
2016-05-21T02:38:37.828802: train-step 4382, loss 0.225917, acc 0.96
2016-05-21T02:38:45.067893: train-step 4383, loss 0.233858, acc 0.96
2016-05-21T02:38:52.141827: train-step 4384, loss 0.292138, acc 0.89
2016-05-21T02:38:59.219054: train-step 4385, loss 0.293295, acc 0.9
2016-05-21T02:39:06.294783: train-step 4386, loss 0.188052, acc 0.98
2016-05-21T02:39:13.227839: train-step 4387, loss 0.237617, acc 0.93
2016-05-21T02:39:20.319443: train-step 4388, loss 0.256721, acc 0.92
2016-05-21T02:39:27.431548: train-step 4389, loss 0.237724, acc 0.93
2016-05-21T02:39:34.511073: train-step 4390, loss 0.249579, acc 0.92
2016-05-21T02:39:41.652327: train-step 4391, loss 0.204286, acc 0.96
2016-05-21T02:39:48.738677: train-step 4392, loss 0.265559, acc 0.91
2016-05-21T02:39:56.199493: train-step 4393, loss 0.228346, acc 0.96
2016-05-21T02:40:03.436406: train-step 4394, loss 0.269727, acc 0.92
2016-05-21T02:40:10.528020: train-step 4395, loss 0.248394, acc 0.97
2016-05-21T02:40:17.627070: train-step 4396, loss 0.254796, acc 0.92
2016-05-21T02:40:25.147178: train-step 4397, loss 0.209518, acc 0.97
2016-05-21T02:40:32.129795: train-step 4398, loss 0.204812, acc 0.98
2016-05-21T02:40:39.189870: train-step 4399, loss 0.255722, acc 0.95
2016-05-21T02:40:46.253162: train-step 4400, loss 0.211465, acc 0.98
2016-05-21T02:40:53.134703: train-step 4401, loss 0.199833, acc 0.94
2016-05-21T02:40:59.966926: train-step 4402, loss 0.296078, acc 0.89
2016-05-21T02:41:07.189874: train-step 4403, loss 0.215592, acc 0.94
2016-05-21T02:41:14.550290: train-step 4404, loss 0.187084, acc 0.97
2016-05-21T02:41:21.530062: train-step 4405, loss 0.219452, acc 0.94
2016-05-21T02:41:28.521825: train-step 4406, loss 0.240901, acc 0.97
2016-05-21T02:41:35.527715: train-step 4407, loss 0.233229, acc 0.96
2016-05-21T02:41:42.701581: train-step 4408, loss 0.330225, acc 0.87
2016-05-21T02:41:50.063370: train-step 4409, loss 0.269841, acc 0.91
2016-05-21T02:41:57.297058: train-step 4410, loss 0.182346, acc 1
2016-05-21T02:42:04.431963: train-step 4411, loss 0.229698, acc 0.97
2016-05-21T02:42:11.483456: train-step 4412, loss 0.247888, acc 0.93
2016-05-21T02:42:18.506109: train-step 4413, loss 0.211436, acc 0.96
2016-05-21T02:42:25.515651: train-step 4414, loss 0.214033, acc 0.97
2016-05-21T02:42:32.604457: train-step 4415, loss 0.21517, acc 0.96
2016-05-21T02:42:39.675974: train-step 4416, loss 0.230468, acc 0.94
2016-05-21T02:42:46.829344: train-step 4417, loss 0.215689, acc 0.96
2016-05-21T02:42:54.213149: train-step 4418, loss 0.26885, acc 0.95
2016-05-21T02:43:01.140721: train-step 4419, loss 0.255364, acc 0.96
2016-05-21T02:43:07.984109: train-step 4420, loss 0.296123, acc 0.89
2016-05-21T02:43:15.137942: train-step 4421, loss 0.247689, acc 0.93
2016-05-21T02:43:22.173387: train-step 4422, loss 0.233207, acc 0.97
2016-05-21T02:43:29.595056: train-step 4423, loss 0.29958, acc 0.88
2016-05-21T02:43:36.811216: train-step 4424, loss 0.210186, acc 0.94
2016-05-21T02:43:43.966040: train-step 4425, loss 0.202967, acc 0.97
2016-05-21T02:43:51.126102: train-step 4426, loss 0.24355, acc 0.91
2016-05-21T02:43:58.104618: train-step 4427, loss 0.244521, acc 0.93
2016-05-21T02:44:05.436564: train-step 4428, loss 0.210419, acc 0.94
2016-05-21T02:44:12.656589: train-step 4429, loss 0.194841, acc 0.97
2016-05-21T02:44:19.922374: train-step 4430, loss 0.188722, acc 0.97
2016-05-21T02:44:27.192419: train-step 4431, loss 0.259966, acc 0.93
2016-05-21T02:44:34.423658: train-step 4432, loss 0.328517, acc 0.87
2016-05-21T02:44:41.368836: train-step 4433, loss 0.242808, acc 0.93
2016-05-21T02:44:48.249070: train-step 4434, loss 0.236798, acc 0.95
2016-05-21T02:44:55.434466: train-step 4435, loss 0.225732, acc 0.94
2016-05-21T02:45:02.400179: train-step 4436, loss 0.220489, acc 0.96
2016-05-21T02:45:09.365284: train-step 4437, loss 0.245209, acc 0.92
2016-05-21T02:45:16.269209: train-step 4438, loss 0.254804, acc 0.94
2016-05-21T02:45:23.418287: train-step 4439, loss 0.190314, acc 0.98
2016-05-21T02:45:30.394445: train-step 4440, loss 0.241659, acc 0.97
2016-05-21T02:45:37.409921: train-step 4441, loss 0.235427, acc 0.95
2016-05-21T02:45:44.370089: train-step 4442, loss 0.221489, acc 0.97
2016-05-21T02:45:51.369697: train-step 4443, loss 0.227864, acc 0.94
2016-05-21T02:45:58.357356: train-step 4444, loss 0.318662, acc 0.9
2016-05-21T02:46:05.324160: train-step 4445, loss 0.268971, acc 0.94
2016-05-21T02:46:12.304448: train-step 4446, loss 0.28764, acc 0.93
2016-05-21T02:46:19.683118: train-step 4447, loss 0.285226, acc 0.93
2016-05-21T02:46:26.843039: train-step 4448, loss 0.241343, acc 0.94
2016-05-21T02:46:33.865369: train-step 4449, loss 0.213678, acc 0.96
2016-05-21T02:46:40.896296: train-step 4450, loss 0.23955, acc 0.95
2016-05-21T02:46:48.350820: train-step 4451, loss 0.27089, acc 0.93
2016-05-21T02:46:55.480542: train-step 4452, loss 0.21565, acc 0.97
2016-05-21T02:47:02.637219: train-step 4453, loss 0.251218, acc 0.93
2016-05-21T02:47:09.860183: train-step 4454, loss 0.22962, acc 0.95
2016-05-21T02:47:16.853350: train-step 4455, loss 0.245236, acc 0.95
2016-05-21T02:47:24.309015: train-step 4456, loss 0.178124, acc 0.99
2016-05-21T02:47:31.368515: train-step 4457, loss 0.324574, acc 0.91
2016-05-21T02:47:38.501473: train-step 4458, loss 0.293596, acc 0.9
2016-05-21T02:47:45.942116: train-step 4459, loss 0.23361, acc 0.96
2016-05-21T02:47:53.221112: train-step 4460, loss 0.233682, acc 0.93
2016-05-21T02:48:00.497851: train-step 4461, loss 0.240883, acc 0.97
2016-05-21T02:48:07.649074: train-step 4462, loss 0.205673, acc 0.95
2016-05-21T02:48:14.713999: train-step 4463, loss 0.317283, acc 0.91
2016-05-21T02:48:21.640820: train-step 4464, loss 0.240386, acc 0.96
2016-05-21T02:48:28.590897: train-step 4465, loss 0.251286, acc 0.92
2016-05-21T02:48:35.619313: train-step 4466, loss 0.277615, acc 0.93
2016-05-21T02:48:42.565480: train-step 4467, loss 0.217888, acc 0.96
2016-05-21T02:48:49.558356: train-step 4468, loss 0.216897, acc 0.97
2016-05-21T02:48:56.626346: train-step 4469, loss 0.217577, acc 0.93
2016-05-21T02:49:03.630453: train-step 4470, loss 0.232973, acc 0.95
2016-05-21T02:49:11.013538: train-step 4471, loss 0.263713, acc 0.93
2016-05-21T02:49:18.217345: train-step 4472, loss 0.209787, acc 0.96
2016-05-21T02:49:25.388409: train-step 4473, loss 0.23264, acc 0.95
2016-05-21T02:49:32.419435: train-step 4474, loss 0.253133, acc 0.94
2016-05-21T02:49:39.431512: train-step 4475, loss 0.258302, acc 0.94
2016-05-21T02:49:46.441899: train-step 4476, loss 0.223857, acc 0.94
2016-05-21T02:49:53.648820: train-step 4477, loss 0.262923, acc 0.92
2016-05-21T02:50:00.701910: train-step 4478, loss 0.223483, acc 0.97
2016-05-21T02:50:08.154342: train-step 4479, loss 0.226573, acc 0.94
2016-05-21T02:50:15.419384: train-step 4480, loss 0.204265, acc 0.95
2016-05-21T02:50:22.617781: train-step 4481, loss 0.231747, acc 0.98
2016-05-21T02:50:29.677992: train-step 4482, loss 0.26427, acc 0.92
2016-05-21T02:50:36.945414: train-step 4483, loss 0.233042, acc 0.94
2016-05-21T02:50:43.929556: train-step 4484, loss 0.222078, acc 0.95
2016-05-21T02:50:50.931220: train-step 4485, loss 0.198341, acc 0.97
2016-05-21T02:50:57.992960: train-step 4486, loss 0.242664, acc 0.93
2016-05-21T02:51:05.444067: train-step 4487, loss 0.224403, acc 0.95
2016-05-21T02:51:12.777416: train-step 4488, loss 0.200592, acc 0.98
2016-05-21T02:51:20.088883: train-step 4489, loss 0.246243, acc 0.94
2016-05-21T02:51:27.364241: train-step 4490, loss 0.244649, acc 0.96
2016-05-21T02:51:34.401271: train-step 4491, loss 0.288703, acc 0.91
2016-05-21T02:51:41.918787: train-step 4492, loss 0.242014, acc 0.92
2016-05-21T02:51:49.195489: train-step 4493, loss 0.253299, acc 0.92
2016-05-21T02:51:56.312901: train-step 4494, loss 0.207537, acc 0.95
2016-05-21T02:52:03.738986: train-step 4495, loss 0.207005, acc 0.97
2016-05-21T02:52:10.907356: train-step 4496, loss 0.25068, acc 0.95
2016-05-21T02:52:17.883320: train-step 4497, loss 0.251607, acc 0.93
2016-05-21T02:52:25.001696: train-step 4498, loss 0.268044, acc 0.92
2016-05-21T02:52:32.094672: train-step 4499, loss 0.245587, acc 0.94
2016-05-21T02:52:39.251587: train-step 4500, loss 0.24431, acc 0.93
epoch number is: 18
2016-05-21T02:52:46.647926: train-step 4501, loss 0.236244, acc 0.94
2016-05-21T02:52:54.089099: train-step 4502, loss 0.221727, acc 0.95
2016-05-21T02:53:01.257458: train-step 4503, loss 0.208984, acc 0.98
2016-05-21T02:53:08.620184: train-step 4504, loss 0.255574, acc 0.93
2016-05-21T02:53:15.980147: train-step 4505, loss 0.250667, acc 0.93
2016-05-21T02:53:23.091635: train-step 4506, loss 0.2192, acc 0.95
2016-05-21T02:53:30.270509: train-step 4507, loss 0.168609, acc 1
2016-05-21T02:53:37.394688: train-step 4508, loss 0.197862, acc 0.98
2016-05-21T02:53:44.497349: train-step 4509, loss 0.171052, acc 0.99
2016-05-21T02:53:51.648138: train-step 4510, loss 0.246669, acc 0.92
2016-05-21T02:53:58.702256: train-step 4511, loss 0.283763, acc 0.92
2016-05-21T02:54:05.830982: train-step 4512, loss 0.224884, acc 0.92
2016-05-21T02:54:12.977480: train-step 4513, loss 0.228616, acc 0.96
2016-05-21T02:54:20.073108: train-step 4514, loss 0.23184, acc 0.93
2016-05-21T02:54:27.123285: train-step 4515, loss 0.22106, acc 0.96
2016-05-21T02:54:34.294762: train-step 4516, loss 0.217798, acc 0.94
2016-05-21T02:54:41.423889: train-step 4517, loss 0.216906, acc 0.97
2016-05-21T02:54:48.789227: train-step 4518, loss 0.242776, acc 0.96
2016-05-21T02:54:56.182272: train-step 4519, loss 0.254114, acc 0.94
2016-05-21T02:55:03.349408: train-step 4520, loss 0.268209, acc 0.93
2016-05-21T02:55:10.843373: train-step 4521, loss 0.267563, acc 0.9
2016-05-21T02:55:18.140714: train-step 4522, loss 0.228289, acc 0.93
2016-05-21T02:55:25.155757: train-step 4523, loss 0.273449, acc 0.95
2016-05-21T02:55:32.274847: train-step 4524, loss 0.194631, acc 0.96
2016-05-21T02:55:39.706558: train-step 4525, loss 0.238445, acc 0.92
2016-05-21T02:55:46.957390: train-step 4526, loss 0.27558, acc 0.93
2016-05-21T02:55:53.898447: train-step 4527, loss 0.202334, acc 0.94
2016-05-21T02:56:00.853984: train-step 4528, loss 0.168001, acc 1
2016-05-21T02:56:07.971515: train-step 4529, loss 0.218627, acc 0.96
2016-05-21T02:56:15.119233: train-step 4530, loss 0.212112, acc 0.96
2016-05-21T02:56:22.141896: train-step 4531, loss 0.272518, acc 0.94
2016-05-21T02:56:29.129348: train-step 4532, loss 0.249441, acc 0.92
2016-05-21T02:56:36.151567: train-step 4533, loss 0.208674, acc 0.95
2016-05-21T02:56:43.374165: train-step 4534, loss 0.219474, acc 0.95
2016-05-21T02:56:50.360734: train-step 4535, loss 0.249941, acc 0.94
2016-05-21T02:56:57.552867: train-step 4536, loss 0.221864, acc 0.96
2016-05-21T02:57:04.660442: train-step 4537, loss 0.242067, acc 0.92
2016-05-21T02:57:11.998003: train-step 4538, loss 0.226666, acc 0.96
2016-05-21T02:57:19.447954: train-step 4539, loss 0.303493, acc 0.89
2016-05-21T02:57:26.731399: train-step 4540, loss 0.248504, acc 0.94
2016-05-21T02:57:33.722952: train-step 4541, loss 0.267037, acc 0.92
2016-05-21T02:57:41.128610: train-step 4542, loss 0.257697, acc 0.92
2016-05-21T02:57:48.495823: train-step 4543, loss 0.255837, acc 0.93
2016-05-21T02:57:55.839757: train-step 4544, loss 0.215879, acc 0.96
2016-05-21T02:58:02.966261: train-step 4545, loss 0.190044, acc 0.96
2016-05-21T02:58:10.526432: train-step 4546, loss 0.245217, acc 0.95
2016-05-21T02:58:17.811976: train-step 4547, loss 0.186201, acc 0.97
2016-05-21T02:58:25.133958: train-step 4548, loss 0.208683, acc 0.97
2016-05-21T02:58:32.580414: train-step 4549, loss 0.246799, acc 0.95
2016-05-21T02:58:39.748070: train-step 4550, loss 0.229284, acc 0.95
2016-05-21T02:58:47.247874: train-step 4551, loss 0.243507, acc 0.92
2016-05-21T02:58:54.573671: train-step 4552, loss 0.264357, acc 0.91
2016-05-21T02:59:01.643017: train-step 4553, loss 0.225793, acc 0.95
2016-05-21T02:59:09.208929: train-step 4554, loss 0.227503, acc 0.96
2016-05-21T02:59:16.479628: train-step 4555, loss 0.272677, acc 0.92
2016-05-21T02:59:23.649435: train-step 4556, loss 0.240245, acc 0.94
2016-05-21T02:59:30.778001: train-step 4557, loss 0.221868, acc 0.94
2016-05-21T02:59:38.626992: train-step 4558, loss 0.282076, acc 0.91
2016-05-21T02:59:46.334993: train-step 4559, loss 0.250589, acc 0.92
2016-05-21T02:59:53.734654: train-step 4560, loss 0.177711, acc 0.99
2016-05-21T03:00:01.496971: train-step 4561, loss 0.207933, acc 0.97
2016-05-21T03:00:08.591966: train-step 4562, loss 0.216878, acc 0.96
2016-05-21T03:00:16.308219: train-step 4563, loss 0.22674, acc 0.93
2016-05-21T03:00:23.922728: train-step 4564, loss 0.220871, acc 0.94
2016-05-21T03:00:30.975920: train-step 4565, loss 0.211463, acc 0.97
2016-05-21T03:00:37.296744: train-step 4566, loss 0.224247, acc 0.96
2016-05-21T03:00:43.916653: train-step 4567, loss 0.243981, acc 0.94
2016-05-21T03:00:50.359323: train-step 4568, loss 0.218669, acc 0.95
2016-05-21T03:00:56.730164: train-step 4569, loss 0.181596, acc 0.97
2016-05-21T03:01:03.690397: train-step 4570, loss 0.325363, acc 0.87
2016-05-21T03:01:10.183192: train-step 4571, loss 0.213302, acc 0.97
2016-05-21T03:01:16.714860: train-step 4572, loss 0.242375, acc 0.93
2016-05-21T03:01:23.553411: train-step 4573, loss 0.23196, acc 0.94
2016-05-21T03:01:30.082029: train-step 4574, loss 0.205206, acc 0.96
2016-05-21T03:01:36.313148: train-step 4575, loss 0.2177, acc 0.96
2016-05-21T03:01:42.537607: train-step 4576, loss 0.293451, acc 0.94
2016-05-21T03:01:48.736633: train-step 4577, loss 0.21118, acc 0.95
2016-05-21T03:01:55.020232: train-step 4578, loss 0.237954, acc 0.96
2016-05-21T03:02:01.263452: train-step 4579, loss 0.22384, acc 0.95
2016-05-21T03:02:07.821431: train-step 4580, loss 0.214258, acc 0.94
2016-05-21T03:02:14.210881: train-step 4581, loss 0.270024, acc 0.94
2016-05-21T03:02:20.448778: train-step 4582, loss 0.196153, acc 0.97
2016-05-21T03:02:26.693796: train-step 4583, loss 0.217846, acc 0.97
2016-05-21T03:02:33.115147: train-step 4584, loss 0.244961, acc 0.91
2016-05-21T03:02:39.312695: train-step 4585, loss 0.231783, acc 0.95
2016-05-21T03:02:45.878448: train-step 4586, loss 0.31649, acc 0.91
2016-05-21T03:02:52.271841: train-step 4587, loss 0.249293, acc 0.95
2016-05-21T03:02:58.703184: train-step 4588, loss 0.229875, acc 0.94
2016-05-21T03:03:05.091943: train-step 4589, loss 0.212637, acc 0.96
2016-05-21T03:03:11.375678: train-step 4590, loss 0.255338, acc 0.95
2016-05-21T03:03:17.621504: train-step 4591, loss 0.245601, acc 0.95
2016-05-21T03:03:24.197824: train-step 4592, loss 0.236999, acc 0.98
2016-05-21T03:03:30.551689: train-step 4593, loss 0.28494, acc 0.89
2016-05-21T03:03:36.748942: train-step 4594, loss 0.174348, acc 0.98
2016-05-21T03:03:42.970974: train-step 4595, loss 0.265929, acc 0.94
2016-05-21T03:03:49.346103: train-step 4596, loss 0.205536, acc 0.97
2016-05-21T03:03:55.552240: train-step 4597, loss 0.208251, acc 0.97
2016-05-21T03:04:01.818779: train-step 4598, loss 0.241748, acc 0.94
2016-05-21T03:04:08.050710: train-step 4599, loss 0.213352, acc 0.97
2016-05-21T03:04:14.294764: train-step 4600, loss 0.155197, acc 1
2016-05-21T03:04:20.494481: train-step 4601, loss 0.183252, acc 0.99
2016-05-21T03:04:26.710481: train-step 4602, loss 0.217105, acc 0.96
2016-05-21T03:04:32.885582: train-step 4603, loss 0.235146, acc 0.94
2016-05-21T03:04:39.115613: train-step 4604, loss 0.215521, acc 0.95
2016-05-21T03:04:45.318347: train-step 4605, loss 0.244052, acc 0.95
2016-05-21T03:04:51.688633: train-step 4606, loss 0.217043, acc 0.97
2016-05-21T03:04:57.966765: train-step 4607, loss 0.271721, acc 0.92
2016-05-21T03:05:04.181122: train-step 4608, loss 0.20899, acc 0.94
2016-05-21T03:05:10.415716: train-step 4609, loss 0.273775, acc 0.93
2016-05-21T03:05:16.617733: train-step 4610, loss 0.218049, acc 0.97
2016-05-21T03:05:22.908807: train-step 4611, loss 0.201716, acc 0.97
2016-05-21T03:05:29.482397: train-step 4612, loss 0.253199, acc 0.95
2016-05-21T03:05:35.818648: train-step 4613, loss 0.217977, acc 0.94
2016-05-21T03:05:42.028145: train-step 4614, loss 0.237482, acc 0.93
2016-05-21T03:05:48.319037: train-step 4615, loss 0.200559, acc 0.99
2016-05-21T03:05:54.688027: train-step 4616, loss 0.22346, acc 0.96
2016-05-21T03:06:01.100750: train-step 4617, loss 0.241427, acc 0.95
2016-05-21T03:06:07.520717: train-step 4618, loss 0.268937, acc 0.94
2016-05-21T03:06:13.797043: train-step 4619, loss 0.202152, acc 0.97
2016-05-21T03:06:20.204739: train-step 4620, loss 0.229079, acc 0.94
2016-05-21T03:06:26.746089: train-step 4621, loss 0.197107, acc 0.97
2016-05-21T03:06:33.132579: train-step 4622, loss 0.233739, acc 0.95
2016-05-21T03:06:39.454130: train-step 4623, loss 0.263232, acc 0.94
2016-05-21T03:06:45.723811: train-step 4624, loss 0.264166, acc 0.91
2016-05-21T03:06:52.308491: train-step 4625, loss 0.251694, acc 0.93
2016-05-21T03:06:58.687309: train-step 4626, loss 0.240654, acc 0.94
2016-05-21T03:07:04.900837: train-step 4627, loss 0.202713, acc 0.97
2016-05-21T03:07:11.448899: train-step 4628, loss 0.220511, acc 0.92
2016-05-21T03:07:17.771461: train-step 4629, loss 0.246189, acc 0.94
2016-05-21T03:07:24.117298: train-step 4630, loss 0.248225, acc 0.92
2016-05-21T03:07:30.346750: train-step 4631, loss 0.261297, acc 0.94
2016-05-21T03:07:36.624455: train-step 4632, loss 0.239477, acc 0.95
2016-05-21T03:07:42.870302: train-step 4633, loss 0.262447, acc 0.93
2016-05-21T03:07:49.056276: train-step 4634, loss 0.217901, acc 0.95
2016-05-21T03:07:55.564806: train-step 4635, loss 0.215397, acc 0.96
2016-05-21T03:08:01.925509: train-step 4636, loss 0.232412, acc 0.93
2016-05-21T03:08:08.309977: train-step 4637, loss 0.25284, acc 0.92
2016-05-21T03:08:14.531179: train-step 4638, loss 0.27321, acc 0.93
2016-05-21T03:08:20.737457: train-step 4639, loss 0.222567, acc 0.95
2016-05-21T03:08:26.911257: train-step 4640, loss 0.204651, acc 0.96
2016-05-21T03:08:33.299484: train-step 4641, loss 0.229016, acc 0.95
2016-05-21T03:08:39.785836: train-step 4642, loss 0.283753, acc 0.91
2016-05-21T03:08:45.981344: train-step 4643, loss 0.246877, acc 0.94
2016-05-21T03:08:52.199630: train-step 4644, loss 0.248055, acc 0.97
2016-05-21T03:08:58.467672: train-step 4645, loss 0.239185, acc 0.96
2016-05-21T03:09:05.091378: train-step 4646, loss 0.177147, acc 0.98
2016-05-21T03:09:11.399327: train-step 4647, loss 0.201038, acc 0.95
2016-05-21T03:09:17.725687: train-step 4648, loss 0.191233, acc 0.98
2016-05-21T03:09:24.093719: train-step 4649, loss 0.25092, acc 0.91
2016-05-21T03:09:30.319169: train-step 4650, loss 0.239004, acc 0.94
2016-05-21T03:09:36.711415: train-step 4651, loss 0.258599, acc 0.91
2016-05-21T03:09:43.260966: train-step 4652, loss 0.210357, acc 0.95
2016-05-21T03:09:49.628144: train-step 4653, loss 0.212922, acc 0.99
2016-05-21T03:09:55.934843: train-step 4654, loss 0.185373, acc 0.99
2016-05-21T03:10:02.121636: train-step 4655, loss 0.231932, acc 0.94
2016-05-21T03:10:08.727388: train-step 4656, loss 0.279438, acc 0.91
2016-05-21T03:10:15.070593: train-step 4657, loss 0.234971, acc 0.96
2016-05-21T03:10:21.301532: train-step 4658, loss 0.248587, acc 0.94
2016-05-21T03:10:27.522221: train-step 4659, loss 0.240402, acc 0.97
2016-05-21T03:10:34.006749: train-step 4660, loss 0.219127, acc 0.94
2016-05-21T03:10:40.401220: train-step 4661, loss 0.227022, acc 0.95
2016-05-21T03:10:46.639721: train-step 4662, loss 0.286295, acc 0.92
2016-05-21T03:10:52.843904: train-step 4663, loss 0.215057, acc 0.95
2016-05-21T03:10:59.194829: train-step 4664, loss 0.241927, acc 0.93
2016-05-21T03:11:05.486111: train-step 4665, loss 0.250978, acc 0.94
2016-05-21T03:11:11.677868: train-step 4666, loss 0.290348, acc 0.91
2016-05-21T03:11:17.866181: train-step 4667, loss 0.165559, acc 0.98
2016-05-21T03:11:24.094616: train-step 4668, loss 0.24645, acc 0.93
2016-05-21T03:11:30.279702: train-step 4669, loss 0.216549, acc 0.96
2016-05-21T03:11:36.629475: train-step 4670, loss 0.211535, acc 0.96
2016-05-21T03:11:43.159178: train-step 4671, loss 0.242717, acc 0.94
2016-05-21T03:11:49.402803: train-step 4672, loss 0.234916, acc 0.94
2016-05-21T03:11:55.618022: train-step 4673, loss 0.245059, acc 0.94
2016-05-21T03:12:01.858738: train-step 4674, loss 0.262864, acc 0.93
2016-05-21T03:12:08.450967: train-step 4675, loss 0.258443, acc 0.92
2016-05-21T03:12:14.763128: train-step 4676, loss 0.175671, acc 0.99
2016-05-21T03:12:20.926040: train-step 4677, loss 0.202988, acc 0.96
2016-05-21T03:12:27.146242: train-step 4678, loss 0.212256, acc 0.94
2016-05-21T03:12:33.354972: train-step 4679, loss 0.27192, acc 0.92
2016-05-21T03:12:39.721410: train-step 4680, loss 0.208281, acc 0.96
2016-05-21T03:12:46.041856: train-step 4681, loss 0.233826, acc 0.94
2016-05-21T03:12:52.394340: train-step 4682, loss 0.242968, acc 0.96
2016-05-21T03:12:58.711893: train-step 4683, loss 0.315821, acc 0.91
2016-05-21T03:13:04.939263: train-step 4684, loss 0.236011, acc 0.96
2016-05-21T03:13:11.142773: train-step 4685, loss 0.226015, acc 0.95
2016-05-21T03:13:17.541866: train-step 4686, loss 0.278212, acc 0.94
2016-05-21T03:13:23.951552: train-step 4687, loss 0.215457, acc 0.96
2016-05-21T03:13:30.286995: train-step 4688, loss 0.279688, acc 0.93
2016-05-21T03:13:36.564974: train-step 4689, loss 0.298189, acc 0.91
2016-05-21T03:13:42.733908: train-step 4690, loss 0.195935, acc 0.97
2016-05-21T03:13:49.002788: train-step 4691, loss 0.24623, acc 0.95
2016-05-21T03:13:55.386996: train-step 4692, loss 0.259476, acc 0.95
2016-05-21T03:14:01.639977: train-step 4693, loss 0.243042, acc 0.95
2016-05-21T03:14:07.857602: train-step 4694, loss 0.208008, acc 0.97
2016-05-21T03:14:14.057778: train-step 4695, loss 0.256785, acc 0.94
2016-05-21T03:14:20.262250: train-step 4696, loss 0.231323, acc 0.94
2016-05-21T03:14:26.485601: train-step 4697, loss 0.273256, acc 0.92
2016-05-21T03:14:32.729090: train-step 4698, loss 0.208708, acc 0.96
2016-05-21T03:14:38.897881: train-step 4699, loss 0.209032, acc 0.96
2016-05-21T03:14:45.119878: train-step 4700, loss 0.26792, acc 0.94
2016-05-21T03:14:51.379150: train-step 4701, loss 0.20824, acc 0.95
2016-05-21T03:14:58.001775: train-step 4702, loss 0.191936, acc 0.95
2016-05-21T03:15:04.243849: train-step 4703, loss 0.255325, acc 0.95
2016-05-21T03:15:10.446679: train-step 4704, loss 0.212379, acc 0.96
2016-05-21T03:15:16.679944: train-step 4705, loss 0.19763, acc 0.99
2016-05-21T03:15:23.027038: train-step 4706, loss 0.219833, acc 0.96
2016-05-21T03:15:29.230121: train-step 4707, loss 0.219702, acc 0.95
2016-05-21T03:15:35.363560: train-step 4708, loss 0.259578, acc 0.96
2016-05-21T03:15:41.583171: train-step 4709, loss 0.207828, acc 0.98
2016-05-21T03:15:47.944812: train-step 4710, loss 0.254988, acc 0.95
2016-05-21T03:15:54.325026: train-step 4711, loss 0.267493, acc 0.95
2016-05-21T03:16:00.612880: train-step 4712, loss 0.228461, acc 0.94
2016-05-21T03:16:06.907650: train-step 4713, loss 0.233024, acc 0.92
2016-05-21T03:16:13.124148: train-step 4714, loss 0.282709, acc 0.9
2016-05-21T03:16:19.645500: train-step 4715, loss 0.267003, acc 0.91
2016-05-21T03:16:26.028884: train-step 4716, loss 0.253992, acc 0.95
2016-05-21T03:16:32.423741: train-step 4717, loss 0.23052, acc 0.97
2016-05-21T03:16:38.699243: train-step 4718, loss 0.193896, acc 0.98
2016-05-21T03:16:44.927752: train-step 4719, loss 0.222017, acc 0.95
2016-05-21T03:16:51.154310: train-step 4720, loss 0.222889, acc 0.95
2016-05-21T03:16:57.362947: train-step 4721, loss 0.261179, acc 0.93
2016-05-21T03:17:03.905497: train-step 4722, loss 0.220795, acc 0.92
2016-05-21T03:17:10.279014: train-step 4723, loss 0.198546, acc 0.96
2016-05-21T03:17:16.689103: train-step 4724, loss 0.271973, acc 0.91
2016-05-21T03:17:22.907395: train-step 4725, loss 0.204133, acc 0.98
2016-05-21T03:17:29.286715: train-step 4726, loss 0.194759, acc 0.98
2016-05-21T03:17:35.831427: train-step 4727, loss 0.203454, acc 0.95
2016-05-21T03:17:42.034441: train-step 4728, loss 0.208109, acc 0.95
2016-05-21T03:17:48.267729: train-step 4729, loss 0.267634, acc 0.95
2016-05-21T03:17:54.447818: train-step 4730, loss 0.245654, acc 0.92
2016-05-21T03:18:00.975797: train-step 4731, loss 0.215621, acc 0.94
2016-05-21T03:18:07.330161: train-step 4732, loss 0.230736, acc 0.94
2016-05-21T03:18:13.507812: train-step 4733, loss 0.23169, acc 0.98
2016-05-21T03:18:19.753991: train-step 4734, loss 0.194809, acc 0.98
2016-05-21T03:18:25.997683: train-step 4735, loss 0.216058, acc 0.97
2016-05-21T03:18:32.261565: train-step 4736, loss 0.254016, acc 0.94
2016-05-21T03:18:38.515163: train-step 4737, loss 0.285669, acc 0.92
2016-05-21T03:18:45.062509: train-step 4738, loss 0.206998, acc 0.95
2016-05-21T03:18:51.412608: train-step 4739, loss 0.212244, acc 0.97
2016-05-21T03:18:57.644651: train-step 4740, loss 0.247989, acc 0.93
2016-05-21T03:19:04.204448: train-step 4741, loss 0.201518, acc 0.97
2016-05-21T03:19:10.519556: train-step 4742, loss 0.259145, acc 0.95
2016-05-21T03:19:16.869101: train-step 4743, loss 0.223255, acc 0.94
2016-05-21T03:19:23.103877: train-step 4744, loss 0.229714, acc 0.93
2016-05-21T03:19:29.322124: train-step 4745, loss 0.288848, acc 0.9
2016-05-21T03:19:35.683779: train-step 4746, loss 0.172037, acc 0.97
2016-05-21T03:19:41.924004: train-step 4747, loss 0.255324, acc 0.94
2016-05-21T03:19:48.184659: train-step 4748, loss 0.216103, acc 0.94
2016-05-21T03:19:54.408397: train-step 4749, loss 0.199758, acc 0.97
2016-05-21T03:20:00.647858: train-step 4750, loss 0.25566, acc 0.93
epoch number is: 19
2016-05-21T03:20:07.111857: train-step 4751, loss 0.234931, acc 0.92
2016-05-21T03:20:13.351467: train-step 4752, loss 0.204677, acc 0.96
2016-05-21T03:20:19.913008: train-step 4753, loss 0.208892, acc 0.95
2016-05-21T03:20:26.295839: train-step 4754, loss 0.250962, acc 0.94
2016-05-21T03:20:32.514833: train-step 4755, loss 0.21532, acc 0.97
2016-05-21T03:20:38.796574: train-step 4756, loss 0.305414, acc 0.91
2016-05-21T03:20:45.020466: train-step 4757, loss 0.223725, acc 0.96
2016-05-21T03:20:51.174733: train-step 4758, loss 0.209191, acc 0.96
2016-05-21T03:20:57.371010: train-step 4759, loss 0.236294, acc 0.93
2016-05-21T03:21:03.582101: train-step 4760, loss 0.215496, acc 0.96
2016-05-21T03:21:09.844483: train-step 4761, loss 0.205383, acc 0.95
2016-05-21T03:21:16.193498: train-step 4762, loss 0.270708, acc 0.95
2016-05-21T03:21:22.358902: train-step 4763, loss 0.216165, acc 0.95
2016-05-21T03:21:28.522205: train-step 4764, loss 0.243352, acc 0.94
2016-05-21T03:21:35.270957: train-step 4765, loss 0.227569, acc 0.97
2016-05-21T03:21:41.851123: train-step 4766, loss 0.225387, acc 0.95
2016-05-21T03:21:48.180937: train-step 4767, loss 0.250832, acc 0.93
2016-05-21T03:21:54.394890: train-step 4768, loss 0.217095, acc 0.96
2016-05-21T03:22:00.948135: train-step 4769, loss 0.175582, acc 0.97
2016-05-21T03:22:07.281026: train-step 4770, loss 0.254961, acc 0.94
2016-05-21T03:22:13.691287: train-step 4771, loss 0.212189, acc 0.97
2016-05-21T03:22:20.016269: train-step 4772, loss 0.224438, acc 0.95
2016-05-21T03:22:26.209305: train-step 4773, loss 0.228144, acc 0.95
2016-05-21T03:22:32.471030: train-step 4774, loss 0.274008, acc 0.94
2016-05-21T03:22:38.672516: train-step 4775, loss 0.210465, acc 0.96
2016-05-21T03:22:45.250464: train-step 4776, loss 0.229241, acc 0.95
2016-05-21T03:22:51.639081: train-step 4777, loss 0.179515, acc 0.98
2016-05-21T03:22:57.870345: train-step 4778, loss 0.221964, acc 0.97
2016-05-21T03:23:04.426020: train-step 4779, loss 0.233136, acc 0.98
2016-05-21T03:23:10.757325: train-step 4780, loss 0.240488, acc 0.96
2016-05-21T03:23:17.100964: train-step 4781, loss 0.18508, acc 0.99
2016-05-21T03:23:23.339623: train-step 4782, loss 0.208711, acc 0.96
2016-05-21T03:23:29.547053: train-step 4783, loss 0.228958, acc 0.96
2016-05-21T03:23:35.751091: train-step 4784, loss 0.279233, acc 0.92
2016-05-21T03:23:42.305726: train-step 4785, loss 0.269475, acc 0.93
2016-05-21T03:23:48.575664: train-step 4786, loss 0.199668, acc 0.99
2016-05-21T03:23:54.807754: train-step 4787, loss 0.233036, acc 0.93
2016-05-21T03:24:00.964855: train-step 4788, loss 0.20278, acc 0.98
2016-05-21T03:24:07.152131: train-step 4789, loss 0.176382, acc 0.97
2016-05-21T03:24:13.333963: train-step 4790, loss 0.226235, acc 0.95
2016-05-21T03:24:19.484677: train-step 4791, loss 0.215681, acc 0.96
2016-05-21T03:24:25.681877: train-step 4792, loss 0.170636, acc 0.99
2016-05-21T03:24:31.925309: train-step 4793, loss 0.206782, acc 0.95
2016-05-21T03:24:38.143322: train-step 4794, loss 0.250099, acc 0.9
2016-05-21T03:24:44.251346: train-step 4795, loss 0.24744, acc 0.93
2016-05-21T03:24:50.434967: train-step 4796, loss 0.192602, acc 0.97
2016-05-21T03:24:56.645650: train-step 4797, loss 0.253661, acc 0.95
2016-05-21T03:25:02.878506: train-step 4798, loss 0.255964, acc 0.96
2016-05-21T03:25:10.073195: train-step 4799, loss 0.205225, acc 0.95
2016-05-21T03:25:16.753818: train-step 4800, loss 0.238382, acc 0.95
2016-05-21T03:25:23.123281: train-step 4801, loss 0.19604, acc 0.96
2016-05-21T03:25:29.437108: train-step 4802, loss 0.215907, acc 0.95
2016-05-21T03:25:35.806194: train-step 4803, loss 0.202586, acc 0.97
2016-05-21T03:25:42.014462: train-step 4804, loss 0.237682, acc 0.95
2016-05-21T03:25:48.284081: train-step 4805, loss 0.268262, acc 0.95
2016-05-21T03:25:54.520097: train-step 4806, loss 0.251647, acc 0.98
2016-05-21T03:26:00.773385: train-step 4807, loss 0.21005, acc 0.96
2016-05-21T03:26:07.017817: train-step 4808, loss 0.268452, acc 0.87
2016-05-21T03:26:13.340971: train-step 4809, loss 0.266541, acc 0.96
2016-05-21T03:26:19.880179: train-step 4810, loss 0.29463, acc 0.88
2016-05-21T03:26:26.128821: train-step 4811, loss 0.210645, acc 0.95
2016-05-21T03:26:32.439474: train-step 4812, loss 0.317345, acc 0.9
2016-05-21T03:26:39.034178: train-step 4813, loss 0.211945, acc 0.94
2016-05-21T03:26:45.357520: train-step 4814, loss 0.325987, acc 0.91
2016-05-21T03:26:51.563428: train-step 4815, loss 0.307805, acc 0.89
2016-05-21T03:26:57.787079: train-step 4816, loss 0.226193, acc 0.95
2016-05-21T03:27:03.968502: train-step 4817, loss 0.270636, acc 0.94
2016-05-21T03:27:10.229097: train-step 4818, loss 0.195145, acc 0.97
2016-05-21T03:27:16.454786: train-step 4819, loss 0.231838, acc 0.95
2016-05-21T03:27:22.825563: train-step 4820, loss 0.222275, acc 0.92
2016-05-21T03:27:29.059136: train-step 4821, loss 0.243136, acc 0.96
2016-05-21T03:27:35.592682: train-step 4822, loss 0.236973, acc 0.94
2016-05-21T03:27:42.009386: train-step 4823, loss 0.217626, acc 0.95
2016-05-21T03:27:48.383259: train-step 4824, loss 0.30002, acc 0.92
2016-05-21T03:27:54.631381: train-step 4825, loss 0.175144, acc 1
2016-05-21T03:28:00.884353: train-step 4826, loss 0.225591, acc 0.95
2016-05-21T03:28:07.082583: train-step 4827, loss 0.205296, acc 0.97
2016-05-21T03:28:13.270839: train-step 4828, loss 0.189039, acc 0.96
2016-05-21T03:28:19.623924: train-step 4829, loss 0.227734, acc 0.95
2016-05-21T03:28:25.943447: train-step 4830, loss 0.230472, acc 0.93
2016-05-21T03:28:32.265414: train-step 4831, loss 0.257198, acc 0.92
2016-05-21T03:28:38.481185: train-step 4832, loss 0.230329, acc 0.97
2016-05-21T03:28:44.702062: train-step 4833, loss 0.235352, acc 0.94
2016-05-21T03:28:50.867361: train-step 4834, loss 0.211052, acc 0.96
2016-05-21T03:28:57.096954: train-step 4835, loss 0.20925, acc 0.98
2016-05-21T03:29:03.456816: train-step 4836, loss 0.231273, acc 0.93
2016-05-21T03:29:09.640642: train-step 4837, loss 0.254684, acc 0.93
2016-05-21T03:29:16.176503: train-step 4838, loss 0.226893, acc 0.96
2016-05-21T03:29:22.547478: train-step 4839, loss 0.222778, acc 0.97
2016-05-21T03:29:28.966629: train-step 4840, loss 0.229477, acc 0.95
2016-05-21T03:29:35.352025: train-step 4841, loss 0.237961, acc 0.91
2016-05-21T03:29:41.585659: train-step 4842, loss 0.227943, acc 0.94
2016-05-21T03:29:47.766764: train-step 4843, loss 0.242531, acc 0.93
2016-05-21T03:29:53.948928: train-step 4844, loss 0.195812, acc 0.99
2016-05-21T03:30:00.205358: train-step 4845, loss 0.215155, acc 0.94
2016-05-21T03:30:06.399329: train-step 4846, loss 0.287212, acc 0.93
2016-05-21T03:30:12.580487: train-step 4847, loss 0.204711, acc 0.96
2016-05-21T03:30:18.787446: train-step 4848, loss 0.231223, acc 0.93
2016-05-21T03:30:25.149567: train-step 4849, loss 0.232104, acc 0.93
2016-05-21T03:30:31.402853: train-step 4850, loss 0.213385, acc 0.97
2016-05-21T03:30:37.669340: train-step 4851, loss 0.268947, acc 0.94
2016-05-21T03:30:43.907150: train-step 4852, loss 0.240224, acc 0.94
2016-05-21T03:30:50.100097: train-step 4853, loss 0.236225, acc 0.95
2016-05-21T03:30:56.330844: train-step 4854, loss 0.207006, acc 0.96
2016-05-21T03:31:02.898276: train-step 4855, loss 0.233651, acc 0.92
2016-05-21T03:31:09.257882: train-step 4856, loss 0.23447, acc 0.93
2016-05-21T03:31:15.626061: train-step 4857, loss 0.231185, acc 0.95
2016-05-21T03:31:22.009170: train-step 4858, loss 0.205997, acc 0.96
2016-05-21T03:31:28.376222: train-step 4859, loss 0.18105, acc 1
2016-05-21T03:31:34.655429: train-step 4860, loss 0.181923, acc 0.96
2016-05-21T03:31:40.988189: train-step 4861, loss 0.180253, acc 0.98
2016-05-21T03:31:47.356468: train-step 4862, loss 0.226416, acc 0.95
2016-05-21T03:31:53.720015: train-step 4863, loss 0.210563, acc 0.95
2016-05-21T03:31:59.937782: train-step 4864, loss 0.302363, acc 0.9
2016-05-21T03:32:06.154046: train-step 4865, loss 0.27201, acc 0.94
2016-05-21T03:32:12.348281: train-step 4866, loss 0.212788, acc 0.96
2016-05-21T03:32:18.556127: train-step 4867, loss 0.219509, acc 0.97
2016-05-21T03:32:24.775820: train-step 4868, loss 0.207703, acc 0.96
2016-05-21T03:32:30.974997: train-step 4869, loss 0.218397, acc 0.96
2016-05-21T03:32:37.200904: train-step 4870, loss 0.220208, acc 0.96
2016-05-21T03:32:43.358180: train-step 4871, loss 0.2343, acc 0.94
2016-05-21T03:32:49.693021: train-step 4872, loss 0.238694, acc 0.97
2016-05-21T03:32:55.838128: train-step 4873, loss 0.192633, acc 0.99
2016-05-21T03:33:02.058571: train-step 4874, loss 0.222905, acc 0.95
2016-05-21T03:33:08.257996: train-step 4875, loss 0.247668, acc 0.92
2016-05-21T03:33:14.484480: train-step 4876, loss 0.271857, acc 0.91
2016-05-21T03:33:21.024942: train-step 4877, loss 0.282407, acc 0.95
2016-05-21T03:33:27.399977: train-step 4878, loss 0.299743, acc 0.88
2016-05-21T03:33:33.776540: train-step 4879, loss 0.213743, acc 0.96
2016-05-21T03:33:39.995166: train-step 4880, loss 0.204241, acc 0.97
2016-05-21T03:33:46.207911: train-step 4881, loss 0.215279, acc 0.96
2016-05-21T03:33:52.501322: train-step 4882, loss 0.252599, acc 0.94
2016-05-21T03:33:58.686529: train-step 4883, loss 0.212336, acc 0.97
2016-05-21T03:34:04.875668: train-step 4884, loss 0.261429, acc 0.94
2016-05-21T03:34:11.081874: train-step 4885, loss 0.265743, acc 0.91
2016-05-21T03:34:17.322863: train-step 4886, loss 0.193774, acc 0.96
2016-05-21T03:34:23.703904: train-step 4887, loss 0.229446, acc 0.95
2016-05-21T03:34:30.090547: train-step 4888, loss 0.264232, acc 0.93
2016-05-21T03:34:36.478607: train-step 4889, loss 0.193907, acc 0.99
2016-05-21T03:34:42.680915: train-step 4890, loss 0.24813, acc 0.91
2016-05-21T03:34:48.973875: train-step 4891, loss 0.220789, acc 0.95
2016-05-21T03:34:55.580495: train-step 4892, loss 0.214827, acc 0.97
2016-05-21T03:35:01.904181: train-step 4893, loss 0.251094, acc 0.91
2016-05-21T03:35:08.151041: train-step 4894, loss 0.226879, acc 0.98
2016-05-21T03:35:14.351587: train-step 4895, loss 0.211387, acc 0.96
2016-05-21T03:35:20.541493: train-step 4896, loss 0.235478, acc 0.94
2016-05-21T03:35:26.716489: train-step 4897, loss 0.231623, acc 0.96
2016-05-21T03:35:32.876920: train-step 4898, loss 0.209969, acc 0.98
2016-05-21T03:35:39.073519: train-step 4899, loss 0.196942, acc 0.96
2016-05-21T03:35:45.300436: train-step 4900, loss 0.245534, acc 0.94
2016-05-21T03:35:51.545628: train-step 4901, loss 0.236335, acc 0.95
2016-05-21T03:35:57.736449: train-step 4902, loss 0.224702, acc 0.94
2016-05-21T03:36:03.923123: train-step 4903, loss 0.220602, acc 0.93
2016-05-21T03:36:10.230860: train-step 4904, loss 0.200295, acc 0.96
2016-05-21T03:36:16.575790: train-step 4905, loss 0.248597, acc 0.96
2016-05-21T03:36:22.718427: train-step 4906, loss 0.195924, acc 0.95
2016-05-21T03:36:28.945275: train-step 4907, loss 0.189905, acc 0.97
2016-05-21T03:36:35.293947: train-step 4908, loss 0.213412, acc 0.94
2016-05-21T03:36:41.706473: train-step 4909, loss 0.210444, acc 0.93
2016-05-21T03:36:48.004254: train-step 4910, loss 0.213868, acc 0.95
2016-05-21T03:36:54.194497: train-step 4911, loss 0.191361, acc 0.98
2016-05-21T03:37:00.432192: train-step 4912, loss 0.200857, acc 0.97
2016-05-21T03:37:06.642300: train-step 4913, loss 0.266075, acc 0.9
2016-05-21T03:37:12.850166: train-step 4914, loss 0.237063, acc 0.94
2016-05-21T03:37:19.244567: train-step 4915, loss 0.225772, acc 0.96
2016-05-21T03:37:25.654222: train-step 4916, loss 0.208088, acc 0.96
2016-05-21T03:37:31.869823: train-step 4917, loss 0.184907, acc 0.97
2016-05-21T03:37:38.200895: train-step 4918, loss 0.185215, acc 0.98
2016-05-21T03:37:44.656616: train-step 4919, loss 0.22268, acc 0.91
2016-05-21T03:37:50.889848: train-step 4920, loss 0.237046, acc 0.96
2016-05-21T03:37:57.183098: train-step 4921, loss 0.237087, acc 0.94
2016-05-21T03:38:03.365370: train-step 4922, loss 0.305709, acc 0.92
2016-05-21T03:38:09.881025: train-step 4923, loss 0.175162, acc 0.98
2016-05-21T03:38:16.236472: train-step 4924, loss 0.243861, acc 0.95
2016-05-21T03:38:22.495536: train-step 4925, loss 0.218972, acc 0.94
2016-05-21T03:38:29.014840: train-step 4926, loss 0.211779, acc 0.98
2016-05-21T03:38:35.376775: train-step 4927, loss 0.223111, acc 0.95
2016-05-21T03:38:41.610311: train-step 4928, loss 0.19963, acc 0.99
2016-05-21T03:38:48.068442: train-step 4929, loss 0.237009, acc 0.93
2016-05-21T03:38:54.469928: train-step 4930, loss 0.189755, acc 0.97
2016-05-21T03:39:00.853216: train-step 4931, loss 0.22799, acc 0.95
2016-05-21T03:39:07.191900: train-step 4932, loss 0.178777, acc 0.98
2016-05-21T03:39:13.537036: train-step 4933, loss 0.227351, acc 0.93
2016-05-21T03:39:19.886150: train-step 4934, loss 0.207792, acc 0.95
2016-05-21T03:39:26.240957: train-step 4935, loss 0.214653, acc 0.96
2016-05-21T03:39:32.496797: train-step 4936, loss 0.204362, acc 0.94
2016-05-21T03:39:38.713081: train-step 4937, loss 0.195567, acc 0.96
2016-05-21T03:39:45.017446: train-step 4938, loss 0.205422, acc 0.95
2016-05-21T03:39:51.381251: train-step 4939, loss 0.292095, acc 0.92
2016-05-21T03:39:57.657685: train-step 4940, loss 0.205572, acc 0.97
2016-05-21T03:40:03.902178: train-step 4941, loss 0.236972, acc 0.93
2016-05-21T03:40:10.503004: train-step 4942, loss 0.205281, acc 0.97
2016-05-21T03:40:16.848205: train-step 4943, loss 0.205431, acc 0.95
2016-05-21T03:40:23.098919: train-step 4944, loss 0.238986, acc 0.94
2016-05-21T03:40:29.664440: train-step 4945, loss 0.233291, acc 0.94
2016-05-21T03:40:36.258533: train-step 4946, loss 0.183925, acc 0.96
2016-05-21T03:40:42.651517: train-step 4947, loss 0.205978, acc 0.97
2016-05-21T03:40:49.023519: train-step 4948, loss 0.227871, acc 0.93
2016-05-21T03:40:55.354742: train-step 4949, loss 0.224654, acc 0.94
2016-05-21T03:41:01.705112: train-step 4950, loss 0.184562, acc 0.98
2016-05-21T03:41:08.029938: train-step 4951, loss 0.162415, acc 0.98
2016-05-21T03:41:14.416652: train-step 4952, loss 0.188978, acc 0.96
2016-05-21T03:41:20.639913: train-step 4953, loss 0.251509, acc 0.93
2016-05-21T03:41:26.843964: train-step 4954, loss 0.260662, acc 0.92
2016-05-21T03:41:33.203061: train-step 4955, loss 0.231995, acc 0.95
2016-05-21T03:41:39.524318: train-step 4956, loss 0.213368, acc 0.96
2016-05-21T03:41:45.846819: train-step 4957, loss 0.178275, acc 0.98
2016-05-21T03:41:52.024556: train-step 4958, loss 0.213178, acc 0.97
2016-05-21T03:41:58.289156: train-step 4959, loss 0.213949, acc 0.96
2016-05-21T03:42:04.656755: train-step 4960, loss 0.257358, acc 0.93
2016-05-21T03:42:10.838279: train-step 4961, loss 0.252125, acc 0.95
2016-05-21T03:42:17.118155: train-step 4962, loss 0.217753, acc 0.96
2016-05-21T03:42:23.560955: train-step 4963, loss 0.260987, acc 0.93
2016-05-21T03:42:30.012333: train-step 4964, loss 0.232888, acc 0.93
2016-05-21T03:42:36.266859: train-step 4965, loss 0.239975, acc 0.94
2016-05-21T03:42:42.531800: train-step 4966, loss 0.272294, acc 0.93
2016-05-21T03:42:48.752962: train-step 4967, loss 0.228776, acc 0.96
2016-05-21T03:42:54.918326: train-step 4968, loss 0.210042, acc 0.93
2016-05-21T03:43:01.404221: train-step 4969, loss 0.265637, acc 0.94
2016-05-21T03:43:07.753138: train-step 4970, loss 0.197371, acc 0.95
2016-05-21T03:43:14.087576: train-step 4971, loss 0.175208, acc 0.97
2016-05-21T03:43:20.267290: train-step 4972, loss 0.238697, acc 0.95
2016-05-21T03:43:26.525709: train-step 4973, loss 0.293771, acc 0.91
2016-05-21T03:43:32.764576: train-step 4974, loss 0.235598, acc 0.94
2016-05-21T03:43:38.916487: train-step 4975, loss 0.233464, acc 0.95
2016-05-21T03:43:45.120567: train-step 4976, loss 0.233965, acc 0.94
2016-05-21T03:43:51.505913: train-step 4977, loss 0.204557, acc 0.93
2016-05-21T03:43:57.723966: train-step 4978, loss 0.2591, acc 0.89
2016-05-21T03:44:03.995238: train-step 4979, loss 0.243819, acc 0.93
2016-05-21T03:44:10.249021: train-step 4980, loss 0.204685, acc 0.96
2016-05-21T03:44:16.451301: train-step 4981, loss 0.253967, acc 0.91
2016-05-21T03:44:22.670198: train-step 4982, loss 0.229512, acc 0.95
2016-05-21T03:44:28.860731: train-step 4983, loss 0.225654, acc 0.95
2016-05-21T03:44:35.137730: train-step 4984, loss 0.224147, acc 0.93
2016-05-21T03:44:41.678150: train-step 4985, loss 0.187895, acc 0.96
2016-05-21T03:44:48.093033: train-step 4986, loss 0.171711, acc 0.95
2016-05-21T03:44:54.432940: train-step 4987, loss 0.236641, acc 0.95
2016-05-21T03:45:00.694300: train-step 4988, loss 0.226434, acc 0.94
2016-05-21T03:45:06.905133: train-step 4989, loss 0.227106, acc 0.95
2016-05-21T03:45:13.211456: train-step 4990, loss 0.294454, acc 0.88
2016-05-21T03:45:19.580373: train-step 4991, loss 0.309643, acc 0.89
2016-05-21T03:45:25.832939: train-step 4992, loss 0.188022, acc 0.98
2016-05-21T03:45:32.014040: train-step 4993, loss 0.260229, acc 0.9
2016-05-21T03:45:38.207500: train-step 4994, loss 0.202772, acc 0.97
2016-05-21T03:45:44.514957: train-step 4995, loss 0.248854, acc 0.91
2016-05-21T03:45:50.679566: train-step 4996, loss 0.203224, acc 0.96
2016-05-21T03:45:56.838416: train-step 4997, loss 0.210207, acc 0.94
2016-05-21T03:46:03.007926: train-step 4998, loss 0.265794, acc 0.96
2016-05-21T03:46:09.336977: train-step 4999, loss 0.226391, acc 0.98
2016-05-21T03:46:15.598319: train-step 5000, loss 0.263819, acc 0.92
2016-05-21T03:46:18.498320  dev-step: 1  acc: 0.9
2016-05-21T03:46:21.070942  dev-step: 2  acc: 0.95
2016-05-21T03:46:23.543616  dev-step: 3  acc: 0.95
2016-05-21T03:46:26.079342  dev-step: 4  acc: 0.93
2016-05-21T03:46:28.643279  dev-step: 5  acc: 0.93
2016-05-21T03:46:31.158229  dev-step: 6  acc: 0.94
2016-05-21T03:46:33.692576  dev-step: 7  acc: 0.92
2016-05-21T03:46:36.219374  dev-step: 8  acc: 0.93
2016-05-21T03:46:38.878075  dev-step: 9  acc: 0.95
2016-05-21T03:46:41.597408  dev-step: 10  acc: 0.97
2016-05-21T03:46:44.223662  dev-step: 11  acc: 0.92
2016-05-21T03:46:46.793256  dev-step: 12  acc: 0.97
2016-05-21T03:46:49.345935  dev-step: 13  acc: 0.92
2016-05-21T03:46:51.864823  dev-step: 14  acc: 0.87
2016-05-21T03:46:54.404502  dev-step: 15  acc: 0.9
2016-05-21T03:46:56.927251  dev-step: 16  acc: 0.92
2016-05-21T03:46:59.448096  dev-step: 17  acc: 0.89
2016-05-21T03:47:01.986699  dev-step: 18  acc: 0.94
2016-05-21T03:47:04.521188  dev-step: 19  acc: 0.91
2016-05-21T03:47:07.038178  dev-step: 20  acc: 0.91
2016-05-21T03:47:09.528500  dev-step: 21  acc: 0.94
2016-05-21T03:47:12.159170  dev-step: 22  acc: 0.91
2016-05-21T03:47:14.745171  dev-step: 23  acc: 0.92
2016-05-21T03:47:17.302647  dev-step: 24  acc: 0.94
2016-05-21T03:47:19.788624  dev-step: 25  acc: 0.86
2016-05-21T03:47:22.328368  dev-step: 26  acc: 0.92
2016-05-21T03:47:24.884799  dev-step: 27  acc: 0.83
2016-05-21T03:47:27.366746  dev-step: 28  acc: 0.91
2016-05-21T03:47:29.851906  dev-step: 29  acc: 0.95
2016-05-21T03:47:32.388851  dev-step: 30  acc: 0.89
2016-05-21T03:47:35.041764  dev-step: 31  acc: 0.9
2016-05-21T03:47:37.716310  dev-step: 32  acc: 0.85
2016-05-21T03:47:40.314086  dev-step: 33  acc: 0.96
2016-05-21T03:47:42.900216  dev-step: 34  acc: 0.92
2016-05-21T03:47:45.455420  dev-step: 35  acc: 0.94
2016-05-21T03:47:48.052251  dev-step: 36  acc: 0.91
2016-05-21T03:47:50.596839  dev-step: 37  acc: 0.93
2016-05-21T03:47:53.166894  dev-step: 38  acc: 0.89
2016-05-21T03:47:55.697748  dev-step: 39  acc: 0.94
2016-05-21T03:47:58.182053  dev-step: 40  acc: 0.96
2016-05-21T03:48:00.731879  dev-step: 41  acc: 0.95
2016-05-21T03:48:03.466957  dev-step: 42  acc: 0.93
2016-05-21T03:48:06.162861  dev-step: 43  acc: 0.87
2016-05-21T03:48:08.799658  dev-step: 44  acc: 0.89
2016-05-21T03:48:11.305512  dev-step: 45  acc: 0.91
2016-05-21T03:48:13.846085  dev-step: 46  acc: 0.84
2016-05-21T03:48:16.474101  dev-step: 47  acc: 0.85
2016-05-21T03:48:19.000386  dev-step: 48  acc: 0.93
2016-05-21T03:48:21.566049  dev-step: 49  acc: 0.97
2016-05-21T03:48:24.082685  dev-step: 50  acc: 0.91
2016-05-21T03:48:26.624971  dev-step: 51  acc: 0.95
2016-05-21T03:48:29.188246  dev-step: 52  acc: 0.92
2016-05-21T03:48:31.655441  dev-step: 53  acc: 0.92
2016-05-21T03:48:34.195599  dev-step: 54  acc: 0.98
2016-05-21T03:48:36.694959  dev-step: 55  acc: 0.89
2016-05-21T03:48:39.365908  dev-step: 56  acc: 0.88
2016-05-21T03:48:42.079110  dev-step: 57  acc: 0.89
2016-05-21T03:48:44.717806  dev-step: 58  acc: 0.91
2016-05-21T03:48:47.255878  dev-step: 59  acc: 0.94
2016-05-21T03:48:49.806338  dev-step: 60  acc: 0.91
2016-05-21T03:48:52.346200  dev-step: 61  acc: 0.9
2016-05-21T03:48:54.890730  dev-step: 62  acc: 0.88
2016-05-21T03:48:57.407206  dev-step: 63  acc: 0.88
2016-05-21T03:49:00.142505  dev-step: 64  acc: 0.97
2016-05-21T03:49:02.760748  dev-step: 65  acc: 0.83
2016-05-21T03:49:05.337273  dev-step: 66  acc: 0.87
2016-05-21T03:49:07.897742  dev-step: 67  acc: 0.93
2016-05-21T03:49:10.394577  dev-step: 68  acc: 0.93
2016-05-21T03:49:12.917878  dev-step: 69  acc: 0.96
2016-05-21T03:49:15.441817  dev-step: 70  acc: 0.96
2016-05-21T03:49:17.969370  dev-step: 71  acc: 0.95
2016-05-21T03:49:20.495335  dev-step: 72  acc: 0.95
2016-05-21T03:49:23.102278  dev-step: 73  acc: 0.94
2016-05-21T03:49:25.723220  dev-step: 74  acc: 0.92
2016-05-21T03:49:28.272468  dev-step: 75  acc: 0.95
2016-05-21T03:49:30.868888  dev-step: 76  acc: 0.92
2016-05-21T03:49:33.494270  dev-step: 77  acc: 0.91
2016-05-21T03:49:36.005811  dev-step: 78  acc: 0.91
2016-05-21T03:49:38.530805  dev-step: 79  acc: 0.92
2016-05-21T03:49:41.051086  dev-step: 80  acc: 0.93
2016-05-21T03:49:43.573146  dev-step: 81  acc: 0.95
2016-05-21T03:49:46.096439  dev-step: 82  acc: 0.9
2016-05-21T03:49:48.569445  dev-step: 83  acc: 0.92
2016-05-21T03:49:51.172789  dev-step: 84  acc: 0.9
2016-05-21T03:49:53.747188  dev-step: 85  acc: 0.94
2016-05-21T03:49:56.320429  dev-step: 86  acc: 0.95
2016-05-21T03:49:58.871583  dev-step: 87  acc: 0.89
2016-05-21T03:50:01.505743  dev-step: 88  acc: 0.95
2016-05-21T03:50:04.082907  dev-step: 89  acc: 0.87
2016-05-21T03:50:06.561495  dev-step: 90  acc: 0.93
2016-05-21T03:50:09.088278  dev-step: 91  acc: 0.85
2016-05-21T03:50:11.616215  dev-step: 92  acc: 0.93
2016-05-21T03:50:14.137889  dev-step: 93  acc: 0.9
2016-05-21T03:50:16.650059  dev-step: 94  acc: 0.96
2016-05-21T03:50:19.246714  dev-step: 95  acc: 0.98
2016-05-21T03:50:21.867428  dev-step: 96  acc: 0.92
2016-05-21T03:50:24.373010  dev-step: 97  acc: 0.85
2016-05-21T03:50:26.945430  dev-step: 98  acc: 0.88
2016-05-21T03:50:29.555219  dev-step: 99  acc: 0.88
2016-05-21T03:50:32.061670  dev-step: 100  acc: 0.97
2016-05-21T03:50:34.625383  dev-step: 101  acc: 0.96
2016-05-21T03:50:37.214716  dev-step: 102  acc: 0.94
2016-05-21T03:50:39.723715  dev-step: 103  acc: 0.95
2016-05-21T03:50:42.237727  dev-step: 104  acc: 0.88
2016-05-21T03:50:44.805994  dev-step: 105  acc: 0.93
2016-05-21T03:50:47.372990  dev-step: 106  acc: 0.95
2016-05-21T03:50:49.940937  dev-step: 107  acc: 0.91
2016-05-21T03:50:52.533331  dev-step: 108  acc: 0.94
2016-05-21T03:50:55.121813  dev-step: 109  acc: 0.85
2016-05-21T03:50:57.660088  dev-step: 110  acc: 0.86
2016-05-21T03:51:00.185549  dev-step: 111  acc: 0.92
2016-05-21T03:51:02.730480  dev-step: 112  acc: 0.91
2016-05-21T03:51:05.244932  dev-step: 113  acc: 0.94
2016-05-21T03:51:07.777046  dev-step: 114  acc: 0.9
2016-05-21T03:51:10.313083  dev-step: 115  acc: 0.89
2016-05-21T03:51:12.838908  dev-step: 116  acc: 0.95
2016-05-21T03:51:15.553460  dev-step: 117  acc: 0.92
2016-05-21T03:51:18.252552  dev-step: 118  acc: 0.91
2016-05-21T03:51:20.885795  dev-step: 119  acc: 0.88
2016-05-21T03:51:23.456291  dev-step: 120  acc: 0.88
2016-05-21T03:51:25.997164  dev-step: 121  acc: 0.89
2016-05-21T03:51:28.514642  dev-step: 122  acc: 0.83
2016-05-21T03:51:31.053349  dev-step: 123  acc: 0.93
2016-05-21T03:51:33.613012  dev-step: 124  acc: 0.91
2016-05-21T03:51:36.095574  dev-step: 125  acc: 0.96
2016-05-21T03:51:38.628184  dev-step: 126  acc: 0.91
2016-05-21T03:51:41.189065  dev-step: 127  acc: 0.91
2016-05-21T03:51:43.711329  dev-step: 128  acc: 0.9
2016-05-21T03:51:46.202773  dev-step: 129  acc: 0.86
2016-05-21T03:51:48.768496  dev-step: 130  acc: 0.89
2016-05-21T03:51:51.481174  dev-step: 131  acc: 0.95
2016-05-21T03:51:54.189387  dev-step: 132  acc: 0.96
2016-05-21T03:51:56.815428  dev-step: 133  acc: 0.95
2016-05-21T03:51:59.384526  dev-step: 134  acc: 0.85
2016-05-21T03:52:01.915231  dev-step: 135  acc: 0.96
2016-05-21T03:52:04.517280  dev-step: 136  acc: 0.92
2016-05-21T03:52:07.055498  dev-step: 137  acc: 0.95
2016-05-21T03:52:09.623890  dev-step: 138  acc: 0.93
2016-05-21T03:52:12.196300  dev-step: 139  acc: 0.86
2016-05-21T03:52:14.797107  dev-step: 140  acc: 0.83
2016-05-21T03:52:17.366188  dev-step: 141  acc: 0.88
2016-05-21T03:52:19.841030  dev-step: 142  acc: 0.86
2016-05-21T03:52:22.327195  dev-step: 143  acc: 0.92
2016-05-21T03:52:24.835784  dev-step: 144  acc: 0.89
2016-05-21T03:52:27.563738  dev-step: 145  acc: 0.84
2016-05-21T03:52:30.271000  dev-step: 146  acc: 0.96
2016-05-21T03:52:32.911841  dev-step: 147  acc: 0.88
2016-05-21T03:52:35.639865  dev-step: 148  acc: 0.9
2016-05-21T03:52:38.296173  dev-step: 149  acc: 0.83
2016-05-21T03:52:40.869424  dev-step: 150  acc: 0.93
2016-05-21T03:52:43.520182  dev-step: 151  acc: 0.9
2016-05-21T03:52:46.185861  dev-step: 152  acc: 0.86
2016-05-21T03:52:48.827602  dev-step: 153  acc: 0.92
2016-05-21T03:52:51.500199  dev-step: 154  acc: 0.9
2016-05-21T03:52:54.220462  dev-step: 155  acc: 0.9
2016-05-21T03:52:56.858695  dev-step: 156  acc: 0.85
2016-05-21T03:52:59.584348  dev-step: 157  acc: 0.91
2016-05-21T03:53:02.288164  dev-step: 158  acc: 0.9
2016-05-21T03:53:04.907961  dev-step: 159  acc: 0.9
2016-05-21T03:53:07.483556  dev-step: 160  acc: 0.9
2016-05-21T03:53:10.012537  dev-step: 161  acc: 0.95
2016-05-21T03:53:12.647056  dev-step: 162  acc: 0.95
2016-05-21T03:53:15.231597  dev-step: 163  acc: 0.93
2016-05-21T03:53:17.759747  dev-step: 164  acc: 0.87
2016-05-21T03:53:20.387784  dev-step: 165  acc: 0.91
2016-05-21T03:53:22.926105  dev-step: 166  acc: 0.83
2016-05-21T03:53:25.491680  dev-step: 167  acc: 0.86
2016-05-21T03:53:28.005056  dev-step: 168  acc: 0.91
2016-05-21T03:53:30.536904  dev-step: 169  acc: 0.88
2016-05-21T03:53:33.066642  dev-step: 170  acc: 0.92
2016-05-21T03:53:35.575439  dev-step: 171  acc: 0.91
2016-05-21T03:53:38.111093  dev-step: 172  acc: 0.9
2016-05-21T03:53:40.621258  dev-step: 173  acc: 0.95
2016-05-21T03:53:43.302609  dev-step: 174  acc: 0.95
2016-05-21T03:53:46.016296  dev-step: 175  acc: 0.9
2016-05-21T03:53:48.653798  dev-step: 176  acc: 0.84
2016-05-21T03:53:51.179348  dev-step: 177  acc: 0.91
2016-05-21T03:53:53.737881  dev-step: 178  acc: 0.89
2016-05-21T03:53:56.200180  dev-step: 179  acc: 0.84
2016-05-21T03:53:58.756834  dev-step: 180  acc: 0.88
2016-05-21T03:54:01.267136  dev-step: 181  acc: 0.88
2016-05-21T03:54:03.779929  dev-step: 182  acc: 0.96
2016-05-21T03:54:06.283548  dev-step: 183  acc: 0.93
2016-05-21T03:54:08.828984  dev-step: 184  acc: 0.93
2016-05-21T03:54:11.488656  dev-step: 185  acc: 0.95
2016-05-21T03:54:14.213417  dev-step: 186  acc: 0.85
2016-05-21T03:54:16.782660  dev-step: 187  acc: 0.91
2016-05-21T03:54:19.347753  dev-step: 188  acc: 0.88
2016-05-21T03:54:21.856774  dev-step: 189  acc: 0.88
2016-05-21T03:54:24.372469  dev-step: 190  acc: 0.9
2016-05-21T03:54:26.916212  dev-step: 191  acc: 0.9
2016-05-21T03:54:29.496881  dev-step: 192  acc: 0.94
2016-05-21T03:54:32.016336  dev-step: 193  acc: 0.87
2016-05-21T03:54:34.549552  dev-step: 194  acc: 0.94
2016-05-21T03:54:37.113576  dev-step: 195  acc: 0.94
2016-05-21T03:54:39.784526  dev-step: 196  acc: 0.91
2016-05-21T03:54:42.446193  dev-step: 197  acc: 0.9
2016-05-21T03:54:45.024381  dev-step: 198  acc: 0.94
2016-05-21T03:54:47.550417  dev-step: 199  acc: 0.9
2016-05-21T03:54:50.062213  dev-step: 200  acc: 0.88
2016-05-21T03:54:52.554882  dev-step: 201  acc: 0.89
2016-05-21T03:54:55.110152  dev-step: 202  acc: 0.94
2016-05-21T03:54:57.646053  dev-step: 203  acc: 0.93
2016-05-21T03:55:00.401191  dev-step: 204  acc: 0.89
2016-05-21T03:55:03.031891  dev-step: 205  acc: 0.88
2016-05-21T03:55:05.619419  dev-step: 206  acc: 0.91
2016-05-21T03:55:08.186324  dev-step: 207  acc: 0.86
2016-05-21T03:55:10.782584  dev-step: 208  acc: 0.91
2016-05-21T03:55:13.420897  dev-step: 209  acc: 0.89
2016-05-21T03:55:15.926075  dev-step: 210  acc: 0.9
2016-05-21T03:55:18.511052  dev-step: 211  acc: 0.89
2016-05-21T03:55:21.103893  dev-step: 212  acc: 0.9
2016-05-21T03:55:23.962837  dev-step: 213  acc: 0.91
2016-05-21T03:55:26.779082  dev-step: 214  acc: 0.85
2016-05-21T03:55:29.473083  dev-step: 215  acc: 0.84
2016-05-21T03:55:32.062890  dev-step: 216  acc: 0.88
2016-05-21T03:55:34.666053  dev-step: 217  acc: 0.89
2016-05-21T03:55:37.186120  dev-step: 218  acc: 0.79
2016-05-21T03:55:39.773760  dev-step: 219  acc: 0.85
2016-05-21T03:55:42.342364  dev-step: 220  acc: 0.91
2016-05-21T03:55:44.861697  dev-step: 221  acc: 0.9
2016-05-21T03:55:47.443036  dev-step: 222  acc: 0.93
2016-05-21T03:55:50.078001  dev-step: 223  acc: 0.94
2016-05-21T03:55:52.635287  dev-step: 224  acc: 0.84
2016-05-21T03:55:55.217555  dev-step: 225  acc: 0.93
2016-05-21T03:55:57.846390  dev-step: 226  acc: 0.88
2016-05-21T03:56:00.348958  dev-step: 227  acc: 0.91
2016-05-21T03:56:02.943979  dev-step: 228  acc: 0.86
2016-05-21T03:56:05.566094  dev-step: 229  acc: 0.92
2016-05-21T03:56:08.083536  dev-step: 230  acc: 0.89
2016-05-21T03:56:10.628277  dev-step: 231  acc: 0.9
2016-05-21T03:56:13.151521  dev-step: 232  acc: 0.87
2016-05-21T03:56:15.668812  dev-step: 233  acc: 0.89
2016-05-21T03:56:18.179790  dev-step: 234  acc: 0.94
2016-05-21T03:56:20.659285  dev-step: 235  acc: 0.93
2016-05-21T03:56:23.241802  dev-step: 236  acc: 0.87
2016-05-21T03:56:25.868968  dev-step: 237  acc: 0.86
2016-05-21T03:56:28.438470  dev-step: 238  acc: 0.92
2016-05-21T03:56:31.036255  dev-step: 239  acc: 0.93
2016-05-21T03:56:33.679301  dev-step: 240  acc: 0.83
2016-05-21T03:56:36.184298  dev-step: 241  acc: 0.85
2016-05-21T03:56:38.784430  dev-step: 242  acc: 0.88
2016-05-21T03:56:41.417135  dev-step: 243  acc: 0.86
2016-05-21T03:56:43.924497  dev-step: 244  acc: 0.88
2016-05-21T03:56:46.506289  dev-step: 245  acc: 0.82
2016-05-21T03:56:49.141033  dev-step: 246  acc: 0.89
2016-05-21T03:56:51.945254  dev-step: 247  acc: 0.94
2016-05-21T03:56:54.736055  dev-step: 248  acc: 0.89
2016-05-21T03:56:57.451381  dev-step: 249  acc: 0.82
2016-05-21T03:57:00.095910  dev-step: 250  acc: 0.86
avg_loss 0.297155, avg_acc 0.90608, real_acc 0.90608
epoch number is: 20
2016-05-21T03:57:06.511653: train-step 5001, loss 0.188989, acc 0.98
2016-05-21T03:57:12.858826: train-step 5002, loss 0.173602, acc 0.98
2016-05-21T03:57:19.213021: train-step 5003, loss 0.226385, acc 0.95
2016-05-21T03:57:25.446119: train-step 5004, loss 0.237013, acc 0.94
2016-05-21T03:57:31.654381: train-step 5005, loss 0.191605, acc 0.96
2016-05-21T03:57:38.025874: train-step 5006, loss 0.251773, acc 0.96
2016-05-21T03:57:44.246874: train-step 5007, loss 0.323147, acc 0.91
2016-05-21T03:57:50.463305: train-step 5008, loss 0.186242, acc 0.97
2016-05-21T03:57:56.664745: train-step 5009, loss 0.209461, acc 0.94
2016-05-21T03:58:02.883635: train-step 5010, loss 0.197236, acc 0.97
2016-05-21T03:58:09.205616: train-step 5011, loss 0.275901, acc 0.94
2016-05-21T03:58:15.340188: train-step 5012, loss 0.232929, acc 0.94
2016-05-21T03:58:21.574046: train-step 5013, loss 0.253844, acc 0.93
2016-05-21T03:58:27.763800: train-step 5014, loss 0.23018, acc 0.95
2016-05-21T03:58:34.088272: train-step 5015, loss 0.206589, acc 0.96
2016-05-21T03:58:40.357724: train-step 5016, loss 0.234388, acc 0.94
2016-05-21T03:58:46.686487: train-step 5017, loss 0.246299, acc 0.93
2016-05-21T03:58:53.191687: train-step 5018, loss 0.245219, acc 0.94
2016-05-21T03:58:59.809157: train-step 5019, loss 0.210254, acc 0.95
2016-05-21T03:59:06.174240: train-step 5020, loss 0.306985, acc 0.88
2016-05-21T03:59:12.562375: train-step 5021, loss 0.209395, acc 0.98
2016-05-21T03:59:18.968782: train-step 5022, loss 0.245497, acc 0.96
2016-05-21T03:59:25.213281: train-step 5023, loss 0.213817, acc 0.96
2016-05-21T03:59:31.600255: train-step 5024, loss 0.236378, acc 0.94
2016-05-21T03:59:38.171569: train-step 5025, loss 0.295582, acc 0.92
2016-05-21T03:59:44.396597: train-step 5026, loss 0.212247, acc 0.93
2016-05-21T03:59:50.729960: train-step 5027, loss 0.246603, acc 0.96
2016-05-21T03:59:57.292491: train-step 5028, loss 0.251408, acc 0.92
2016-05-21T04:00:03.879391: train-step 5029, loss 0.22975, acc 0.95
2016-05-21T04:00:10.154584: train-step 5030, loss 0.21062, acc 0.97
2016-05-21T04:00:16.388731: train-step 5031, loss 0.213913, acc 0.94
2016-05-21T04:00:22.931415: train-step 5032, loss 0.208671, acc 0.93
2016-05-21T04:00:29.310080: train-step 5033, loss 0.25154, acc 0.95
2016-05-21T04:00:35.690094: train-step 5034, loss 0.224298, acc 0.94
2016-05-21T04:00:41.926667: train-step 5035, loss 0.19236, acc 0.97
2016-05-21T04:00:48.191832: train-step 5036, loss 0.239978, acc 0.95
2016-05-21T04:00:54.481853: train-step 5037, loss 0.2147, acc 0.97
2016-05-21T04:01:01.109093: train-step 5038, loss 0.212712, acc 0.97
2016-05-21T04:01:07.390959: train-step 5039, loss 0.203315, acc 0.96
2016-05-21T04:01:13.596945: train-step 5040, loss 0.183657, acc 0.96
2016-05-21T04:01:19.846016: train-step 5041, loss 0.25741, acc 0.91
2016-05-21T04:01:26.140976: train-step 5042, loss 0.24554, acc 0.93
2016-05-21T04:01:32.397922: train-step 5043, loss 0.240225, acc 0.92
2016-05-21T04:01:38.932483: train-step 5044, loss 0.223455, acc 0.94
2016-05-21T04:01:45.302971: train-step 5045, loss 0.240052, acc 0.96
2016-05-21T04:01:51.669245: train-step 5046, loss 0.222963, acc 0.95
2016-05-21T04:01:57.924184: train-step 5047, loss 0.207307, acc 0.96
2016-05-21T04:02:04.118037: train-step 5048, loss 0.180104, acc 0.96
2016-05-21T04:02:10.483620: train-step 5049, loss 0.243161, acc 0.9
2016-05-21T04:02:16.837056: train-step 5050, loss 0.205993, acc 0.98
2016-05-21T04:02:23.089478: train-step 5051, loss 0.19629, acc 0.99
2016-05-21T04:02:29.298955: train-step 5052, loss 0.201822, acc 0.96
2016-05-21T04:02:35.558132: train-step 5053, loss 0.192423, acc 0.98
2016-05-21T04:02:41.864526: train-step 5054, loss 0.239191, acc 0.93
2016-05-21T04:02:48.119666: train-step 5055, loss 0.158787, acc 0.98
2016-05-21T04:02:54.687824: train-step 5056, loss 0.215129, acc 0.95
2016-05-21T04:03:01.038251: train-step 5057, loss 0.198443, acc 0.98
2016-05-21T04:03:07.457863: train-step 5058, loss 0.20525, acc 0.96
2016-05-21T04:03:13.736953: train-step 5059, loss 0.212584, acc 0.95
2016-05-21T04:03:19.940739: train-step 5060, loss 0.198256, acc 0.98
2016-05-21T04:03:26.246831: train-step 5061, loss 0.208229, acc 0.96
2016-05-21T04:03:32.505944: train-step 5062, loss 0.159096, acc 1
2016-05-21T04:03:38.753361: train-step 5063, loss 0.217538, acc 0.96
2016-05-21T04:03:44.994635: train-step 5064, loss 0.19055, acc 0.98
2016-05-21T04:03:51.567064: train-step 5065, loss 0.208183, acc 0.96
2016-05-21T04:03:57.863683: train-step 5066, loss 0.200283, acc 0.97
2016-05-21T04:04:04.070775: train-step 5067, loss 0.281258, acc 0.93
2016-05-21T04:04:10.267498: train-step 5068, loss 0.244241, acc 0.93
2016-05-21T04:04:16.526034: train-step 5069, loss 0.178722, acc 0.99
2016-05-21T04:04:22.801864: train-step 5070, loss 0.177642, acc 0.97
2016-05-21T04:04:29.055191: train-step 5071, loss 0.237562, acc 0.95
2016-05-21T04:04:35.597009: train-step 5072, loss 0.21666, acc 0.95
2016-05-21T04:04:41.923762: train-step 5073, loss 0.21714, acc 0.94
2016-05-21T04:04:48.145770: train-step 5074, loss 0.180868, acc 0.98
2016-05-21T04:04:54.672767: train-step 5075, loss 0.220042, acc 0.97
2016-05-21T04:05:01.000330: train-step 5076, loss 0.168162, acc 0.99
2016-05-21T04:05:07.394392: train-step 5077, loss 0.219586, acc 0.96
2016-05-21T04:05:13.612296: train-step 5078, loss 0.264365, acc 0.96
2016-05-21T04:05:19.951274: train-step 5079, loss 0.203546, acc 0.96
2016-05-21T04:05:26.477491: train-step 5080, loss 0.175947, acc 0.98
2016-05-21T04:05:32.753851: train-step 5081, loss 0.21625, acc 0.94
2016-05-21T04:05:39.096927: train-step 5082, loss 0.213282, acc 0.95
2016-05-21T04:05:45.879397: train-step 5083, loss 0.187673, acc 0.98
2016-05-21T04:05:52.518443: train-step 5084, loss 0.220144, acc 0.94
2016-05-21T04:05:58.878914: train-step 5085, loss 0.20135, acc 0.95
2016-05-21T04:06:05.222843: train-step 5086, loss 0.240832, acc 0.93
2016-05-21T04:06:11.609651: train-step 5087, loss 0.215736, acc 0.97
2016-05-21T04:06:17.861212: train-step 5088, loss 0.244026, acc 0.93
2016-05-21T04:06:24.284855: train-step 5089, loss 0.203634, acc 0.96
2016-05-21T04:06:30.827852: train-step 5090, loss 0.202812, acc 0.98
2016-05-21T04:06:37.060166: train-step 5091, loss 0.200478, acc 0.97
2016-05-21T04:06:43.430715: train-step 5092, loss 0.181022, acc 0.98
2016-05-21T04:06:49.988110: train-step 5093, loss 0.211746, acc 0.93
2016-05-21T04:06:56.209336: train-step 5094, loss 0.218503, acc 0.96
2016-05-21T04:07:02.457150: train-step 5095, loss 0.266557, acc 0.92
2016-05-21T04:07:09.071104: train-step 5096, loss 0.266865, acc 0.93
2016-05-21T04:07:15.414982: train-step 5097, loss 0.215466, acc 0.97
2016-05-21T04:07:21.765189: train-step 5098, loss 0.160941, acc 0.99
2016-05-21T04:07:28.005324: train-step 5099, loss 0.179827, acc 0.99
2016-05-21T04:07:34.247739: train-step 5100, loss 0.267675, acc 0.91
2016-05-21T04:07:40.477116: train-step 5101, loss 0.244618, acc 0.93
2016-05-21T04:07:46.822590: train-step 5102, loss 0.191783, acc 0.95
2016-05-21T04:07:53.129871: train-step 5103, loss 0.26615, acc 0.92
2016-05-21T04:07:59.469591: train-step 5104, loss 0.207023, acc 0.97
2016-05-21T04:08:05.780849: train-step 5105, loss 0.175148, acc 0.97
2016-05-21T04:08:12.133239: train-step 5106, loss 0.184714, acc 1
2016-05-21T04:08:18.517865: train-step 5107, loss 0.224453, acc 0.95
2016-05-21T04:08:24.854881: train-step 5108, loss 0.20382, acc 0.98
2016-05-21T04:08:31.109884: train-step 5109, loss 0.19888, acc 0.95
2016-05-21T04:08:37.295494: train-step 5110, loss 0.222945, acc 0.95
2016-05-21T04:08:43.872637: train-step 5111, loss 0.216657, acc 0.95
2016-05-21T04:08:50.184729: train-step 5112, loss 0.221001, acc 0.94
2016-05-21T04:08:56.427087: train-step 5113, loss 0.214173, acc 0.97
2016-05-21T04:09:03.002712: train-step 5114, loss 0.182631, acc 0.97
2016-05-21T04:09:09.306705: train-step 5115, loss 0.250239, acc 0.95
2016-05-21T04:09:15.687568: train-step 5116, loss 0.259141, acc 0.93
2016-05-21T04:09:21.927955: train-step 5117, loss 0.182607, acc 0.97
2016-05-21T04:09:28.172684: train-step 5118, loss 0.187891, acc 0.97
2016-05-21T04:09:34.431243: train-step 5119, loss 0.23512, acc 0.93
2016-05-21T04:09:40.981878: train-step 5120, loss 0.187087, acc 0.98
2016-05-21T04:09:47.247427: train-step 5121, loss 0.247935, acc 0.95
2016-05-21T04:09:53.508885: train-step 5122, loss 0.275716, acc 0.93
2016-05-21T04:10:00.062658: train-step 5123, loss 0.241606, acc 0.94
2016-05-21T04:10:06.423694: train-step 5124, loss 0.196828, acc 0.95
2016-05-21T04:10:12.646977: train-step 5125, loss 0.259451, acc 0.94
2016-05-21T04:10:19.160143: train-step 5126, loss 0.206694, acc 0.96
2016-05-21T04:10:25.462885: train-step 5127, loss 0.277606, acc 0.93
2016-05-21T04:10:31.847246: train-step 5128, loss 0.284962, acc 0.93
2016-05-21T04:10:38.069043: train-step 5129, loss 0.205105, acc 0.96
2016-05-21T04:10:44.415757: train-step 5130, loss 0.216396, acc 0.94
2016-05-21T04:10:50.944417: train-step 5131, loss 0.179637, acc 0.99
2016-05-21T04:10:57.299146: train-step 5132, loss 0.169821, acc 0.98
2016-05-21T04:11:03.632390: train-step 5133, loss 0.279447, acc 0.92
2016-05-21T04:11:09.994632: train-step 5134, loss 0.225482, acc 0.95
2016-05-21T04:11:16.366916: train-step 5135, loss 0.17982, acc 0.98
2016-05-21T04:11:22.754600: train-step 5136, loss 0.281988, acc 0.93
2016-05-21T04:11:29.002198: train-step 5137, loss 0.228674, acc 0.96
2016-05-21T04:11:35.272416: train-step 5138, loss 0.229298, acc 0.98
2016-05-21T04:11:41.859461: train-step 5139, loss 0.193909, acc 0.96
2016-05-21T04:11:48.150438: train-step 5140, loss 0.210306, acc 0.94
2016-05-21T04:11:54.504515: train-step 5141, loss 0.192065, acc 0.98
2016-05-21T04:12:00.915248: train-step 5142, loss 0.205727, acc 0.95
2016-05-21T04:12:07.267433: train-step 5143, loss 0.230291, acc 0.93
2016-05-21T04:12:13.680706: train-step 5144, loss 0.187678, acc 0.97
2016-05-21T04:12:20.063570: train-step 5145, loss 0.30721, acc 0.89
2016-05-21T04:12:26.432708: train-step 5146, loss 0.321238, acc 0.9
2016-05-21T04:12:32.688453: train-step 5147, loss 0.197005, acc 0.98
2016-05-21T04:12:39.252914: train-step 5148, loss 0.176374, acc 0.99
2016-05-21T04:12:45.614705: train-step 5149, loss 0.208144, acc 0.95
2016-05-21T04:12:51.975539: train-step 5150, loss 0.196275, acc 0.96
2016-05-21T04:12:58.229195: train-step 5151, loss 0.27265, acc 0.95
2016-05-21T04:13:04.427991: train-step 5152, loss 0.208536, acc 0.98
2016-05-21T04:13:10.713177: train-step 5153, loss 0.250898, acc 0.92
2016-05-21T04:13:16.937198: train-step 5154, loss 0.213568, acc 0.95
2016-05-21T04:13:23.189066: train-step 5155, loss 0.19573, acc 0.95
2016-05-21T04:13:29.417311: train-step 5156, loss 0.252809, acc 0.94
2016-05-21T04:13:35.989823: train-step 5157, loss 0.238727, acc 0.95
2016-05-21T04:13:42.304275: train-step 5158, loss 0.197336, acc 0.96
2016-05-21T04:13:48.522944: train-step 5159, loss 0.206085, acc 0.96
2016-05-21T04:13:55.038300: train-step 5160, loss 0.212517, acc 0.96
2016-05-21T04:14:01.400304: train-step 5161, loss 0.216994, acc 0.97
2016-05-21T04:14:07.641709: train-step 5162, loss 0.211148, acc 0.96
2016-05-21T04:14:14.090202: train-step 5163, loss 0.239645, acc 0.92
2016-05-21T04:14:20.428003: train-step 5164, loss 0.220497, acc 0.97
2016-05-21T04:14:26.788257: train-step 5165, loss 0.267881, acc 0.96
2016-05-21T04:14:33.135444: train-step 5166, loss 0.196031, acc 0.94
2016-05-21T04:14:39.443675: train-step 5167, loss 0.210797, acc 0.97
2016-05-21T04:14:45.679401: train-step 5168, loss 0.225792, acc 0.95
2016-05-21T04:14:52.266608: train-step 5169, loss 0.234707, acc 0.94
2016-05-21T04:14:58.611318: train-step 5170, loss 0.208297, acc 0.96
2016-05-21T04:15:04.808871: train-step 5171, loss 0.17772, acc 1
2016-05-21T04:15:11.265062: train-step 5172, loss 0.227115, acc 0.95
2016-05-21T04:15:17.568293: train-step 5173, loss 0.196115, acc 0.97
2016-05-21T04:15:23.750802: train-step 5174, loss 0.26245, acc 0.89
2016-05-21T04:15:30.225481: train-step 5175, loss 0.190639, acc 0.97
2016-05-21T04:15:37.433510: train-step 5176, loss 0.242714, acc 0.94
2016-05-21T04:15:44.097720: train-step 5177, loss 0.211104, acc 0.94
2016-05-21T04:15:50.451838: train-step 5178, loss 0.22902, acc 0.98
2016-05-21T04:15:56.637483: train-step 5179, loss 0.272725, acc 0.93
2016-05-21T04:16:02.798612: train-step 5180, loss 0.198491, acc 0.96
2016-05-21T04:16:09.093050: train-step 5181, loss 0.273815, acc 0.91
2016-05-21T04:16:15.465395: train-step 5182, loss 0.195518, acc 0.96
2016-05-21T04:16:21.727150: train-step 5183, loss 0.268598, acc 0.88
2016-05-21T04:16:27.976027: train-step 5184, loss 0.191412, acc 0.97
2016-05-21T04:16:34.232842: train-step 5185, loss 0.189768, acc 0.96
2016-05-21T04:16:40.785042: train-step 5186, loss 0.322961, acc 0.87
2016-05-21T04:16:47.136217: train-step 5187, loss 0.229539, acc 0.95
2016-05-21T04:16:53.375980: train-step 5188, loss 0.220532, acc 0.94
2016-05-21T04:16:59.943005: train-step 5189, loss 0.266942, acc 0.92
2016-05-21T04:17:06.307916: train-step 5190, loss 0.207632, acc 0.98
2016-05-21T04:17:12.689316: train-step 5191, loss 0.218528, acc 0.96
2016-05-21T04:17:19.035059: train-step 5192, loss 0.197827, acc 0.98
2016-05-21T04:17:25.282682: train-step 5193, loss 0.212368, acc 0.96
2016-05-21T04:17:31.578420: train-step 5194, loss 0.226233, acc 0.97
2016-05-21T04:17:38.168473: train-step 5195, loss 0.262711, acc 0.91
2016-05-21T04:17:44.769551: train-step 5196, loss 0.15215, acc 1
2016-05-21T04:17:51.097676: train-step 5197, loss 0.260284, acc 0.94
2016-05-21T04:17:57.326451: train-step 5198, loss 0.214285, acc 0.96
2016-05-21T04:18:03.856049: train-step 5199, loss 0.162037, acc 0.99
2016-05-21T04:18:10.202280: train-step 5200, loss 0.185392, acc 0.98
2016-05-21T04:18:16.553171: train-step 5201, loss 0.243489, acc 0.95
2016-05-21T04:18:22.818325: train-step 5202, loss 0.193368, acc 0.99
2016-05-21T04:18:29.029057: train-step 5203, loss 0.18536, acc 0.96
2016-05-21T04:18:35.334813: train-step 5204, loss 0.232182, acc 0.97
2016-05-21T04:18:41.652618: train-step 5205, loss 0.205574, acc 0.97
2016-05-21T04:18:47.998590: train-step 5206, loss 0.200316, acc 0.96
2016-05-21T04:18:54.313201: train-step 5207, loss 0.21949, acc 0.95
2016-05-21T04:19:00.697904: train-step 5208, loss 0.315965, acc 0.9
2016-05-21T04:19:06.890690: train-step 5209, loss 0.228127, acc 0.93
2016-05-21T04:19:13.038634: train-step 5210, loss 0.242332, acc 0.95
2016-05-21T04:19:19.404406: train-step 5211, loss 0.192644, acc 0.97
2016-05-21T04:19:25.607573: train-step 5212, loss 0.31737, acc 0.9
2016-05-21T04:19:31.890293: train-step 5213, loss 0.191228, acc 0.95
2016-05-21T04:19:38.396268: train-step 5214, loss 0.262246, acc 0.92
2016-05-21T04:19:45.017464: train-step 5215, loss 0.162328, acc 0.99
2016-05-21T04:19:51.321354: train-step 5216, loss 0.219114, acc 0.94
2016-05-21T04:19:57.532237: train-step 5217, loss 0.221573, acc 0.94
2016-05-21T04:20:04.105085: train-step 5218, loss 0.237434, acc 0.95
2016-05-21T04:20:10.400238: train-step 5219, loss 0.256334, acc 0.94
2016-05-21T04:20:16.787092: train-step 5220, loss 0.198116, acc 0.98
2016-05-21T04:20:23.122789: train-step 5221, loss 0.187627, acc 0.97
2016-05-21T04:20:29.383039: train-step 5222, loss 0.226372, acc 0.96
2016-05-21T04:20:35.657881: train-step 5223, loss 0.214058, acc 0.95
2016-05-21T04:20:41.809033: train-step 5224, loss 0.2171, acc 0.95
2016-05-21T04:20:48.028336: train-step 5225, loss 0.249325, acc 0.94
2016-05-21T04:20:54.223004: train-step 5226, loss 0.254188, acc 0.91
2016-05-21T04:21:00.592444: train-step 5227, loss 0.208145, acc 0.99
2016-05-21T04:21:06.841602: train-step 5228, loss 0.162369, acc 0.98
2016-05-21T04:21:13.038872: train-step 5229, loss 0.194271, acc 0.95
2016-05-21T04:21:19.410768: train-step 5230, loss 0.19554, acc 0.96
2016-05-21T04:21:25.734453: train-step 5231, loss 0.218606, acc 0.95
2016-05-21T04:21:32.004877: train-step 5232, loss 0.196505, acc 0.97
2016-05-21T04:21:38.200292: train-step 5233, loss 0.19111, acc 0.97
2016-05-21T04:21:44.738289: train-step 5234, loss 0.244449, acc 0.92
2016-05-21T04:21:51.095481: train-step 5235, loss 0.21778, acc 0.96
2016-05-21T04:21:57.338732: train-step 5236, loss 0.188204, acc 0.95
2016-05-21T04:22:03.855027: train-step 5237, loss 0.186646, acc 0.97
2016-05-21T04:22:10.218245: train-step 5238, loss 0.234884, acc 0.93
2016-05-21T04:22:16.603480: train-step 5239, loss 0.248325, acc 0.94
2016-05-21T04:22:22.861266: train-step 5240, loss 0.182449, acc 0.95
2016-05-21T04:22:29.049373: train-step 5241, loss 0.185133, acc 0.97
2016-05-21T04:22:35.376947: train-step 5242, loss 0.263366, acc 0.93
2016-05-21T04:22:41.677155: train-step 5243, loss 0.302034, acc 0.88
2016-05-21T04:22:47.944007: train-step 5244, loss 0.259497, acc 0.92
2016-05-21T04:22:54.268074: train-step 5245, loss 0.218378, acc 0.94
2016-05-21T04:23:00.631892: train-step 5246, loss 0.237639, acc 0.94
2016-05-21T04:23:06.842526: train-step 5247, loss 0.180872, acc 0.97
2016-05-21T04:23:13.043000: train-step 5248, loss 0.192585, acc 0.97
2016-05-21T04:23:19.249711: train-step 5249, loss 0.220467, acc 0.94
2016-05-21T04:23:25.475453: train-step 5250, loss 0.198827, acc 0.96
epoch number is: 21
2016-05-21T04:23:32.114815: train-step 5251, loss 0.201875, acc 0.97
2016-05-21T04:23:38.460345: train-step 5252, loss 0.196629, acc 0.95
2016-05-21T04:23:44.859852: train-step 5253, loss 0.218872, acc 0.95
2016-05-21T04:23:51.243456: train-step 5254, loss 0.213093, acc 0.95
2016-05-21T04:23:57.593350: train-step 5255, loss 0.204433, acc 0.97
2016-05-21T04:24:03.968720: train-step 5256, loss 0.208239, acc 0.98
2016-05-21T04:24:10.292753: train-step 5257, loss 0.218928, acc 0.94
2016-05-21T04:24:16.688790: train-step 5258, loss 0.218942, acc 0.97
2016-05-21T04:24:22.896997: train-step 5259, loss 0.197421, acc 0.97
2016-05-21T04:24:29.315291: train-step 5260, loss 0.237853, acc 0.96
2016-05-21T04:24:35.767735: train-step 5261, loss 0.196127, acc 0.97
2016-05-21T04:24:42.004611: train-step 5262, loss 0.201801, acc 0.97
2016-05-21T04:24:48.342112: train-step 5263, loss 0.211975, acc 0.96
2016-05-21T04:24:54.935280: train-step 5264, loss 0.229528, acc 0.95
2016-05-21T04:25:01.239747: train-step 5265, loss 0.243617, acc 0.95
2016-05-21T04:25:07.603147: train-step 5266, loss 0.227196, acc 0.95
2016-05-21T04:25:13.967217: train-step 5267, loss 0.207217, acc 0.98
2016-05-21T04:25:20.193604: train-step 5268, loss 0.186885, acc 0.99
2016-05-21T04:25:26.690842: train-step 5269, loss 0.194989, acc 0.97
2016-05-21T04:25:33.078925: train-step 5270, loss 0.193662, acc 0.97
2016-05-21T04:25:39.480574: train-step 5271, loss 0.24211, acc 0.96
2016-05-21T04:25:45.685356: train-step 5272, loss 0.186214, acc 0.97
2016-05-21T04:25:51.973196: train-step 5273, loss 0.243737, acc 0.95
2016-05-21T04:25:58.564369: train-step 5274, loss 0.207742, acc 0.97
2016-05-21T04:26:04.780559: train-step 5275, loss 0.199256, acc 0.95
2016-05-21T04:26:11.000129: train-step 5276, loss 0.221124, acc 0.97
2016-05-21T04:26:17.183116: train-step 5277, loss 0.220809, acc 0.97
2016-05-21T04:26:23.396550: train-step 5278, loss 0.250632, acc 0.93
2016-05-21T04:26:29.706952: train-step 5279, loss 0.227921, acc 0.92
2016-05-21T04:26:35.946977: train-step 5280, loss 0.206319, acc 0.96
2016-05-21T04:26:42.422674: train-step 5281, loss 0.220287, acc 0.97
2016-05-21T04:26:48.827669: train-step 5282, loss 0.179399, acc 0.97
2016-05-21T04:26:55.076227: train-step 5283, loss 0.260631, acc 0.89
2016-05-21T04:27:01.458229: train-step 5284, loss 0.186506, acc 0.98
2016-05-21T04:27:07.973783: train-step 5285, loss 0.204296, acc 0.96
2016-05-21T04:27:14.330973: train-step 5286, loss 0.196539, acc 0.96
2016-05-21T04:27:20.631803: train-step 5287, loss 0.250567, acc 0.95
2016-05-21T04:27:26.849665: train-step 5288, loss 0.193209, acc 0.97
2016-05-21T04:27:33.454486: train-step 5289, loss 0.221819, acc 0.94
2016-05-21T04:27:39.770771: train-step 5290, loss 0.191477, acc 0.97
2016-05-21T04:27:46.075301: train-step 5291, loss 0.205553, acc 0.97
2016-05-21T04:27:52.458379: train-step 5292, loss 0.204079, acc 0.97
2016-05-21T04:27:58.670660: train-step 5293, loss 0.211279, acc 0.96
2016-05-21T04:28:04.919457: train-step 5294, loss 0.241818, acc 0.92
2016-05-21T04:28:11.093239: train-step 5295, loss 0.184319, acc 0.96
2016-05-21T04:28:17.311548: train-step 5296, loss 0.206477, acc 0.96
2016-05-21T04:28:23.516214: train-step 5297, loss 0.181716, acc 0.98
2016-05-21T04:28:29.728083: train-step 5298, loss 0.233662, acc 0.93
2016-05-21T04:28:36.050155: train-step 5299, loss 0.202247, acc 0.96
2016-05-21T04:28:42.320795: train-step 5300, loss 0.229979, acc 0.96
2016-05-21T04:28:48.671545: train-step 5301, loss 0.256495, acc 0.91
2016-05-21T04:28:55.229278: train-step 5302, loss 0.187335, acc 0.98
2016-05-21T04:29:01.543836: train-step 5303, loss 0.223763, acc 0.95
2016-05-21T04:29:07.912040: train-step 5304, loss 0.196455, acc 0.96
2016-05-21T04:29:14.302676: train-step 5305, loss 0.216173, acc 0.96
2016-05-21T04:29:20.676944: train-step 5306, loss 0.231454, acc 0.94
2016-05-21T04:29:26.985100: train-step 5307, loss 0.1792, acc 0.98
2016-05-21T04:29:33.879273: train-step 5308, loss 0.157973, acc 0.99
2016-05-21T04:29:40.413503: train-step 5309, loss 0.194742, acc 0.97
2016-05-21T04:29:46.793961: train-step 5310, loss 0.193595, acc 0.96
2016-05-21T04:29:53.044836: train-step 5311, loss 0.220448, acc 0.96
2016-05-21T04:29:59.232940: train-step 5312, loss 0.239676, acc 0.94
2016-05-21T04:30:05.453310: train-step 5313, loss 0.223899, acc 0.93
2016-05-21T04:30:11.707904: train-step 5314, loss 0.21854, acc 0.94
2016-05-21T04:30:17.899436: train-step 5315, loss 0.23698, acc 0.94
2016-05-21T04:30:24.093031: train-step 5316, loss 0.213562, acc 0.97
2016-05-21T04:30:30.277644: train-step 5317, loss 0.172118, acc 0.97
2016-05-21T04:30:36.587800: train-step 5318, loss 0.183776, acc 0.98
2016-05-21T04:30:42.806364: train-step 5319, loss 0.222739, acc 0.93
2016-05-21T04:30:49.051085: train-step 5320, loss 0.256807, acc 0.93
2016-05-21T04:30:55.264258: train-step 5321, loss 0.199292, acc 0.96
2016-05-21T04:31:01.550197: train-step 5322, loss 0.199237, acc 0.97
2016-05-21T04:31:07.885999: train-step 5323, loss 0.286304, acc 0.89
2016-05-21T04:31:14.102246: train-step 5324, loss 0.208148, acc 0.96
2016-05-21T04:31:20.661474: train-step 5325, loss 0.209725, acc 0.94
2016-05-21T04:31:27.033458: train-step 5326, loss 0.171949, acc 0.96
2016-05-21T04:31:33.434277: train-step 5327, loss 0.273536, acc 0.95
2016-05-21T04:31:39.608842: train-step 5328, loss 0.250528, acc 0.94
2016-05-21T04:31:45.844434: train-step 5329, loss 0.191175, acc 0.96
2016-05-21T04:31:52.015651: train-step 5330, loss 0.190157, acc 0.97
2016-05-21T04:31:58.233390: train-step 5331, loss 0.197469, acc 0.96
2016-05-21T04:32:04.485360: train-step 5332, loss 0.220877, acc 0.94
2016-05-21T04:32:10.743529: train-step 5333, loss 0.204418, acc 0.97
2016-05-21T04:32:17.260272: train-step 5334, loss 0.189855, acc 0.97
2016-05-21T04:32:23.615894: train-step 5335, loss 0.257098, acc 0.92
2016-05-21T04:32:29.842531: train-step 5336, loss 0.215915, acc 0.95
2016-05-21T04:32:36.401953: train-step 5337, loss 0.247965, acc 0.93
2016-05-21T04:32:42.773331: train-step 5338, loss 0.187126, acc 0.97
2016-05-21T04:32:49.146510: train-step 5339, loss 0.24425, acc 0.95
2016-05-21T04:32:55.349654: train-step 5340, loss 0.156008, acc 1
2016-05-21T04:33:01.709631: train-step 5341, loss 0.28938, acc 0.93
2016-05-21T04:33:08.269901: train-step 5342, loss 0.241491, acc 0.93
2016-05-21T04:33:14.512636: train-step 5343, loss 0.277914, acc 0.9
2016-05-21T04:33:20.760156: train-step 5344, loss 0.222689, acc 0.96
2016-05-21T04:33:26.962200: train-step 5345, loss 0.224011, acc 0.94
2016-05-21T04:33:33.235113: train-step 5346, loss 0.202973, acc 0.99
2016-05-21T04:33:39.543190: train-step 5347, loss 0.215741, acc 0.94
2016-05-21T04:33:45.709828: train-step 5348, loss 0.228987, acc 0.96
2016-05-21T04:33:52.211461: train-step 5349, loss 0.205886, acc 0.97
2016-05-21T04:33:58.572668: train-step 5350, loss 0.230983, acc 0.95
2016-05-21T04:34:04.755155: train-step 5351, loss 0.21698, acc 0.93
2016-05-21T04:34:11.042207: train-step 5352, loss 0.201555, acc 0.97
2016-05-21T04:34:17.267519: train-step 5353, loss 0.298326, acc 0.9
2016-05-21T04:34:23.490451: train-step 5354, loss 0.195457, acc 0.97
2016-05-21T04:34:29.707706: train-step 5355, loss 0.215408, acc 0.95
2016-05-21T04:34:36.100242: train-step 5356, loss 0.238633, acc 0.93
2016-05-21T04:34:42.334161: train-step 5357, loss 0.240365, acc 0.95
2016-05-21T04:34:48.910497: train-step 5358, loss 0.19843, acc 0.98
2016-05-21T04:34:55.273010: train-step 5359, loss 0.226423, acc 0.94
2016-05-21T04:35:01.675335: train-step 5360, loss 0.251518, acc 0.94
2016-05-21T04:35:08.038013: train-step 5361, loss 0.218691, acc 0.94
2016-05-21T04:35:14.303529: train-step 5362, loss 0.251883, acc 0.95
2016-05-21T04:35:20.555815: train-step 5363, loss 0.211185, acc 0.96
2016-05-21T04:35:26.779956: train-step 5364, loss 0.176518, acc 0.96
2016-05-21T04:35:33.361288: train-step 5365, loss 0.227851, acc 0.95
2016-05-21T04:35:39.739405: train-step 5366, loss 0.266542, acc 0.93
2016-05-21T04:35:45.973048: train-step 5367, loss 0.221612, acc 0.96
2016-05-21T04:35:52.523893: train-step 5368, loss 0.190525, acc 0.99
2016-05-21T04:35:58.888586: train-step 5369, loss 0.221, acc 0.96
2016-05-21T04:36:05.313531: train-step 5370, loss 0.230756, acc 0.95
2016-05-21T04:36:11.541094: train-step 5371, loss 0.18783, acc 0.98
2016-05-21T04:36:17.704927: train-step 5372, loss 0.182592, acc 0.98
2016-05-21T04:36:23.852634: train-step 5373, loss 0.188685, acc 0.97
2016-05-21T04:36:30.075035: train-step 5374, loss 0.19655, acc 0.97
2016-05-21T04:36:36.281721: train-step 5375, loss 0.199264, acc 0.97
2016-05-21T04:36:42.575799: train-step 5376, loss 0.217882, acc 0.97
2016-05-21T04:36:48.879760: train-step 5377, loss 0.202567, acc 0.96
2016-05-21T04:36:55.106412: train-step 5378, loss 0.188619, acc 0.96
2016-05-21T04:37:01.673700: train-step 5379, loss 0.213928, acc 0.95
2016-05-21T04:37:07.992859: train-step 5380, loss 0.184248, acc 0.97
2016-05-21T04:37:14.232160: train-step 5381, loss 0.188977, acc 0.96
2016-05-21T04:37:20.824421: train-step 5382, loss 0.214842, acc 0.95
2016-05-21T04:37:27.201065: train-step 5383, loss 0.204502, acc 0.96
2016-05-21T04:37:33.548472: train-step 5384, loss 0.270257, acc 0.93
2016-05-21T04:37:39.800723: train-step 5385, loss 0.178374, acc 0.97
2016-05-21T04:37:46.059347: train-step 5386, loss 0.213651, acc 0.96
2016-05-21T04:37:52.352168: train-step 5387, loss 0.248762, acc 0.94
2016-05-21T04:37:58.725303: train-step 5388, loss 0.234081, acc 0.94
2016-05-21T04:38:05.016560: train-step 5389, loss 0.248047, acc 0.94
2016-05-21T04:38:11.240172: train-step 5390, loss 0.222579, acc 0.97
2016-05-21T04:38:17.846970: train-step 5391, loss 0.217072, acc 0.95
2016-05-21T04:38:24.151090: train-step 5392, loss 0.221535, acc 0.97
2016-05-21T04:38:30.293472: train-step 5393, loss 0.197146, acc 0.98
2016-05-21T04:38:36.493549: train-step 5394, loss 0.190105, acc 0.97
2016-05-21T04:38:42.971402: train-step 5395, loss 0.266327, acc 0.93
2016-05-21T04:38:49.409435: train-step 5396, loss 0.195499, acc 0.98
2016-05-21T04:38:55.648302: train-step 5397, loss 0.211569, acc 0.96
2016-05-21T04:39:01.915206: train-step 5398, loss 0.222237, acc 0.95
2016-05-21T04:39:08.113257: train-step 5399, loss 0.216562, acc 0.97
2016-05-21T04:39:14.730270: train-step 5400, loss 0.206181, acc 0.93
2016-05-21T04:39:21.115546: train-step 5401, loss 0.172777, acc 0.97
2016-05-21T04:39:27.484339: train-step 5402, loss 0.255762, acc 0.93
2016-05-21T04:39:33.834782: train-step 5403, loss 0.26327, acc 0.91
2016-05-21T04:39:40.069718: train-step 5404, loss 0.197043, acc 0.96
2016-05-21T04:39:46.511127: train-step 5405, loss 0.220986, acc 0.94
2016-05-21T04:39:52.932443: train-step 5406, loss 0.186121, acc 0.97
2016-05-21T04:39:59.118527: train-step 5407, loss 0.25526, acc 0.93
2016-05-21T04:40:05.436384: train-step 5408, loss 0.250267, acc 0.94
2016-05-21T04:40:12.014840: train-step 5409, loss 0.156418, acc 0.98
2016-05-21T04:40:18.250279: train-step 5410, loss 0.199153, acc 0.97
2016-05-21T04:40:24.491751: train-step 5411, loss 0.274299, acc 0.92
2016-05-21T04:40:31.112060: train-step 5412, loss 0.290275, acc 0.9
2016-05-21T04:40:37.406327: train-step 5413, loss 0.174541, acc 0.98
2016-05-21T04:40:43.619455: train-step 5414, loss 0.215227, acc 0.96
2016-05-21T04:40:49.827600: train-step 5415, loss 0.181181, acc 0.99
2016-05-21T04:40:56.038385: train-step 5416, loss 0.191118, acc 0.95
2016-05-21T04:41:02.200637: train-step 5417, loss 0.262735, acc 0.94
2016-05-21T04:41:08.393925: train-step 5418, loss 0.170836, acc 0.97
2016-05-21T04:41:14.611989: train-step 5419, loss 0.227909, acc 0.93
2016-05-21T04:41:20.829127: train-step 5420, loss 0.172555, acc 1
2016-05-21T04:41:27.221942: train-step 5421, loss 0.188387, acc 0.98
2016-05-21T04:41:33.672399: train-step 5422, loss 0.232743, acc 0.96
2016-05-21T04:41:39.860864: train-step 5423, loss 0.242458, acc 0.95
2016-05-21T04:41:46.095323: train-step 5424, loss 0.191075, acc 0.97
2016-05-21T04:41:52.414976: train-step 5425, loss 0.24071, acc 0.97
2016-05-21T04:41:58.763087: train-step 5426, loss 0.20899, acc 0.94
2016-05-21T04:42:05.058865: train-step 5427, loss 0.281027, acc 0.94
2016-05-21T04:42:11.260372: train-step 5428, loss 0.236521, acc 0.95
2016-05-21T04:42:17.458790: train-step 5429, loss 0.217734, acc 0.96
2016-05-21T04:42:23.966179: train-step 5430, loss 0.252822, acc 0.91
2016-05-21T04:42:30.372088: train-step 5431, loss 0.200916, acc 0.98
2016-05-21T04:42:36.729889: train-step 5432, loss 0.222453, acc 0.95
2016-05-21T04:42:43.061474: train-step 5433, loss 0.194441, acc 0.96
2016-05-21T04:42:49.384525: train-step 5434, loss 0.251267, acc 0.96
2016-05-21T04:42:55.585377: train-step 5435, loss 0.197653, acc 0.96
2016-05-21T04:43:02.190084: train-step 5436, loss 0.255216, acc 0.95
2016-05-21T04:43:08.541841: train-step 5437, loss 0.188683, acc 0.94
2016-05-21T04:43:14.769218: train-step 5438, loss 0.255157, acc 0.92
2016-05-21T04:43:21.274047: train-step 5439, loss 0.289267, acc 0.89
2016-05-21T04:43:27.620924: train-step 5440, loss 0.178926, acc 0.96
2016-05-21T04:43:33.995163: train-step 5441, loss 0.182986, acc 0.96
2016-05-21T04:43:40.393588: train-step 5442, loss 0.218384, acc 0.95
2016-05-21T04:43:46.688912: train-step 5443, loss 0.191919, acc 0.96
2016-05-21T04:43:52.957541: train-step 5444, loss 0.20014, acc 0.95
2016-05-21T04:43:59.210587: train-step 5445, loss 0.227653, acc 0.96
2016-05-21T04:44:05.791382: train-step 5446, loss 0.247525, acc 0.93
2016-05-21T04:44:12.088697: train-step 5447, loss 0.225792, acc 0.94
2016-05-21T04:44:18.312669: train-step 5448, loss 0.241196, acc 0.93
2016-05-21T04:44:24.855852: train-step 5449, loss 0.168355, acc 0.98
2016-05-21T04:44:31.202686: train-step 5450, loss 0.215012, acc 0.96
2016-05-21T04:44:37.409671: train-step 5451, loss 0.203822, acc 0.96
2016-05-21T04:44:43.610974: train-step 5452, loss 0.218753, acc 0.97
2016-05-21T04:44:49.782259: train-step 5453, loss 0.220533, acc 0.94
2016-05-21T04:44:56.040168: train-step 5454, loss 0.262853, acc 0.91
2016-05-21T04:45:02.485587: train-step 5455, loss 0.254528, acc 0.94
2016-05-21T04:45:08.988837: train-step 5456, loss 0.236293, acc 0.96
2016-05-21T04:45:15.351912: train-step 5457, loss 0.241697, acc 0.93
2016-05-21T04:45:21.656841: train-step 5458, loss 0.229794, acc 0.94
2016-05-21T04:45:27.857243: train-step 5459, loss 0.214629, acc 0.94
2016-05-21T04:45:34.072594: train-step 5460, loss 0.19874, acc 0.97
2016-05-21T04:45:40.448229: train-step 5461, loss 0.215266, acc 0.97
2016-05-21T04:45:46.647824: train-step 5462, loss 0.192771, acc 0.98
2016-05-21T04:45:53.109425: train-step 5463, loss 0.21578, acc 0.97
2016-05-21T04:45:59.457443: train-step 5464, loss 0.222665, acc 0.97
2016-05-21T04:46:05.848292: train-step 5465, loss 0.243631, acc 0.92
2016-05-21T04:46:12.070439: train-step 5466, loss 0.212851, acc 0.95
2016-05-21T04:46:18.331002: train-step 5467, loss 0.224608, acc 0.96
2016-05-21T04:46:24.560670: train-step 5468, loss 0.261608, acc 0.89
2016-05-21T04:46:30.834003: train-step 5469, loss 0.28526, acc 0.93
2016-05-21T04:46:37.342122: train-step 5470, loss 0.248703, acc 0.95
2016-05-21T04:46:43.644934: train-step 5471, loss 0.231791, acc 0.93
2016-05-21T04:46:50.046259: train-step 5472, loss 0.204677, acc 0.95
2016-05-21T04:46:56.330904: train-step 5473, loss 0.205995, acc 0.95
2016-05-21T04:47:02.535245: train-step 5474, loss 0.21501, acc 0.95
2016-05-21T04:47:08.849504: train-step 5475, loss 0.231103, acc 0.96
2016-05-21T04:47:15.106555: train-step 5476, loss 0.280098, acc 0.89
2016-05-21T04:47:21.448973: train-step 5477, loss 0.282784, acc 0.92
2016-05-21T04:47:28.037737: train-step 5478, loss 0.206557, acc 0.98
2016-05-21T04:47:34.652371: train-step 5479, loss 0.206857, acc 0.96
2016-05-21T04:47:40.983989: train-step 5480, loss 0.166908, acc 0.99
2016-05-21T04:47:47.155298: train-step 5481, loss 0.191729, acc 0.96
2016-05-21T04:47:53.687488: train-step 5482, loss 0.246958, acc 0.91
2016-05-21T04:47:59.990080: train-step 5483, loss 0.220266, acc 0.96
2016-05-21T04:48:06.381357: train-step 5484, loss 0.244012, acc 0.94
2016-05-21T04:48:12.659193: train-step 5485, loss 0.175479, acc 0.97
2016-05-21T04:48:18.849758: train-step 5486, loss 0.178039, acc 0.97
2016-05-21T04:48:25.174985: train-step 5487, loss 0.214211, acc 0.95
2016-05-21T04:48:31.482295: train-step 5488, loss 0.251013, acc 0.91
2016-05-21T04:48:37.800057: train-step 5489, loss 0.250554, acc 0.92
2016-05-21T04:48:44.121643: train-step 5490, loss 0.164449, acc 0.98
2016-05-21T04:48:50.514779: train-step 5491, loss 0.232424, acc 0.95
2016-05-21T04:48:56.913078: train-step 5492, loss 0.197844, acc 0.96
2016-05-21T04:49:03.241716: train-step 5493, loss 0.239427, acc 0.93
2016-05-21T04:49:09.552002: train-step 5494, loss 0.164886, acc 0.98
2016-05-21T04:49:15.854147: train-step 5495, loss 0.20497, acc 0.97
2016-05-21T04:49:22.250730: train-step 5496, loss 0.188397, acc 0.98
2016-05-21T04:49:28.500207: train-step 5497, loss 0.195862, acc 0.96
2016-05-21T04:49:34.716692: train-step 5498, loss 0.172871, acc 0.97
2016-05-21T04:49:41.036542: train-step 5499, loss 0.191459, acc 0.97
2016-05-21T04:49:47.289765: train-step 5500, loss 0.163818, acc 1
epoch number is: 22
2016-05-21T04:49:53.684290: train-step 5501, loss 0.242369, acc 0.94
2016-05-21T04:49:59.866346: train-step 5502, loss 0.197685, acc 0.98
2016-05-21T04:50:06.454457: train-step 5503, loss 0.205322, acc 0.96
2016-05-21T04:50:12.769999: train-step 5504, loss 0.218672, acc 0.94
2016-05-21T04:50:18.976389: train-step 5505, loss 0.200749, acc 0.97
2016-05-21T04:50:25.525177: train-step 5506, loss 0.237252, acc 0.96
2016-05-21T04:50:31.872367: train-step 5507, loss 0.224222, acc 0.94
2016-05-21T04:50:38.275617: train-step 5508, loss 0.214136, acc 0.96
2016-05-21T04:50:44.520804: train-step 5509, loss 0.203874, acc 0.95
2016-05-21T04:50:50.654480: train-step 5510, loss 0.197467, acc 0.97
2016-05-21T04:50:57.001324: train-step 5511, loss 0.216399, acc 0.96
2016-05-21T04:51:03.237176: train-step 5512, loss 0.178555, acc 0.98
2016-05-21T04:51:09.443688: train-step 5513, loss 0.1736, acc 0.98
2016-05-21T04:51:15.660343: train-step 5514, loss 0.225595, acc 0.97
2016-05-21T04:51:21.849230: train-step 5515, loss 0.212021, acc 0.94
2016-05-21T04:51:28.591092: train-step 5516, loss 0.190513, acc 0.99
2016-05-21T04:51:35.230584: train-step 5517, loss 0.204722, acc 0.96
2016-05-21T04:51:41.578931: train-step 5518, loss 0.214991, acc 0.95
2016-05-21T04:51:47.731345: train-step 5519, loss 0.216846, acc 0.95
2016-05-21T04:51:53.932276: train-step 5520, loss 0.16553, acc 1
2016-05-21T04:52:00.249563: train-step 5521, loss 0.184218, acc 0.99
2016-05-21T04:52:06.599529: train-step 5522, loss 0.240954, acc 0.94
2016-05-21T04:52:12.769854: train-step 5523, loss 0.212309, acc 0.94
2016-05-21T04:52:18.941535: train-step 5524, loss 0.199031, acc 0.97
2016-05-21T04:52:25.302608: train-step 5525, loss 0.209668, acc 0.97
2016-05-21T04:52:31.588248: train-step 5526, loss 0.254415, acc 0.91
2016-05-21T04:52:37.920417: train-step 5527, loss 0.16446, acc 0.99
2016-05-21T04:52:44.489148: train-step 5528, loss 0.185756, acc 0.97
2016-05-21T04:52:51.055540: train-step 5529, loss 0.197598, acc 0.98
2016-05-21T04:52:57.415158: train-step 5530, loss 0.195431, acc 0.96
2016-05-21T04:53:03.584580: train-step 5531, loss 0.225782, acc 0.95
2016-05-21T04:53:10.100091: train-step 5532, loss 0.167936, acc 0.98
2016-05-21T04:53:16.434293: train-step 5533, loss 0.211661, acc 0.98
2016-05-21T04:53:22.795389: train-step 5534, loss 0.196877, acc 0.98
2016-05-21T04:53:29.187077: train-step 5535, loss 0.188487, acc 0.97
2016-05-21T04:53:35.411364: train-step 5536, loss 0.23878, acc 0.93
2016-05-21T04:53:41.612856: train-step 5537, loss 0.161236, acc 1
2016-05-21T04:53:47.831307: train-step 5538, loss 0.213929, acc 0.95
2016-05-21T04:53:54.048032: train-step 5539, loss 0.208952, acc 0.96
2016-05-21T04:54:00.553735: train-step 5540, loss 0.220288, acc 0.95
2016-05-21T04:54:06.983947: train-step 5541, loss 0.187317, acc 0.99
2016-05-21T04:54:13.356613: train-step 5542, loss 0.176244, acc 0.99
2016-05-21T04:54:19.690864: train-step 5543, loss 0.197974, acc 0.97
2016-05-21T04:54:26.012029: train-step 5544, loss 0.189297, acc 0.97
2016-05-21T04:54:32.746437: train-step 5545, loss 0.213311, acc 0.97
2016-05-21T04:54:39.336452: train-step 5546, loss 0.222145, acc 0.94
2016-05-21T04:54:45.694299: train-step 5547, loss 0.19678, acc 0.96
2016-05-21T04:54:52.051609: train-step 5548, loss 0.249269, acc 0.95
2016-05-21T04:54:58.428842: train-step 5549, loss 0.200627, acc 0.98
2016-05-21T04:55:04.697515: train-step 5550, loss 0.208818, acc 0.94
2016-05-21T04:55:11.070990: train-step 5551, loss 0.199258, acc 0.96
2016-05-21T04:55:17.620360: train-step 5552, loss 0.202597, acc 0.98
2016-05-21T04:55:23.817448: train-step 5553, loss 0.186132, acc 0.97
2016-05-21T04:55:30.168570: train-step 5554, loss 0.17931, acc 0.96
2016-05-21T04:55:36.687159: train-step 5555, loss 0.246746, acc 0.93
2016-05-21T04:55:43.310463: train-step 5556, loss 0.214228, acc 0.96
2016-05-21T04:55:49.594401: train-step 5557, loss 0.211463, acc 0.96
2016-05-21T04:55:55.813181: train-step 5558, loss 0.209836, acc 0.95
2016-05-21T04:56:02.008417: train-step 5559, loss 0.303759, acc 0.9
2016-05-21T04:56:08.383099: train-step 5560, loss 0.203326, acc 0.98
2016-05-21T04:56:14.758290: train-step 5561, loss 0.189128, acc 0.97
2016-05-21T04:56:21.100515: train-step 5562, loss 0.264294, acc 0.94
2016-05-21T04:56:27.331271: train-step 5563, loss 0.211201, acc 0.95
2016-05-21T04:56:33.572053: train-step 5564, loss 0.192311, acc 0.95
2016-05-21T04:56:40.159415: train-step 5565, loss 0.255706, acc 0.95
2016-05-21T04:56:46.511241: train-step 5566, loss 0.199004, acc 0.94
2016-05-21T04:56:52.879365: train-step 5567, loss 0.207732, acc 0.96
2016-05-21T04:56:59.096667: train-step 5568, loss 0.186862, acc 0.96
2016-05-21T04:57:05.618380: train-step 5569, loss 0.24039, acc 0.94
2016-05-21T04:57:11.987241: train-step 5570, loss 0.229709, acc 0.96
2016-05-21T04:57:18.335121: train-step 5571, loss 0.207843, acc 0.95
2016-05-21T04:57:24.615062: train-step 5572, loss 0.254261, acc 0.94
2016-05-21T04:57:31.003762: train-step 5573, loss 0.192143, acc 0.96
2016-05-21T04:57:37.541111: train-step 5574, loss 0.234848, acc 0.92
2016-05-21T04:57:43.740709: train-step 5575, loss 0.204938, acc 0.95
2016-05-21T04:57:50.013428: train-step 5576, loss 0.194776, acc 0.98
2016-05-21T04:57:56.231507: train-step 5577, loss 0.191166, acc 0.97
2016-05-21T04:58:02.498970: train-step 5578, loss 0.227796, acc 0.92
2016-05-21T04:58:08.807702: train-step 5579, loss 0.225294, acc 0.96
2016-05-21T04:58:15.164498: train-step 5580, loss 0.19606, acc 0.98
2016-05-21T04:58:21.554091: train-step 5581, loss 0.213987, acc 0.98
2016-05-21T04:58:27.835231: train-step 5582, loss 0.222464, acc 0.96
2016-05-21T04:58:34.104890: train-step 5583, loss 0.183752, acc 0.98
2016-05-21T04:58:40.313268: train-step 5584, loss 0.240098, acc 0.94
2016-05-21T04:58:46.903405: train-step 5585, loss 0.261797, acc 0.93
2016-05-21T04:58:53.259419: train-step 5586, loss 0.222509, acc 0.95
2016-05-21T04:58:59.424054: train-step 5587, loss 0.208223, acc 0.97
2016-05-21T04:59:05.995036: train-step 5588, loss 0.221965, acc 0.97
2016-05-21T04:59:12.337249: train-step 5589, loss 0.234309, acc 0.94
2016-05-21T04:59:18.726006: train-step 5590, loss 0.224373, acc 0.97
2016-05-21T04:59:24.997838: train-step 5591, loss 0.273137, acc 0.91
2016-05-21T04:59:31.181811: train-step 5592, loss 0.236787, acc 0.93
2016-05-21T04:59:37.471593: train-step 5593, loss 0.237893, acc 0.94
2016-05-21T04:59:43.676656: train-step 5594, loss 0.204299, acc 0.97
2016-05-21T04:59:49.918035: train-step 5595, loss 0.207272, acc 0.94
2016-05-21T04:59:56.477311: train-step 5596, loss 0.282856, acc 0.92
2016-05-21T05:00:02.782470: train-step 5597, loss 0.219248, acc 0.96
2016-05-21T05:00:08.972313: train-step 5598, loss 0.195595, acc 0.96
2016-05-21T05:00:15.219682: train-step 5599, loss 0.23521, acc 0.94
2016-05-21T05:00:21.543491: train-step 5600, loss 0.201866, acc 0.95
2016-05-21T05:00:27.709040: train-step 5601, loss 0.173939, acc 0.98
2016-05-21T05:00:33.880142: train-step 5602, loss 0.163232, acc 0.99
2016-05-21T05:00:40.218096: train-step 5603, loss 0.189337, acc 0.95
2016-05-21T05:00:46.368361: train-step 5604, loss 0.21061, acc 0.95
2016-05-21T05:00:52.633979: train-step 5605, loss 0.178832, acc 0.94
2016-05-21T05:00:58.950419: train-step 5606, loss 0.29, acc 0.91
2016-05-21T05:01:05.467551: train-step 5607, loss 0.22633, acc 0.95
2016-05-21T05:01:11.710568: train-step 5608, loss 0.189485, acc 0.97
2016-05-21T05:01:17.964849: train-step 5609, loss 0.195876, acc 0.98
2016-05-21T05:01:24.178669: train-step 5610, loss 0.21499, acc 0.95
2016-05-21T05:01:30.707719: train-step 5611, loss 0.194965, acc 0.96
2016-05-21T05:01:37.041629: train-step 5612, loss 0.233567, acc 0.95
2016-05-21T05:01:43.198871: train-step 5613, loss 0.227893, acc 0.96
2016-05-21T05:01:49.429578: train-step 5614, loss 0.177065, acc 0.99
2016-05-21T05:01:55.631159: train-step 5615, loss 0.167935, acc 0.99
2016-05-21T05:02:01.824508: train-step 5616, loss 0.199602, acc 0.96
2016-05-21T05:02:08.023990: train-step 5617, loss 0.240008, acc 0.93
2016-05-21T05:02:14.292164: train-step 5618, loss 0.203416, acc 0.96
2016-05-21T05:02:20.558518: train-step 5619, loss 0.248478, acc 0.96
2016-05-21T05:02:27.176618: train-step 5620, loss 0.199003, acc 0.98
2016-05-21T05:02:33.560827: train-step 5621, loss 0.231305, acc 0.94
2016-05-21T05:02:39.738428: train-step 5622, loss 0.219862, acc 0.96
2016-05-21T05:02:46.253080: train-step 5623, loss 0.22248, acc 0.97
2016-05-21T05:02:52.616980: train-step 5624, loss 0.202458, acc 0.97
2016-05-21T05:02:59.058018: train-step 5625, loss 0.172323, acc 0.97
2016-05-21T05:03:05.255134: train-step 5626, loss 0.17672, acc 0.98
2016-05-21T05:03:11.426987: train-step 5627, loss 0.216688, acc 0.96
2016-05-21T05:03:17.756991: train-step 5628, loss 0.220795, acc 0.94
2016-05-21T05:03:24.017928: train-step 5629, loss 0.192089, acc 0.98
2016-05-21T05:03:30.343896: train-step 5630, loss 0.231635, acc 0.96
2016-05-21T05:03:36.901021: train-step 5631, loss 0.184134, acc 0.97
2016-05-21T05:03:43.512359: train-step 5632, loss 0.176978, acc 0.96
2016-05-21T05:03:49.835884: train-step 5633, loss 0.190168, acc 0.98
2016-05-21T05:03:56.215548: train-step 5634, loss 0.217837, acc 0.96
2016-05-21T05:04:02.390721: train-step 5635, loss 0.217668, acc 0.95
2016-05-21T05:04:08.886566: train-step 5636, loss 0.167813, acc 0.98
2016-05-21T05:04:15.337593: train-step 5637, loss 0.224396, acc 0.94
2016-05-21T05:04:21.582626: train-step 5638, loss 0.248206, acc 0.96
2016-05-21T05:04:27.750693: train-step 5639, loss 0.222665, acc 0.95
2016-05-21T05:04:34.129171: train-step 5640, loss 0.227267, acc 0.94
2016-05-21T05:04:40.322207: train-step 5641, loss 0.161576, acc 0.97
2016-05-21T05:04:46.527706: train-step 5642, loss 0.233257, acc 0.94
2016-05-21T05:04:52.786406: train-step 5643, loss 0.224837, acc 0.96
2016-05-21T05:04:59.404283: train-step 5644, loss 0.230235, acc 0.94
2016-05-21T05:05:05.741116: train-step 5645, loss 0.21603, acc 0.96
2016-05-21T05:05:12.105899: train-step 5646, loss 0.218168, acc 0.95
2016-05-21T05:05:18.357041: train-step 5647, loss 0.196292, acc 0.98
2016-05-21T05:05:24.839214: train-step 5648, loss 0.213721, acc 0.95
2016-05-21T05:05:31.274113: train-step 5649, loss 0.271547, acc 0.94
2016-05-21T05:05:37.463278: train-step 5650, loss 0.167306, acc 0.99
2016-05-21T05:05:43.625244: train-step 5651, loss 0.154951, acc 0.98
2016-05-21T05:05:49.941874: train-step 5652, loss 0.201097, acc 0.98
2016-05-21T05:05:56.164517: train-step 5653, loss 0.23977, acc 0.94
2016-05-21T05:06:02.372350: train-step 5654, loss 0.299059, acc 0.92
2016-05-21T05:06:08.600462: train-step 5655, loss 0.208498, acc 0.95
2016-05-21T05:06:15.190081: train-step 5656, loss 0.203562, acc 0.95
2016-05-21T05:06:21.550914: train-step 5657, loss 0.150215, acc 0.99
2016-05-21T05:06:27.790039: train-step 5658, loss 0.267277, acc 0.91
2016-05-21T05:06:34.025720: train-step 5659, loss 0.239714, acc 0.97
2016-05-21T05:06:40.278146: train-step 5660, loss 0.192987, acc 0.96
2016-05-21T05:06:46.483227: train-step 5661, loss 0.20904, acc 0.96
2016-05-21T05:06:52.655873: train-step 5662, loss 0.258477, acc 0.93
2016-05-21T05:06:58.915505: train-step 5663, loss 0.21054, acc 0.95
2016-05-21T05:07:05.269997: train-step 5664, loss 0.151392, acc 0.98
2016-05-21T05:07:11.634819: train-step 5665, loss 0.203692, acc 0.95
2016-05-21T05:07:17.867990: train-step 5666, loss 0.245638, acc 0.93
2016-05-21T05:07:24.133657: train-step 5667, loss 0.140596, acc 1
2016-05-21T05:07:30.464200: train-step 5668, loss 0.206339, acc 0.97
2016-05-21T05:07:37.059097: train-step 5669, loss 0.187837, acc 0.97
2016-05-21T05:07:43.332383: train-step 5670, loss 0.181506, acc 0.98
2016-05-21T05:07:49.528780: train-step 5671, loss 0.226101, acc 0.96
2016-05-21T05:07:55.730739: train-step 5672, loss 0.192381, acc 0.95
2016-05-21T05:08:01.940328: train-step 5673, loss 0.199203, acc 0.98
2016-05-21T05:08:08.162407: train-step 5674, loss 0.215509, acc 0.94
2016-05-21T05:08:14.428396: train-step 5675, loss 0.24712, acc 0.95
2016-05-21T05:08:20.634946: train-step 5676, loss 0.226766, acc 0.92
2016-05-21T05:08:26.821256: train-step 5677, loss 0.23603, acc 0.96
2016-05-21T05:08:33.183450: train-step 5678, loss 0.161219, acc 0.98
2016-05-21T05:08:39.580815: train-step 5679, loss 0.174526, acc 0.99
2016-05-21T05:08:45.820938: train-step 5680, loss 0.220569, acc 0.98
2016-05-21T05:08:52.023776: train-step 5681, loss 0.215276, acc 0.96
2016-05-21T05:08:58.336104: train-step 5682, loss 0.182366, acc 0.94
2016-05-21T05:09:04.685430: train-step 5683, loss 0.235043, acc 0.93
2016-05-21T05:09:10.957154: train-step 5684, loss 0.255458, acc 0.95
2016-05-21T05:09:17.175626: train-step 5685, loss 0.229359, acc 0.95
2016-05-21T05:09:23.770949: train-step 5686, loss 0.266785, acc 0.93
2016-05-21T05:09:30.128944: train-step 5687, loss 0.173794, acc 0.99
2016-05-21T05:09:36.370368: train-step 5688, loss 0.165923, acc 0.99
2016-05-21T05:09:42.906749: train-step 5689, loss 0.200169, acc 0.96
2016-05-21T05:09:49.453400: train-step 5690, loss 0.217191, acc 0.98
2016-05-21T05:09:55.858137: train-step 5691, loss 0.230899, acc 0.95
2016-05-21T05:10:02.187190: train-step 5692, loss 0.189069, acc 0.96
2016-05-21T05:10:08.401549: train-step 5693, loss 0.156598, acc 1
2016-05-21T05:10:14.712395: train-step 5694, loss 0.181632, acc 0.97
2016-05-21T05:10:21.309614: train-step 5695, loss 0.246608, acc 0.94
2016-05-21T05:10:27.620667: train-step 5696, loss 0.219144, acc 0.94
2016-05-21T05:10:33.981586: train-step 5697, loss 0.198575, acc 0.97
2016-05-21T05:10:40.203374: train-step 5698, loss 0.209968, acc 0.94
2016-05-21T05:10:46.756753: train-step 5699, loss 0.213323, acc 0.93
2016-05-21T05:10:53.128658: train-step 5700, loss 0.224227, acc 0.93
2016-05-21T05:10:59.497337: train-step 5701, loss 0.232672, acc 0.94
2016-05-21T05:11:05.724285: train-step 5702, loss 0.231462, acc 0.95
2016-05-21T05:11:11.944711: train-step 5703, loss 0.223245, acc 0.93
2016-05-21T05:11:18.278099: train-step 5704, loss 0.18086, acc 0.96
2016-05-21T05:11:24.625654: train-step 5705, loss 0.175053, acc 0.99
2016-05-21T05:11:30.935236: train-step 5706, loss 0.203943, acc 0.96
2016-05-21T05:11:37.144434: train-step 5707, loss 0.234718, acc 0.93
2016-05-21T05:11:43.362978: train-step 5708, loss 0.217544, acc 0.95
2016-05-21T05:11:49.719231: train-step 5709, loss 0.202066, acc 0.96
2016-05-21T05:11:55.938716: train-step 5710, loss 0.218721, acc 0.97
2016-05-21T05:12:02.134983: train-step 5711, loss 0.19875, acc 0.97
2016-05-21T05:12:08.493089: train-step 5712, loss 0.177526, acc 0.98
2016-05-21T05:12:14.824563: train-step 5713, loss 0.216231, acc 0.92
2016-05-21T05:12:21.046189: train-step 5714, loss 0.202023, acc 0.98
2016-05-21T05:12:27.297989: train-step 5715, loss 0.156528, acc 0.99
2016-05-21T05:12:33.533729: train-step 5716, loss 0.236259, acc 0.92
2016-05-21T05:12:39.763001: train-step 5717, loss 0.223468, acc 0.96
2016-05-21T05:12:45.910693: train-step 5718, loss 0.178345, acc 0.98
2016-05-21T05:12:52.087102: train-step 5719, loss 0.184437, acc 0.98
2016-05-21T05:12:58.298410: train-step 5720, loss 0.179389, acc 0.97
2016-05-21T05:13:04.643380: train-step 5721, loss 0.224715, acc 0.93
2016-05-21T05:13:11.007399: train-step 5722, loss 0.188562, acc 0.98
2016-05-21T05:13:17.373282: train-step 5723, loss 0.212621, acc 0.96
2016-05-21T05:13:23.726822: train-step 5724, loss 0.256923, acc 0.91
2016-05-21T05:13:30.028545: train-step 5725, loss 0.249133, acc 0.95
2016-05-21T05:13:36.267919: train-step 5726, loss 0.213775, acc 0.97
2016-05-21T05:13:42.499339: train-step 5727, loss 0.193626, acc 0.96
2016-05-21T05:13:48.710654: train-step 5728, loss 0.174111, acc 0.98
2016-05-21T05:13:54.901026: train-step 5729, loss 0.196622, acc 0.96
2016-05-21T05:14:01.120062: train-step 5730, loss 0.185975, acc 0.97
2016-05-21T05:14:07.340860: train-step 5731, loss 0.203614, acc 0.98
2016-05-21T05:14:14.063941: train-step 5732, loss 0.178904, acc 0.98
2016-05-21T05:14:20.613565: train-step 5733, loss 0.175973, acc 0.98
2016-05-21T05:14:26.959412: train-step 5734, loss 0.226152, acc 0.95
2016-05-21T05:14:33.320070: train-step 5735, loss 0.182901, acc 0.98
2016-05-21T05:14:39.662044: train-step 5736, loss 0.165925, acc 0.99
2016-05-21T05:14:45.901348: train-step 5737, loss 0.238067, acc 0.96
2016-05-21T05:14:52.091565: train-step 5738, loss 0.192502, acc 0.98
2016-05-21T05:14:58.422409: train-step 5739, loss 0.170551, acc 0.98
2016-05-21T05:15:04.696723: train-step 5740, loss 0.185979, acc 0.98
2016-05-21T05:15:10.945413: train-step 5741, loss 0.265789, acc 0.93
2016-05-21T05:15:17.538254: train-step 5742, loss 0.204983, acc 0.95
2016-05-21T05:15:24.141711: train-step 5743, loss 0.220319, acc 0.95
2016-05-21T05:15:30.421766: train-step 5744, loss 0.221617, acc 0.93
2016-05-21T05:15:36.593326: train-step 5745, loss 0.239487, acc 0.94
2016-05-21T05:15:43.098124: train-step 5746, loss 0.197289, acc 0.97
2016-05-21T05:15:49.463460: train-step 5747, loss 0.227782, acc 0.93
2016-05-21T05:15:55.707032: train-step 5748, loss 0.28603, acc 0.91
2016-05-21T05:16:01.961996: train-step 5749, loss 0.208883, acc 0.96
2016-05-21T05:16:08.187971: train-step 5750, loss 0.21664, acc 0.94
epoch number is: 23
2016-05-21T05:16:14.657507: train-step 5751, loss 0.182544, acc 0.98
2016-05-21T05:16:20.878876: train-step 5752, loss 0.182917, acc 0.98
2016-05-21T05:16:27.134111: train-step 5753, loss 0.186181, acc 0.97
2016-05-21T05:16:33.320505: train-step 5754, loss 0.18449, acc 0.97
2016-05-21T05:16:39.482643: train-step 5755, loss 0.187325, acc 0.96
2016-05-21T05:16:45.690405: train-step 5756, loss 0.233193, acc 0.95
2016-05-21T05:16:51.944563: train-step 5757, loss 0.264154, acc 0.92
2016-05-21T05:16:58.100158: train-step 5758, loss 0.202445, acc 0.97
2016-05-21T05:17:04.330900: train-step 5759, loss 0.182996, acc 0.99
2016-05-21T05:17:10.480239: train-step 5760, loss 0.240058, acc 0.94
2016-05-21T05:17:16.674644: train-step 5761, loss 0.181136, acc 0.98
2016-05-21T05:17:22.858590: train-step 5762, loss 0.183499, acc 0.99
2016-05-21T05:17:29.121699: train-step 5763, loss 0.232327, acc 0.95
2016-05-21T05:17:35.331002: train-step 5764, loss 0.171857, acc 0.98
2016-05-21T05:17:41.544810: train-step 5765, loss 0.267186, acc 0.93
2016-05-21T05:17:47.713666: train-step 5766, loss 0.191837, acc 0.97
2016-05-21T05:17:53.966046: train-step 5767, loss 0.172591, acc 0.99
2016-05-21T05:18:00.210584: train-step 5768, loss 0.221116, acc 0.95
2016-05-21T05:18:06.398776: train-step 5769, loss 0.167111, acc 0.97
2016-05-21T05:18:12.599976: train-step 5770, loss 0.182918, acc 0.98
2016-05-21T05:18:18.836804: train-step 5771, loss 0.218457, acc 0.94
2016-05-21T05:18:25.298797: train-step 5772, loss 0.25999, acc 0.93
2016-05-21T05:18:31.787590: train-step 5773, loss 0.203734, acc 0.96
2016-05-21T05:18:38.032287: train-step 5774, loss 0.180328, acc 0.98
2016-05-21T05:18:44.466176: train-step 5775, loss 0.209621, acc 0.97
2016-05-21T05:18:50.926424: train-step 5776, loss 0.220275, acc 0.92
2016-05-21T05:18:57.134822: train-step 5777, loss 0.18865, acc 0.97
2016-05-21T05:19:03.394878: train-step 5778, loss 0.207655, acc 0.97
2016-05-21T05:19:09.584041: train-step 5779, loss 0.201645, acc 0.96
2016-05-21T05:19:16.181897: train-step 5780, loss 0.194409, acc 0.95
2016-05-21T05:19:22.523537: train-step 5781, loss 0.190979, acc 0.96
2016-05-21T05:19:28.680108: train-step 5782, loss 0.212657, acc 0.96
2016-05-21T05:19:34.893023: train-step 5783, loss 0.191655, acc 0.96
2016-05-21T05:19:41.086068: train-step 5784, loss 0.196841, acc 0.97
2016-05-21T05:19:47.412901: train-step 5785, loss 0.203031, acc 0.95
2016-05-21T05:19:53.731095: train-step 5786, loss 0.212075, acc 0.93
2016-05-21T05:20:00.009053: train-step 5787, loss 0.185107, acc 0.99
2016-05-21T05:20:06.164389: train-step 5788, loss 0.202313, acc 0.97
2016-05-21T05:20:12.386612: train-step 5789, loss 0.231474, acc 0.93
2016-05-21T05:20:18.709824: train-step 5790, loss 0.208228, acc 0.94
2016-05-21T05:20:24.950637: train-step 5791, loss 0.226739, acc 0.94
2016-05-21T05:20:31.188169: train-step 5792, loss 0.199782, acc 0.97
2016-05-21T05:20:37.409030: train-step 5793, loss 0.177622, acc 0.97
2016-05-21T05:20:43.688751: train-step 5794, loss 0.163764, acc 0.98
2016-05-21T05:20:50.211402: train-step 5795, loss 0.189336, acc 0.94
2016-05-21T05:20:56.461330: train-step 5796, loss 0.254247, acc 0.93
2016-05-21T05:21:02.700669: train-step 5797, loss 0.245424, acc 0.95
2016-05-21T05:21:08.899310: train-step 5798, loss 0.240299, acc 0.95
2016-05-21T05:21:15.422196: train-step 5799, loss 0.217978, acc 0.95
2016-05-21T05:21:21.802646: train-step 5800, loss 0.202099, acc 0.98
2016-05-21T05:21:28.192087: train-step 5801, loss 0.186188, acc 0.97
2016-05-21T05:21:34.408636: train-step 5802, loss 0.202672, acc 0.97
2016-05-21T05:21:40.586103: train-step 5803, loss 0.203864, acc 0.95
2016-05-21T05:21:46.930194: train-step 5804, loss 0.203115, acc 0.98
2016-05-21T05:21:53.187337: train-step 5805, loss 0.253295, acc 0.94
2016-05-21T05:21:59.409518: train-step 5806, loss 0.200507, acc 0.95
2016-05-21T05:22:05.631027: train-step 5807, loss 0.19336, acc 0.96
2016-05-21T05:22:11.876804: train-step 5808, loss 0.173424, acc 0.97
2016-05-21T05:22:18.372891: train-step 5809, loss 0.175199, acc 0.96
2016-05-21T05:22:24.691837: train-step 5810, loss 0.183649, acc 0.99
2016-05-21T05:22:30.929873: train-step 5811, loss 0.22185, acc 0.94
2016-05-21T05:22:37.155410: train-step 5812, loss 0.216003, acc 0.95
2016-05-21T05:22:43.370264: train-step 5813, loss 0.169018, acc 0.97
2016-05-21T05:22:49.616364: train-step 5814, loss 0.186631, acc 0.98
2016-05-21T05:22:56.170475: train-step 5815, loss 0.236185, acc 0.96
2016-05-21T05:23:02.472347: train-step 5816, loss 0.190529, acc 0.95
2016-05-21T05:23:08.696484: train-step 5817, loss 0.211471, acc 0.97
2016-05-21T05:23:15.216353: train-step 5818, loss 0.271638, acc 0.92
2016-05-21T05:23:21.599192: train-step 5819, loss 0.168836, acc 0.98
2016-05-21T05:23:27.944732: train-step 5820, loss 0.193252, acc 0.95
2016-05-21T05:23:34.153189: train-step 5821, loss 0.19234, acc 0.97
2016-05-21T05:23:40.561726: train-step 5822, loss 0.222636, acc 0.93
2016-05-21T05:23:47.076538: train-step 5823, loss 0.232636, acc 0.96
2016-05-21T05:23:53.306046: train-step 5824, loss 0.243759, acc 0.95
2016-05-21T05:23:59.531268: train-step 5825, loss 0.178487, acc 0.97
2016-05-21T05:24:05.785916: train-step 5826, loss 0.173918, acc 0.99
2016-05-21T05:24:11.984812: train-step 5827, loss 0.18914, acc 0.97
2016-05-21T05:24:18.184456: train-step 5828, loss 0.254539, acc 0.94
2016-05-21T05:24:24.457086: train-step 5829, loss 0.248315, acc 0.95
2016-05-21T05:24:30.685651: train-step 5830, loss 0.2305, acc 0.97
2016-05-21T05:24:36.878393: train-step 5831, loss 0.231705, acc 0.96
2016-05-21T05:24:43.384258: train-step 5832, loss 0.220986, acc 0.95
2016-05-21T05:24:49.708553: train-step 5833, loss 0.18811, acc 0.96
2016-05-21T05:24:55.947271: train-step 5834, loss 0.18994, acc 0.98
2016-05-21T05:25:02.170232: train-step 5835, loss 0.182599, acc 0.96
2016-05-21T05:25:08.354689: train-step 5836, loss 0.209885, acc 0.96
2016-05-21T05:25:14.516838: train-step 5837, loss 0.1711, acc 1
2016-05-21T05:25:20.738934: train-step 5838, loss 0.163167, acc 0.97
2016-05-21T05:25:26.903281: train-step 5839, loss 0.211951, acc 0.94
2016-05-21T05:25:33.046065: train-step 5840, loss 0.224663, acc 0.98
2016-05-21T05:25:39.280715: train-step 5841, loss 0.224979, acc 0.95
2016-05-21T05:25:45.652821: train-step 5842, loss 0.230527, acc 0.95
2016-05-21T05:25:52.002371: train-step 5843, loss 0.215193, acc 0.96
2016-05-21T05:25:58.256523: train-step 5844, loss 0.188985, acc 0.98
2016-05-21T05:26:04.689167: train-step 5845, loss 0.184137, acc 0.97
2016-05-21T05:26:11.211778: train-step 5846, loss 0.167518, acc 0.99
2016-05-21T05:26:17.517156: train-step 5847, loss 0.180305, acc 0.99
2016-05-21T05:26:23.875908: train-step 5848, loss 0.17312, acc 0.98
2016-05-21T05:26:30.651663: train-step 5849, loss 0.241971, acc 0.94
2016-05-21T05:26:37.280447: train-step 5850, loss 0.196595, acc 0.97
2016-05-21T05:26:43.612030: train-step 5851, loss 0.195903, acc 0.95
2016-05-21T05:26:49.824076: train-step 5852, loss 0.196562, acc 0.97
2016-05-21T05:26:56.321328: train-step 5853, loss 0.164964, acc 0.97
2016-05-21T05:27:02.632140: train-step 5854, loss 0.180633, acc 0.99
2016-05-21T05:27:09.029565: train-step 5855, loss 0.246741, acc 0.95
2016-05-21T05:27:15.196288: train-step 5856, loss 0.196909, acc 0.96
2016-05-21T05:27:21.695228: train-step 5857, loss 0.228409, acc 0.96
2016-05-21T05:27:28.131148: train-step 5858, loss 0.190124, acc 0.95
2016-05-21T05:27:34.314100: train-step 5859, loss 0.199459, acc 0.95
2016-05-21T05:27:40.553136: train-step 5860, loss 0.193601, acc 0.97
2016-05-21T05:27:46.765349: train-step 5861, loss 0.204254, acc 0.96
2016-05-21T05:27:52.987937: train-step 5862, loss 0.227344, acc 0.96
2016-05-21T05:27:59.179602: train-step 5863, loss 0.186991, acc 0.97
2016-05-21T05:28:05.561997: train-step 5864, loss 0.21478, acc 0.95
2016-05-21T05:28:11.806596: train-step 5865, loss 0.249442, acc 0.93
2016-05-21T05:28:18.066196: train-step 5866, loss 0.213299, acc 0.95
2016-05-21T05:28:24.320668: train-step 5867, loss 0.189532, acc 0.98
2016-05-21T05:28:30.573674: train-step 5868, loss 0.186356, acc 0.97
2016-05-21T05:28:36.791536: train-step 5869, loss 0.208852, acc 0.97
2016-05-21T05:28:42.990517: train-step 5870, loss 0.199497, acc 0.95
2016-05-21T05:28:49.173146: train-step 5871, loss 0.19245, acc 0.97
2016-05-21T05:28:55.342164: train-step 5872, loss 0.186411, acc 0.99
2016-05-21T05:29:01.659789: train-step 5873, loss 0.212778, acc 0.97
2016-05-21T05:29:07.839389: train-step 5874, loss 0.18023, acc 0.97
2016-05-21T05:29:14.054931: train-step 5875, loss 0.15763, acc 0.99
2016-05-21T05:29:20.200947: train-step 5876, loss 0.220661, acc 0.95
2016-05-21T05:29:26.415647: train-step 5877, loss 0.169629, acc 0.98
2016-05-21T05:29:32.598313: train-step 5878, loss 0.194404, acc 0.96
2016-05-21T05:29:38.816358: train-step 5879, loss 0.246052, acc 0.94
2016-05-21T05:29:45.025573: train-step 5880, loss 0.14168, acc 1
2016-05-21T05:29:51.283559: train-step 5881, loss 0.176137, acc 0.98
2016-05-21T05:29:57.473104: train-step 5882, loss 0.191501, acc 0.98
2016-05-21T05:30:03.699481: train-step 5883, loss 0.251684, acc 0.92
2016-05-21T05:30:10.030676: train-step 5884, loss 0.20841, acc 0.98
2016-05-21T05:30:16.205229: train-step 5885, loss 0.20191, acc 0.96
2016-05-21T05:30:22.365401: train-step 5886, loss 0.208812, acc 0.95
2016-05-21T05:30:28.712767: train-step 5887, loss 0.194131, acc 0.96
2016-05-21T05:30:35.092361: train-step 5888, loss 0.226769, acc 0.95
2016-05-21T05:30:41.355964: train-step 5889, loss 0.200784, acc 0.96
2016-05-21T05:30:47.574971: train-step 5890, loss 0.230616, acc 0.93
2016-05-21T05:30:53.821719: train-step 5891, loss 0.198521, acc 0.96
2016-05-21T05:31:00.372595: train-step 5892, loss 0.217865, acc 0.96
2016-05-21T05:31:06.740571: train-step 5893, loss 0.202401, acc 0.98
2016-05-21T05:31:13.127781: train-step 5894, loss 0.207239, acc 0.97
2016-05-21T05:31:19.319116: train-step 5895, loss 0.206144, acc 0.96
2016-05-21T05:31:25.758202: train-step 5896, loss 0.217792, acc 0.95
2016-05-21T05:31:32.244598: train-step 5897, loss 0.271112, acc 0.94
2016-05-21T05:31:38.504211: train-step 5898, loss 0.259971, acc 0.9
2016-05-21T05:31:44.732697: train-step 5899, loss 0.192611, acc 0.98
2016-05-21T05:31:50.933094: train-step 5900, loss 0.211883, acc 0.97
2016-05-21T05:31:57.163262: train-step 5901, loss 0.193205, acc 0.98
2016-05-21T05:32:03.389016: train-step 5902, loss 0.207064, acc 0.95
2016-05-21T05:32:09.590564: train-step 5903, loss 0.171105, acc 0.98
2016-05-21T05:32:15.791748: train-step 5904, loss 0.178109, acc 0.97
2016-05-21T05:32:22.013774: train-step 5905, loss 0.200022, acc 0.96
2016-05-21T05:32:28.369617: train-step 5906, loss 0.22555, acc 0.93
2016-05-21T05:32:34.622980: train-step 5907, loss 0.169141, acc 0.98
2016-05-21T05:32:40.819566: train-step 5908, loss 0.196488, acc 0.96
2016-05-21T05:32:47.011015: train-step 5909, loss 0.202556, acc 0.97
2016-05-21T05:32:53.230765: train-step 5910, loss 0.148797, acc 0.98
2016-05-21T05:32:59.774130: train-step 5911, loss 0.206007, acc 0.96
2016-05-21T05:33:06.153797: train-step 5912, loss 0.220654, acc 0.94
2016-05-21T05:33:12.560475: train-step 5913, loss 0.173846, acc 0.99
2016-05-21T05:33:18.831998: train-step 5914, loss 0.19332, acc 0.98
2016-05-21T05:33:25.245990: train-step 5915, loss 0.267323, acc 0.92
2016-05-21T05:33:31.784616: train-step 5916, loss 0.212092, acc 0.96
2016-05-21T05:33:37.986168: train-step 5917, loss 0.20201, acc 0.95
2016-05-21T05:33:44.226328: train-step 5918, loss 0.19004, acc 0.97
2016-05-21T05:33:50.456605: train-step 5919, loss 0.199982, acc 0.95
2016-05-21T05:33:56.649468: train-step 5920, loss 0.193615, acc 0.96
2016-05-21T05:34:02.896024: train-step 5921, loss 0.219091, acc 0.94
2016-05-21T05:34:09.136541: train-step 5922, loss 0.217671, acc 0.94
2016-05-21T05:34:15.484519: train-step 5923, loss 0.180332, acc 0.99
2016-05-21T05:34:21.704183: train-step 5924, loss 0.191703, acc 0.97
2016-05-21T05:34:27.937607: train-step 5925, loss 0.204413, acc 0.96
2016-05-21T05:34:34.113842: train-step 5926, loss 0.232887, acc 0.95
2016-05-21T05:34:40.639037: train-step 5927, loss 0.182075, acc 0.97
2016-05-21T05:34:46.991805: train-step 5928, loss 0.228767, acc 0.95
2016-05-21T05:34:53.339498: train-step 5929, loss 0.160352, acc 0.97
2016-05-21T05:34:59.581431: train-step 5930, loss 0.147743, acc 0.99
2016-05-21T05:35:05.839074: train-step 5931, loss 0.17832, acc 0.98
2016-05-21T05:35:12.028614: train-step 5932, loss 0.187343, acc 0.98
2016-05-21T05:35:18.259272: train-step 5933, loss 0.202709, acc 0.97
2016-05-21T05:35:24.460079: train-step 5934, loss 0.218554, acc 0.95
2016-05-21T05:35:30.933821: train-step 5935, loss 0.185237, acc 0.99
2016-05-21T05:35:37.356672: train-step 5936, loss 0.196694, acc 0.96
2016-05-21T05:35:43.566392: train-step 5937, loss 0.199472, acc 0.95
2016-05-21T05:35:49.745352: train-step 5938, loss 0.230706, acc 0.93
2016-05-21T05:35:56.032941: train-step 5939, loss 0.214895, acc 0.94
2016-05-21T05:36:02.367232: train-step 5940, loss 0.151291, acc 0.98
2016-05-21T05:36:08.669847: train-step 5941, loss 0.168286, acc 0.97
2016-05-21T05:36:14.893982: train-step 5942, loss 0.252192, acc 0.93
2016-05-21T05:36:21.030886: train-step 5943, loss 0.227087, acc 0.93
2016-05-21T05:36:27.228813: train-step 5944, loss 0.19455, acc 0.95
2016-05-21T05:36:33.459548: train-step 5945, loss 0.222627, acc 0.96
2016-05-21T05:36:39.626899: train-step 5946, loss 0.247468, acc 0.92
2016-05-21T05:36:45.843696: train-step 5947, loss 0.190957, acc 0.96
2016-05-21T05:36:52.138321: train-step 5948, loss 0.194156, acc 0.97
2016-05-21T05:36:58.413284: train-step 5949, loss 0.259867, acc 0.92
2016-05-21T05:37:04.671112: train-step 5950, loss 0.202467, acc 0.95
2016-05-21T05:37:10.862245: train-step 5951, loss 0.211309, acc 0.94
2016-05-21T05:37:17.116710: train-step 5952, loss 0.169274, acc 0.98
2016-05-21T05:37:23.436062: train-step 5953, loss 0.175716, acc 0.98
2016-05-21T05:37:29.625009: train-step 5954, loss 0.182224, acc 0.96
2016-05-21T05:37:35.860690: train-step 5955, loss 0.185458, acc 0.99
2016-05-21T05:37:42.028860: train-step 5956, loss 0.19327, acc 0.96
2016-05-21T05:37:48.318645: train-step 5957, loss 0.190385, acc 0.97
2016-05-21T05:37:54.562093: train-step 5958, loss 0.207457, acc 0.96
2016-05-21T05:38:00.906938: train-step 5959, loss 0.203789, acc 0.93
2016-05-21T05:38:07.469943: train-step 5960, loss 0.174694, acc 0.99
2016-05-21T05:38:13.825249: train-step 5961, loss 0.207456, acc 0.98
2016-05-21T05:38:20.189513: train-step 5962, loss 0.160367, acc 0.98
2016-05-21T05:38:26.517960: train-step 5963, loss 0.193472, acc 0.98
2016-05-21T05:38:32.718137: train-step 5964, loss 0.251631, acc 0.94
2016-05-21T05:38:38.894937: train-step 5965, loss 0.194493, acc 0.98
2016-05-21T05:38:45.036693: train-step 5966, loss 0.176418, acc 0.98
2016-05-21T05:38:51.235963: train-step 5967, loss 0.25499, acc 0.95
2016-05-21T05:38:57.639105: train-step 5968, loss 0.1882, acc 0.95
2016-05-21T05:39:04.110782: train-step 5969, loss 0.235209, acc 0.95
2016-05-21T05:39:10.337812: train-step 5970, loss 0.247139, acc 0.92
2016-05-21T05:39:16.674540: train-step 5971, loss 0.217552, acc 0.95
2016-05-21T05:39:23.229969: train-step 5972, loss 0.224662, acc 0.93
2016-05-21T05:39:29.531254: train-step 5973, loss 0.261111, acc 0.92
2016-05-21T05:39:35.863815: train-step 5974, loss 0.178053, acc 0.97
2016-05-21T05:39:42.040458: train-step 5975, loss 0.168149, acc 0.98
2016-05-21T05:39:48.190280: train-step 5976, loss 0.209066, acc 0.97
2016-05-21T05:39:54.357223: train-step 5977, loss 0.208617, acc 0.97
2016-05-21T05:40:00.535007: train-step 5978, loss 0.213087, acc 0.94
2016-05-21T05:40:06.984332: train-step 5979, loss 0.238255, acc 0.93
2016-05-21T05:40:13.372220: train-step 5980, loss 0.235433, acc 0.94
2016-05-21T05:40:19.626664: train-step 5981, loss 0.160794, acc 0.99
2016-05-21T05:40:25.848003: train-step 5982, loss 0.252178, acc 0.91
2016-05-21T05:40:32.181780: train-step 5983, loss 0.228382, acc 0.95
2016-05-21T05:40:38.514565: train-step 5984, loss 0.196667, acc 0.95
2016-05-21T05:40:44.815962: train-step 5985, loss 0.224913, acc 0.93
2016-05-21T05:40:51.047488: train-step 5986, loss 0.216471, acc 0.96
2016-05-21T05:40:57.623221: train-step 5987, loss 0.161117, acc 0.98
2016-05-21T05:41:03.944225: train-step 5988, loss 0.19287, acc 0.94
2016-05-21T05:41:10.164237: train-step 5989, loss 0.219551, acc 0.93
2016-05-21T05:41:16.678591: train-step 5990, loss 0.212161, acc 0.95
2016-05-21T05:41:23.043903: train-step 5991, loss 0.278056, acc 0.94
2016-05-21T05:41:29.449556: train-step 5992, loss 0.235961, acc 0.94
2016-05-21T05:41:35.665830: train-step 5993, loss 0.240003, acc 0.93
2016-05-21T05:41:41.848316: train-step 5994, loss 0.176676, acc 0.97
2016-05-21T05:41:48.180184: train-step 5995, loss 0.205512, acc 0.97
2016-05-21T05:41:54.397947: train-step 5996, loss 0.16816, acc 0.98
2016-05-21T05:42:00.633265: train-step 5997, loss 0.204784, acc 0.96
2016-05-21T05:42:06.829834: train-step 5998, loss 0.200591, acc 0.95
2016-05-21T05:42:13.067383: train-step 5999, loss 0.197166, acc 0.95
2016-05-21T05:42:19.460579: train-step 6000, loss 0.190268, acc 0.95
epoch number is: 24
2016-05-21T05:42:25.793247: train-step 6001, loss 0.171764, acc 0.98
2016-05-21T05:42:31.932668: train-step 6002, loss 0.179734, acc 0.98
2016-05-21T05:42:38.112060: train-step 6003, loss 0.171802, acc 0.99
2016-05-21T05:42:44.467916: train-step 6004, loss 0.174629, acc 0.97
2016-05-21T05:42:50.750992: train-step 6005, loss 0.188881, acc 0.96
2016-05-21T05:42:57.035543: train-step 6006, loss 0.200112, acc 0.97
2016-05-21T05:43:03.261681: train-step 6007, loss 0.203998, acc 0.97
2016-05-21T05:43:09.478611: train-step 6008, loss 0.175389, acc 0.98
2016-05-21T05:43:15.759892: train-step 6009, loss 0.238556, acc 0.94
2016-05-21T05:43:21.987546: train-step 6010, loss 0.179118, acc 0.98
2016-05-21T05:43:28.143869: train-step 6011, loss 0.227246, acc 0.97
2016-05-21T05:43:34.355985: train-step 6012, loss 0.180737, acc 0.98
2016-05-21T05:43:40.546163: train-step 6013, loss 0.187416, acc 0.98
2016-05-21T05:43:46.739282: train-step 6014, loss 0.22291, acc 0.95
2016-05-21T05:43:52.960983: train-step 6015, loss 0.249834, acc 0.96
2016-05-21T05:43:59.132458: train-step 6016, loss 0.224398, acc 0.97
2016-05-21T05:44:05.371534: train-step 6017, loss 0.20995, acc 0.95
2016-05-21T05:44:11.672387: train-step 6018, loss 0.192056, acc 0.98
2016-05-21T05:44:17.831285: train-step 6019, loss 0.197432, acc 0.98
2016-05-21T05:44:24.318858: train-step 6020, loss 0.214961, acc 0.95
2016-05-21T05:44:30.694250: train-step 6021, loss 0.256156, acc 0.9
2016-05-21T05:44:36.861591: train-step 6022, loss 0.179865, acc 0.98
2016-05-21T05:44:43.358853: train-step 6023, loss 0.195373, acc 0.99
2016-05-21T05:44:49.755144: train-step 6024, loss 0.203581, acc 0.99
2016-05-21T05:44:56.153519: train-step 6025, loss 0.193758, acc 0.97
2016-05-21T05:45:02.506585: train-step 6026, loss 0.167206, acc 0.98
2016-05-21T05:45:08.819519: train-step 6027, loss 0.180091, acc 0.98
2016-05-21T05:45:15.076185: train-step 6028, loss 0.203915, acc 0.95
2016-05-21T05:45:21.682569: train-step 6029, loss 0.173615, acc 0.96
2016-05-21T05:45:28.021725: train-step 6030, loss 0.206976, acc 0.95
2016-05-21T05:45:34.221373: train-step 6031, loss 0.205149, acc 0.96
2016-05-21T05:45:40.408783: train-step 6032, loss 0.220076, acc 0.94
2016-05-21T05:45:46.736832: train-step 6033, loss 0.18364, acc 0.98
2016-05-21T05:45:52.964304: train-step 6034, loss 0.238006, acc 0.91
2016-05-21T05:45:59.460792: train-step 6035, loss 0.212789, acc 0.96
2016-05-21T05:46:05.848459: train-step 6036, loss 0.169422, acc 0.97
2016-05-21T05:46:12.094392: train-step 6037, loss 0.221716, acc 0.96
2016-05-21T05:46:18.271862: train-step 6038, loss 0.187172, acc 0.96
2016-05-21T05:46:24.600772: train-step 6039, loss 0.175395, acc 0.99
2016-05-21T05:46:30.825972: train-step 6040, loss 0.190983, acc 0.97
2016-05-21T05:46:37.089078: train-step 6041, loss 0.194844, acc 0.97
2016-05-21T05:46:43.356311: train-step 6042, loss 0.190775, acc 0.97
2016-05-21T05:46:49.959431: train-step 6043, loss 0.178546, acc 0.97
2016-05-21T05:46:56.295319: train-step 6044, loss 0.214923, acc 0.97
2016-05-21T05:47:02.695544: train-step 6045, loss 0.218171, acc 0.94
2016-05-21T05:47:09.100063: train-step 6046, loss 0.16916, acc 0.98
2016-05-21T05:47:15.458214: train-step 6047, loss 0.193141, acc 0.97
2016-05-21T05:47:21.747703: train-step 6048, loss 0.176211, acc 0.98
2016-05-21T05:47:27.920786: train-step 6049, loss 0.181586, acc 0.96
2016-05-21T05:47:34.069325: train-step 6050, loss 0.169792, acc 0.98
2016-05-21T05:47:40.314922: train-step 6051, loss 0.161486, acc 0.98
2016-05-21T05:47:46.868799: train-step 6052, loss 0.19754, acc 0.97
2016-05-21T05:47:53.213954: train-step 6053, loss 0.174687, acc 0.97
2016-05-21T05:47:59.546077: train-step 6054, loss 0.177757, acc 1
2016-05-21T05:48:05.886305: train-step 6055, loss 0.179224, acc 0.96
2016-05-21T05:48:12.135143: train-step 6056, loss 0.204459, acc 0.96
2016-05-21T05:48:18.334682: train-step 6057, loss 0.205974, acc 0.98
2016-05-21T05:48:24.674055: train-step 6058, loss 0.192528, acc 0.95
2016-05-21T05:48:30.934261: train-step 6059, loss 0.193565, acc 0.93
2016-05-21T05:48:37.253059: train-step 6060, loss 0.242621, acc 0.95
2016-05-21T05:48:43.803714: train-step 6061, loss 0.16777, acc 0.98
2016-05-21T05:48:50.089000: train-step 6062, loss 0.190241, acc 0.96
2016-05-21T05:48:56.351297: train-step 6063, loss 0.17284, acc 0.97
2016-05-21T05:49:02.588413: train-step 6064, loss 0.188307, acc 0.97
2016-05-21T05:49:08.773973: train-step 6065, loss 0.21639, acc 0.94
2016-05-21T05:49:15.169209: train-step 6066, loss 0.144552, acc 0.99
2016-05-21T05:49:21.553307: train-step 6067, loss 0.193321, acc 0.97
2016-05-21T05:49:27.780479: train-step 6068, loss 0.239971, acc 0.95
2016-05-21T05:49:33.969255: train-step 6069, loss 0.231261, acc 0.97
2016-05-21T05:49:40.302323: train-step 6070, loss 0.209602, acc 0.95
2016-05-21T05:49:46.512730: train-step 6071, loss 0.194637, acc 0.96
2016-05-21T05:49:52.772126: train-step 6072, loss 0.21335, acc 0.97
2016-05-21T05:49:58.976684: train-step 6073, loss 0.181868, acc 0.98
2016-05-21T05:50:05.518133: train-step 6074, loss 0.175028, acc 0.97
2016-05-21T05:50:11.824607: train-step 6075, loss 0.185434, acc 0.96
2016-05-21T05:50:18.033563: train-step 6076, loss 0.18839, acc 0.98
2016-05-21T05:50:24.525243: train-step 6077, loss 0.172847, acc 0.99
2016-05-21T05:50:30.917370: train-step 6078, loss 0.145318, acc 1
2016-05-21T05:50:37.125208: train-step 6079, loss 0.168595, acc 0.97
2016-05-21T05:50:43.627152: train-step 6080, loss 0.229564, acc 0.94
2016-05-21T05:50:50.077637: train-step 6081, loss 0.204951, acc 0.98
2016-05-21T05:50:56.242048: train-step 6082, loss 0.213997, acc 0.96
2016-05-21T05:51:02.416090: train-step 6083, loss 0.214442, acc 0.94
2016-05-21T05:51:08.650280: train-step 6084, loss 0.191419, acc 0.97
2016-05-21T05:51:14.870329: train-step 6085, loss 0.182697, acc 0.98
2016-05-21T05:51:21.187352: train-step 6086, loss 0.276559, acc 0.91
2016-05-21T05:51:27.777624: train-step 6087, loss 0.218076, acc 0.93
2016-05-21T05:51:34.022725: train-step 6088, loss 0.15679, acc 0.99
2016-05-21T05:51:40.217123: train-step 6089, loss 0.17162, acc 0.99
2016-05-21T05:51:46.367229: train-step 6090, loss 0.188351, acc 0.98
2016-05-21T05:51:52.922132: train-step 6091, loss 0.223506, acc 0.97
2016-05-21T05:51:59.327986: train-step 6092, loss 0.191657, acc 0.97
2016-05-21T05:52:05.539980: train-step 6093, loss 0.15628, acc 0.98
2016-05-21T05:52:11.723960: train-step 6094, loss 0.191708, acc 0.97
2016-05-21T05:52:18.089509: train-step 6095, loss 0.206465, acc 0.97
2016-05-21T05:52:24.345856: train-step 6096, loss 0.167914, acc 0.98
2016-05-21T05:52:30.546523: train-step 6097, loss 0.211558, acc 0.96
2016-05-21T05:52:36.686878: train-step 6098, loss 0.186183, acc 0.96
2016-05-21T05:52:43.070756: train-step 6099, loss 0.180154, acc 0.99
2016-05-21T05:52:49.536529: train-step 6100, loss 0.163218, acc 0.99
2016-05-21T05:52:55.739272: train-step 6101, loss 0.194675, acc 0.96
2016-05-21T05:53:01.997033: train-step 6102, loss 0.236178, acc 0.93
2016-05-21T05:53:08.211399: train-step 6103, loss 0.153246, acc 0.99
2016-05-21T05:53:14.414488: train-step 6104, loss 0.204744, acc 0.95
2016-05-21T05:53:20.587890: train-step 6105, loss 0.169736, acc 0.99
2016-05-21T05:53:26.815729: train-step 6106, loss 0.186838, acc 0.97
2016-05-21T05:53:33.301237: train-step 6107, loss 0.171813, acc 0.98
2016-05-21T05:53:39.687836: train-step 6108, loss 0.17949, acc 0.98
2016-05-21T05:53:46.062215: train-step 6109, loss 0.166137, acc 0.99
2016-05-21T05:53:52.318632: train-step 6110, loss 0.247151, acc 0.94
2016-05-21T05:53:58.518755: train-step 6111, loss 0.216896, acc 0.96
2016-05-21T05:54:04.815472: train-step 6112, loss 0.24849, acc 0.94
2016-05-21T05:54:11.100624: train-step 6113, loss 0.249376, acc 0.92
2016-05-21T05:54:17.403633: train-step 6114, loss 0.236781, acc 0.95
2016-05-21T05:54:23.770967: train-step 6115, loss 0.228686, acc 0.95
2016-05-21T05:54:30.147974: train-step 6116, loss 0.194064, acc 0.96
2016-05-21T05:54:36.371642: train-step 6117, loss 0.199674, acc 0.96
2016-05-21T05:54:42.548838: train-step 6118, loss 0.239787, acc 0.91
2016-05-21T05:54:48.780316: train-step 6119, loss 0.159553, acc 0.99
2016-05-21T05:54:55.023788: train-step 6120, loss 0.188233, acc 0.98
2016-05-21T05:55:01.268088: train-step 6121, loss 0.194154, acc 0.97
2016-05-21T05:55:07.513892: train-step 6122, loss 0.20962, acc 0.97
2016-05-21T05:55:13.733274: train-step 6123, loss 0.192862, acc 0.99
2016-05-21T05:55:20.008307: train-step 6124, loss 0.204085, acc 0.95
2016-05-21T05:55:26.249553: train-step 6125, loss 0.180317, acc 0.98
2016-05-21T05:55:32.415598: train-step 6126, loss 0.171086, acc 0.99
2016-05-21T05:55:38.625921: train-step 6127, loss 0.17832, acc 0.96
2016-05-21T05:55:44.861688: train-step 6128, loss 0.16201, acc 0.97
2016-05-21T05:55:51.093320: train-step 6129, loss 0.176209, acc 0.99
2016-05-21T05:55:57.355476: train-step 6130, loss 0.19123, acc 0.97
2016-05-21T05:56:03.526359: train-step 6131, loss 0.213011, acc 0.94
2016-05-21T05:56:09.694885: train-step 6132, loss 0.179482, acc 0.98
2016-05-21T05:56:16.164613: train-step 6133, loss 0.213928, acc 0.94
2016-05-21T05:56:22.536348: train-step 6134, loss 0.199531, acc 0.95
2016-05-21T05:56:28.698265: train-step 6135, loss 0.195959, acc 0.98
2016-05-21T05:56:35.176441: train-step 6136, loss 0.167731, acc 0.99
2016-05-21T05:56:41.631180: train-step 6137, loss 0.206281, acc 0.96
2016-05-21T05:56:47.851453: train-step 6138, loss 0.229565, acc 0.94
2016-05-21T05:56:54.225834: train-step 6139, loss 0.189603, acc 0.98
2016-05-21T05:57:00.813475: train-step 6140, loss 0.193199, acc 0.95
2016-05-21T05:57:07.099346: train-step 6141, loss 0.222609, acc 0.95
2016-05-21T05:57:13.360939: train-step 6142, loss 0.220347, acc 0.95
2016-05-21T05:57:19.559518: train-step 6143, loss 0.16717, acc 0.98
2016-05-21T05:57:26.081873: train-step 6144, loss 0.191868, acc 0.96
2016-05-21T05:57:32.372746: train-step 6145, loss 0.190635, acc 0.96
2016-05-21T05:57:38.593839: train-step 6146, loss 0.210348, acc 0.96
2016-05-21T05:57:44.794909: train-step 6147, loss 0.182371, acc 0.99
2016-05-21T05:57:50.949053: train-step 6148, loss 0.168216, acc 0.99
2016-05-21T05:57:57.123507: train-step 6149, loss 0.199802, acc 0.96
2016-05-21T05:58:03.376295: train-step 6150, loss 0.1676, acc 0.99
2016-05-21T05:58:09.629283: train-step 6151, loss 0.167061, acc 0.96
2016-05-21T05:58:15.855242: train-step 6152, loss 0.227519, acc 0.95
2016-05-21T05:58:22.019806: train-step 6153, loss 0.205047, acc 0.95
2016-05-21T05:58:28.232911: train-step 6154, loss 0.217369, acc 0.95
2016-05-21T05:58:34.601086: train-step 6155, loss 0.216373, acc 0.96
2016-05-21T05:58:40.885218: train-step 6156, loss 0.194273, acc 0.98
2016-05-21T05:58:47.091388: train-step 6157, loss 0.229919, acc 0.94
2016-05-21T05:58:53.443258: train-step 6158, loss 0.182049, acc 0.97
2016-05-21T05:58:59.647991: train-step 6159, loss 0.246002, acc 0.93
2016-05-21T05:59:05.978744: train-step 6160, loss 0.222542, acc 0.92
2016-05-21T05:59:12.513280: train-step 6161, loss 0.234491, acc 0.92
2016-05-21T05:59:18.867822: train-step 6162, loss 0.228781, acc 0.92
2016-05-21T05:59:25.165336: train-step 6163, loss 0.215252, acc 0.96
2016-05-21T05:59:31.318337: train-step 6164, loss 0.21416, acc 0.96
2016-05-21T05:59:37.557482: train-step 6165, loss 0.162801, acc 1
2016-05-21T05:59:43.744592: train-step 6166, loss 0.205022, acc 0.95
2016-05-21T05:59:49.899462: train-step 6167, loss 0.196463, acc 0.97
2016-05-21T05:59:56.415475: train-step 6168, loss 0.214086, acc 0.96
2016-05-21T06:00:02.777153: train-step 6169, loss 0.195415, acc 0.96
2016-05-21T06:00:09.000508: train-step 6170, loss 0.163878, acc 0.98
2016-05-21T06:00:15.220254: train-step 6171, loss 0.201708, acc 0.97
2016-05-21T06:00:21.547206: train-step 6172, loss 0.170813, acc 0.96
2016-05-21T06:00:27.835315: train-step 6173, loss 0.215569, acc 0.95
2016-05-21T06:00:34.061537: train-step 6174, loss 0.218311, acc 0.94
2016-05-21T06:00:40.251816: train-step 6175, loss 0.202405, acc 0.95
2016-05-21T06:00:46.518536: train-step 6176, loss 0.227686, acc 0.97
2016-05-21T06:00:52.831687: train-step 6177, loss 0.158084, acc 0.99
2016-05-21T06:00:58.988802: train-step 6178, loss 0.154697, acc 0.98
2016-05-21T06:01:05.172962: train-step 6179, loss 0.190506, acc 0.96
2016-05-21T06:01:11.509908: train-step 6180, loss 0.186791, acc 0.96
2016-05-21T06:01:17.765628: train-step 6181, loss 0.22576, acc 0.96
2016-05-21T06:01:23.974024: train-step 6182, loss 0.226063, acc 0.92
2016-05-21T06:01:30.332888: train-step 6183, loss 0.183514, acc 0.95
2016-05-21T06:01:36.910244: train-step 6184, loss 0.221902, acc 0.94
2016-05-21T06:01:43.184477: train-step 6185, loss 0.236619, acc 0.95
2016-05-21T06:01:49.494992: train-step 6186, loss 0.206709, acc 0.96
2016-05-21T06:01:56.038468: train-step 6187, loss 0.220299, acc 0.97
2016-05-21T06:02:02.378667: train-step 6188, loss 0.223082, acc 0.94
2016-05-21T06:02:08.709130: train-step 6189, loss 0.193043, acc 0.97
2016-05-21T06:02:14.936992: train-step 6190, loss 0.215172, acc 0.93
2016-05-21T06:02:21.501911: train-step 6191, loss 0.237707, acc 0.94
2016-05-21T06:02:27.878130: train-step 6192, loss 0.187486, acc 0.99
2016-05-21T06:02:34.236005: train-step 6193, loss 0.173417, acc 0.97
2016-05-21T06:02:40.558949: train-step 6194, loss 0.218481, acc 0.96
2016-05-21T06:02:47.381182: train-step 6195, loss 0.174619, acc 0.98
2016-05-21T06:02:53.894702: train-step 6196, loss 0.249507, acc 0.94
2016-05-21T06:03:00.234826: train-step 6197, loss 0.191798, acc 0.96
2016-05-21T06:03:06.576536: train-step 6198, loss 0.204193, acc 0.96
2016-05-21T06:03:12.951867: train-step 6199, loss 0.226163, acc 0.94
2016-05-21T06:03:19.174009: train-step 6200, loss 0.179206, acc 0.97
2016-05-21T06:03:25.711043: train-step 6201, loss 0.282271, acc 0.94
2016-05-21T06:03:32.091143: train-step 6202, loss 0.183798, acc 0.98
2016-05-21T06:03:38.285483: train-step 6203, loss 0.190861, acc 0.97
2016-05-21T06:03:44.543452: train-step 6204, loss 0.193274, acc 0.97
2016-05-21T06:03:50.943277: train-step 6205, loss 0.231536, acc 0.92
2016-05-21T06:03:57.461382: train-step 6206, loss 0.176482, acc 0.98
2016-05-21T06:04:03.718343: train-step 6207, loss 0.203484, acc 0.97
2016-05-21T06:04:10.076253: train-step 6208, loss 0.188762, acc 0.97
2016-05-21T06:04:16.626284: train-step 6209, loss 0.160489, acc 0.97
2016-05-21T06:04:23.238279: train-step 6210, loss 0.207784, acc 0.96
2016-05-21T06:04:29.533472: train-step 6211, loss 0.193722, acc 0.97
2016-05-21T06:04:35.924876: train-step 6212, loss 0.192217, acc 0.96
2016-05-21T06:04:42.287197: train-step 6213, loss 0.209812, acc 0.97
2016-05-21T06:04:48.556966: train-step 6214, loss 0.2369, acc 0.94
2016-05-21T06:04:54.963371: train-step 6215, loss 0.163334, acc 0.98
2016-05-21T06:05:01.507727: train-step 6216, loss 0.252852, acc 0.92
2016-05-21T06:05:07.690812: train-step 6217, loss 0.19167, acc 0.99
2016-05-21T06:05:13.944949: train-step 6218, loss 0.176487, acc 0.98
2016-05-21T06:05:20.137201: train-step 6219, loss 0.182776, acc 0.98
2016-05-21T06:05:26.373276: train-step 6220, loss 0.261605, acc 0.91
2016-05-21T06:05:32.689635: train-step 6221, loss 0.188622, acc 0.95
2016-05-21T06:05:39.099632: train-step 6222, loss 0.248007, acc 0.91
2016-05-21T06:05:45.296463: train-step 6223, loss 0.177329, acc 0.97
2016-05-21T06:05:51.532475: train-step 6224, loss 0.258592, acc 0.9
2016-05-21T06:05:57.764085: train-step 6225, loss 0.234308, acc 0.93
2016-05-21T06:06:04.291199: train-step 6226, loss 0.205213, acc 0.95
2016-05-21T06:06:10.643159: train-step 6227, loss 0.187772, acc 0.99
2016-05-21T06:06:16.998675: train-step 6228, loss 0.164778, acc 0.97
2016-05-21T06:06:23.233099: train-step 6229, loss 0.22351, acc 0.94
2016-05-21T06:06:29.777243: train-step 6230, loss 0.194874, acc 0.98
2016-05-21T06:06:36.128251: train-step 6231, loss 0.24123, acc 0.94
2016-05-21T06:06:42.481217: train-step 6232, loss 0.211047, acc 0.94
2016-05-21T06:06:48.728652: train-step 6233, loss 0.223255, acc 0.94
2016-05-21T06:06:55.093558: train-step 6234, loss 0.171635, acc 0.99
2016-05-21T06:07:01.638468: train-step 6235, loss 0.227363, acc 0.96
2016-05-21T06:07:07.938424: train-step 6236, loss 0.191218, acc 0.97
2016-05-21T06:07:14.280233: train-step 6237, loss 0.206094, acc 0.96
2016-05-21T06:07:20.476012: train-step 6238, loss 0.160589, acc 0.98
2016-05-21T06:07:27.021490: train-step 6239, loss 0.215017, acc 0.95
2016-05-21T06:07:33.390380: train-step 6240, loss 0.193855, acc 0.98
2016-05-21T06:07:39.584109: train-step 6241, loss 0.228974, acc 0.96
2016-05-21T06:07:45.815552: train-step 6242, loss 0.207291, acc 0.97
2016-05-21T06:07:52.232539: train-step 6243, loss 0.226702, acc 0.97
2016-05-21T06:07:58.638135: train-step 6244, loss 0.170023, acc 0.98
2016-05-21T06:08:04.899782: train-step 6245, loss 0.175206, acc 0.98
2016-05-21T06:08:11.127570: train-step 6246, loss 0.195266, acc 0.96
2016-05-21T06:08:17.358995: train-step 6247, loss 0.156706, acc 0.99
2016-05-21T06:08:23.559955: train-step 6248, loss 0.170216, acc 0.97
2016-05-21T06:08:29.805802: train-step 6249, loss 0.190304, acc 0.96
2016-05-21T06:08:36.256351: train-step 6250, loss 0.181676, acc 0.97
epoch number is: 25
2016-05-21T06:08:42.913425: train-step 6251, loss 0.18605, acc 0.97
2016-05-21T06:08:49.104671: train-step 6252, loss 0.190598, acc 0.97
2016-05-21T06:08:55.469657: train-step 6253, loss 0.181261, acc 0.97
2016-05-21T06:09:01.982647: train-step 6254, loss 0.177783, acc 0.97
2016-05-21T06:09:08.353446: train-step 6255, loss 0.228207, acc 0.94
2016-05-21T06:09:14.707682: train-step 6256, loss 0.203081, acc 0.97
2016-05-21T06:09:21.506584: train-step 6257, loss 0.161478, acc 0.99
2016-05-21T06:09:28.146356: train-step 6258, loss 0.192141, acc 0.96
2016-05-21T06:09:34.508857: train-step 6259, loss 0.196414, acc 0.96
2016-05-21T06:09:40.852890: train-step 6260, loss 0.154246, acc 0.98
2016-05-21T06:09:47.222577: train-step 6261, loss 0.178111, acc 0.97
2016-05-21T06:09:53.613646: train-step 6262, loss 0.174889, acc 0.98
2016-05-21T06:09:59.865072: train-step 6263, loss 0.173464, acc 0.99
2016-05-21T06:10:06.101871: train-step 6264, loss 0.194248, acc 0.95
2016-05-21T06:10:12.313305: train-step 6265, loss 0.208804, acc 0.97
2016-05-21T06:10:18.863577: train-step 6266, loss 0.159448, acc 0.99
2016-05-21T06:10:25.250504: train-step 6267, loss 0.197783, acc 0.96
2016-05-21T06:10:31.483115: train-step 6268, loss 0.202248, acc 0.97
2016-05-21T06:10:37.682601: train-step 6269, loss 0.224654, acc 0.96
2016-05-21T06:10:44.039815: train-step 6270, loss 0.241647, acc 0.95
2016-05-21T06:10:50.220639: train-step 6271, loss 0.171888, acc 0.98
2016-05-21T06:10:56.487077: train-step 6272, loss 0.187499, acc 0.95
2016-05-21T06:11:02.686262: train-step 6273, loss 0.182516, acc 0.96
2016-05-21T06:11:08.897259: train-step 6274, loss 0.215596, acc 0.96
2016-05-21T06:11:15.094347: train-step 6275, loss 0.208267, acc 0.95
2016-05-21T06:11:21.448640: train-step 6276, loss 0.154561, acc 0.98
2016-05-21T06:11:27.613133: train-step 6277, loss 0.214684, acc 0.95
2016-05-21T06:11:34.119857: train-step 6278, loss 0.184584, acc 0.96
2016-05-21T06:11:40.448246: train-step 6279, loss 0.20036, acc 0.97
2016-05-21T06:11:46.604308: train-step 6280, loss 0.223752, acc 0.96
2016-05-21T06:11:52.744670: train-step 6281, loss 0.210017, acc 0.95
2016-05-21T06:11:59.133890: train-step 6282, loss 0.138833, acc 1
2016-05-21T06:12:05.345064: train-step 6283, loss 0.186492, acc 0.96
2016-05-21T06:12:11.523132: train-step 6284, loss 0.172248, acc 0.98
2016-05-21T06:12:17.867310: train-step 6285, loss 0.188424, acc 0.97
2016-05-21T06:12:24.183537: train-step 6286, loss 0.17082, acc 0.99
2016-05-21T06:12:30.475040: train-step 6287, loss 0.161062, acc 0.98
2016-05-21T06:12:36.675829: train-step 6288, loss 0.207082, acc 0.97
2016-05-21T06:12:43.269127: train-step 6289, loss 0.199729, acc 0.96
2016-05-21T06:12:49.589492: train-step 6290, loss 0.193709, acc 0.95
2016-05-21T06:12:55.839430: train-step 6291, loss 0.195683, acc 0.98
2016-05-21T06:13:02.376085: train-step 6292, loss 0.226024, acc 0.93
2016-05-21T06:13:08.691334: train-step 6293, loss 0.189871, acc 0.97
2016-05-21T06:13:14.871383: train-step 6294, loss 0.217439, acc 0.96
2016-05-21T06:13:21.146507: train-step 6295, loss 0.195009, acc 0.98
2016-05-21T06:13:27.413207: train-step 6296, loss 0.191387, acc 0.96
2016-05-21T06:13:33.678966: train-step 6297, loss 0.224855, acc 0.94
2016-05-21T06:13:39.931015: train-step 6298, loss 0.199582, acc 0.95
2016-05-21T06:13:46.470383: train-step 6299, loss 0.186471, acc 0.95
2016-05-21T06:13:52.869389: train-step 6300, loss 0.183353, acc 0.97
2016-05-21T06:13:59.276479: train-step 6301, loss 0.234002, acc 0.95
2016-05-21T06:14:05.508961: train-step 6302, loss 0.188477, acc 0.97
2016-05-21T06:14:11.716673: train-step 6303, loss 0.214634, acc 0.96
2016-05-21T06:14:18.014637: train-step 6304, loss 0.209369, acc 0.94
2016-05-21T06:14:24.356092: train-step 6305, loss 0.216009, acc 0.95
2016-05-21T06:14:30.697739: train-step 6306, loss 0.177353, acc 0.98
2016-05-21T06:14:37.064733: train-step 6307, loss 0.19783, acc 0.95
2016-05-21T06:14:43.451399: train-step 6308, loss 0.170844, acc 0.98
2016-05-21T06:14:49.701961: train-step 6309, loss 0.189502, acc 0.98
2016-05-21T06:14:55.894242: train-step 6310, loss 0.171978, acc 0.98
2016-05-21T06:15:02.119587: train-step 6311, loss 0.167974, acc 0.99
2016-05-21T06:15:08.595461: train-step 6312, loss 0.195771, acc 0.97
2016-05-21T06:15:15.058917: train-step 6313, loss 0.265458, acc 0.94
2016-05-21T06:15:21.313304: train-step 6314, loss 0.243147, acc 0.94
2016-05-21T06:15:27.550285: train-step 6315, loss 0.205604, acc 0.94
2016-05-21T06:15:33.760869: train-step 6316, loss 0.187522, acc 0.98
2016-05-21T06:15:39.983207: train-step 6317, loss 0.191807, acc 0.97
2016-05-21T06:15:46.525570: train-step 6318, loss 0.169398, acc 0.99
2016-05-21T06:15:52.852692: train-step 6319, loss 0.211118, acc 0.96
2016-05-21T06:15:59.252223: train-step 6320, loss 0.147614, acc 0.99
2016-05-21T06:16:05.473697: train-step 6321, loss 0.148952, acc 0.99
2016-05-21T06:16:11.693176: train-step 6322, loss 0.186169, acc 0.98
2016-05-21T06:16:18.023747: train-step 6323, loss 0.187498, acc 0.98
2016-05-21T06:16:24.270753: train-step 6324, loss 0.187447, acc 0.97
2016-05-21T06:16:30.474794: train-step 6325, loss 0.225083, acc 0.95
2016-05-21T06:16:36.668235: train-step 6326, loss 0.163311, acc 0.98
2016-05-21T06:16:42.888327: train-step 6327, loss 0.210781, acc 0.95
2016-05-21T06:16:49.099568: train-step 6328, loss 0.163438, acc 0.99
2016-05-21T06:16:55.454552: train-step 6329, loss 0.172802, acc 0.96
2016-05-21T06:17:01.698260: train-step 6330, loss 0.144906, acc 1
2016-05-21T06:17:07.917853: train-step 6331, loss 0.188258, acc 0.97
2016-05-21T06:17:14.206774: train-step 6332, loss 0.172968, acc 0.96
2016-05-21T06:17:20.462959: train-step 6333, loss 0.170296, acc 0.97
2016-05-21T06:17:26.655462: train-step 6334, loss 0.220043, acc 0.94
2016-05-21T06:17:32.873702: train-step 6335, loss 0.214389, acc 0.95
2016-05-21T06:17:39.053501: train-step 6336, loss 0.17911, acc 0.99
2016-05-21T06:17:45.306769: train-step 6337, loss 0.190409, acc 0.96
2016-05-21T06:17:51.567584: train-step 6338, loss 0.221952, acc 0.94
2016-05-21T06:17:57.796751: train-step 6339, loss 0.156937, acc 0.99
2016-05-21T06:18:03.961193: train-step 6340, loss 0.179771, acc 0.96
2016-05-21T06:18:10.194525: train-step 6341, loss 0.160695, acc 0.96
2016-05-21T06:18:16.361954: train-step 6342, loss 0.236273, acc 0.95
2016-05-21T06:18:22.573347: train-step 6343, loss 0.177632, acc 0.97
2016-05-21T06:18:28.778979: train-step 6344, loss 0.232673, acc 0.95
2016-05-21T06:18:35.129165: train-step 6345, loss 0.234443, acc 0.95
2016-05-21T06:18:41.685278: train-step 6346, loss 0.205109, acc 0.95
2016-05-21T06:18:47.921781: train-step 6347, loss 0.187512, acc 0.98
2016-05-21T06:18:54.175005: train-step 6348, loss 0.215481, acc 0.93
2016-05-21T06:19:00.358036: train-step 6349, loss 0.193232, acc 0.98
2016-05-21T06:19:06.483332: train-step 6350, loss 0.199257, acc 0.97
2016-05-21T06:19:12.849734: train-step 6351, loss 0.190718, acc 0.96
2016-05-21T06:19:19.227278: train-step 6352, loss 0.209787, acc 0.94
2016-05-21T06:19:25.453401: train-step 6353, loss 0.235941, acc 0.95
2016-05-21T06:19:31.686589: train-step 6354, loss 0.144372, acc 0.99
2016-05-21T06:19:37.940264: train-step 6355, loss 0.173597, acc 0.98
2016-05-21T06:19:44.183141: train-step 6356, loss 0.210267, acc 0.97
2016-05-21T06:19:50.687590: train-step 6357, loss 0.223478, acc 0.98
2016-05-21T06:19:57.056720: train-step 6358, loss 0.187036, acc 0.97
2016-05-21T06:20:03.403618: train-step 6359, loss 0.220199, acc 0.96
2016-05-21T06:20:09.647611: train-step 6360, loss 0.216017, acc 0.94
2016-05-21T06:20:16.048878: train-step 6361, loss 0.17888, acc 0.97
2016-05-21T06:20:22.561837: train-step 6362, loss 0.181601, acc 0.97
2016-05-21T06:20:28.731903: train-step 6363, loss 0.172436, acc 0.98
2016-05-21T06:20:34.924098: train-step 6364, loss 0.144469, acc 1
2016-05-21T06:20:41.158831: train-step 6365, loss 0.21048, acc 0.96
2016-05-21T06:20:47.383192: train-step 6366, loss 0.20458, acc 0.94
2016-05-21T06:20:53.702484: train-step 6367, loss 0.17373, acc 0.98
2016-05-21T06:21:00.074569: train-step 6368, loss 0.205985, acc 0.95
2016-05-21T06:21:06.461197: train-step 6369, loss 0.198106, acc 0.95
2016-05-21T06:21:12.839975: train-step 6370, loss 0.223863, acc 0.95
2016-05-21T06:21:19.134669: train-step 6371, loss 0.21224, acc 0.94
2016-05-21T06:21:25.336882: train-step 6372, loss 0.206365, acc 0.94
2016-05-21T06:21:31.945970: train-step 6373, loss 0.225221, acc 0.94
2016-05-21T06:21:38.231867: train-step 6374, loss 0.166402, acc 0.99
2016-05-21T06:21:44.466502: train-step 6375, loss 0.19302, acc 0.97
2016-05-21T06:21:51.019865: train-step 6376, loss 0.198583, acc 0.95
2016-05-21T06:21:57.380101: train-step 6377, loss 0.169546, acc 0.96
2016-05-21T06:22:03.769681: train-step 6378, loss 0.169242, acc 0.99
2016-05-21T06:22:10.139614: train-step 6379, loss 0.169099, acc 0.99
2016-05-21T06:22:16.499986: train-step 6380, loss 0.165661, acc 0.98
2016-05-21T06:22:22.820964: train-step 6381, loss 0.175602, acc 0.95
2016-05-21T06:22:29.045454: train-step 6382, loss 0.149595, acc 0.99
2016-05-21T06:22:35.242414: train-step 6383, loss 0.189019, acc 0.98
2016-05-21T06:22:41.567855: train-step 6384, loss 0.219673, acc 0.94
2016-05-21T06:22:47.941110: train-step 6385, loss 0.171113, acc 0.98
2016-05-21T06:22:54.188307: train-step 6386, loss 0.196384, acc 0.97
2016-05-21T06:23:00.398152: train-step 6387, loss 0.178455, acc 0.97
2016-05-21T06:23:06.753673: train-step 6388, loss 0.1801, acc 0.99
2016-05-21T06:23:13.058995: train-step 6389, loss 0.272515, acc 0.94
2016-05-21T06:23:19.412892: train-step 6390, loss 0.183139, acc 0.97
2016-05-21T06:23:25.769044: train-step 6391, loss 0.181164, acc 0.97
2016-05-21T06:23:32.158189: train-step 6392, loss 0.218339, acc 0.95
2016-05-21T06:23:38.530484: train-step 6393, loss 0.19529, acc 0.97
2016-05-21T06:23:44.880907: train-step 6394, loss 0.226805, acc 0.96
2016-05-21T06:23:51.172636: train-step 6395, loss 0.236708, acc 0.92
2016-05-21T06:23:57.536175: train-step 6396, loss 0.175741, acc 0.98
2016-05-21T06:24:03.869006: train-step 6397, loss 0.195881, acc 0.97
2016-05-21T06:24:10.111724: train-step 6398, loss 0.208808, acc 0.96
2016-05-21T06:24:16.268671: train-step 6399, loss 0.191483, acc 0.97
2016-05-21T06:24:22.561963: train-step 6400, loss 0.19589, acc 0.97
2016-05-21T06:24:28.781563: train-step 6401, loss 0.188524, acc 0.97
2016-05-21T06:24:35.081520: train-step 6402, loss 0.226036, acc 0.95
2016-05-21T06:24:41.596181: train-step 6403, loss 0.203972, acc 0.93
2016-05-21T06:24:47.888096: train-step 6404, loss 0.197526, acc 0.96
2016-05-21T06:24:54.058688: train-step 6405, loss 0.164501, acc 0.99
2016-05-21T06:25:00.223875: train-step 6406, loss 0.205401, acc 0.95
2016-05-21T06:25:06.433721: train-step 6407, loss 0.169939, acc 0.97
2016-05-21T06:25:12.988898: train-step 6408, loss 0.150028, acc 0.99
2016-05-21T06:25:19.254379: train-step 6409, loss 0.181247, acc 0.98
2016-05-21T06:25:25.419178: train-step 6410, loss 0.173777, acc 0.98
2016-05-21T06:25:32.002493: train-step 6411, loss 0.230277, acc 0.93
2016-05-21T06:25:38.366970: train-step 6412, loss 0.183528, acc 0.98
2016-05-21T06:25:44.520059: train-step 6413, loss 0.187303, acc 0.98
2016-05-21T06:25:50.745300: train-step 6414, loss 0.184292, acc 0.96
2016-05-21T06:25:56.970064: train-step 6415, loss 0.243611, acc 0.97
2016-05-21T06:26:03.327548: train-step 6416, loss 0.20317, acc 0.96
2016-05-21T06:26:09.589589: train-step 6417, loss 0.206102, acc 0.98
2016-05-21T06:26:15.908578: train-step 6418, loss 0.204443, acc 0.96
2016-05-21T06:26:22.470168: train-step 6419, loss 0.170599, acc 0.98
2016-05-21T06:26:28.815759: train-step 6420, loss 0.178142, acc 1
2016-05-21T06:26:35.133452: train-step 6421, loss 0.180597, acc 0.97
2016-05-21T06:26:41.388386: train-step 6422, loss 0.177967, acc 0.97
2016-05-21T06:26:47.922656: train-step 6423, loss 0.195703, acc 0.98
2016-05-21T06:26:54.295624: train-step 6424, loss 0.155836, acc 0.98
2016-05-21T06:27:00.569616: train-step 6425, loss 0.239956, acc 0.94
2016-05-21T06:27:06.810868: train-step 6426, loss 0.197549, acc 0.97
2016-05-21T06:27:13.212676: train-step 6427, loss 0.197065, acc 0.96
2016-05-21T06:27:19.689778: train-step 6428, loss 0.214461, acc 0.96
2016-05-21T06:27:26.043597: train-step 6429, loss 0.204868, acc 0.95
2016-05-21T06:27:32.962907: train-step 6430, loss 0.2336, acc 0.94
2016-05-21T06:27:39.431627: train-step 6431, loss 0.214727, acc 0.96
2016-05-21T06:27:45.653085: train-step 6432, loss 0.23125, acc 0.95
2016-05-21T06:27:52.044370: train-step 6433, loss 0.196483, acc 0.96
2016-05-21T06:27:58.594680: train-step 6434, loss 0.204311, acc 0.97
2016-05-21T06:28:04.909066: train-step 6435, loss 0.220124, acc 0.95
2016-05-21T06:28:11.236416: train-step 6436, loss 0.182776, acc 0.97
2016-05-21T06:28:17.395887: train-step 6437, loss 0.216233, acc 0.94
2016-05-21T06:28:23.987997: train-step 6438, loss 0.160823, acc 0.97
2016-05-21T06:28:30.345654: train-step 6439, loss 0.167707, acc 0.99
2016-05-21T06:28:36.585706: train-step 6440, loss 0.201244, acc 0.96
2016-05-21T06:28:43.149436: train-step 6441, loss 0.218308, acc 0.93
2016-05-21T06:28:49.724225: train-step 6442, loss 0.160194, acc 0.97
2016-05-21T06:28:56.148351: train-step 6443, loss 0.197167, acc 0.97
2016-05-21T06:29:02.410728: train-step 6444, loss 0.194774, acc 0.98
2016-05-21T06:29:08.527948: train-step 6445, loss 0.177196, acc 0.97
2016-05-21T06:29:14.873891: train-step 6446, loss 0.148673, acc 0.99
2016-05-21T06:29:21.177553: train-step 6447, loss 0.17729, acc 0.98
2016-05-21T06:29:27.472478: train-step 6448, loss 0.240587, acc 0.91
2016-05-21T06:29:33.617628: train-step 6449, loss 0.244029, acc 0.95
2016-05-21T06:29:39.864066: train-step 6450, loss 0.200641, acc 0.94
2016-05-21T06:29:46.147272: train-step 6451, loss 0.155098, acc 0.99
2016-05-21T06:29:52.531078: train-step 6452, loss 0.188499, acc 0.94
2016-05-21T06:29:58.781392: train-step 6453, loss 0.203568, acc 0.97
2016-05-21T06:30:05.191265: train-step 6454, loss 0.216463, acc 0.95
2016-05-21T06:30:11.619168: train-step 6455, loss 0.188687, acc 0.96
2016-05-21T06:30:17.998849: train-step 6456, loss 0.21062, acc 0.96
2016-05-21T06:30:24.272729: train-step 6457, loss 0.268072, acc 0.92
2016-05-21T06:30:30.433773: train-step 6458, loss 0.224825, acc 0.97
2016-05-21T06:30:36.634494: train-step 6459, loss 0.172046, acc 0.97
2016-05-21T06:30:42.902348: train-step 6460, loss 0.180565, acc 0.97
2016-05-21T06:30:49.066251: train-step 6461, loss 0.283576, acc 0.92
2016-05-21T06:30:55.266561: train-step 6462, loss 0.239809, acc 0.93
2016-05-21T06:31:01.659203: train-step 6463, loss 0.161494, acc 0.99
2016-05-21T06:31:08.042507: train-step 6464, loss 0.235434, acc 0.93
2016-05-21T06:31:14.209631: train-step 6465, loss 0.237753, acc 0.95
2016-05-21T06:31:20.469522: train-step 6466, loss 0.188194, acc 0.98
2016-05-21T06:31:26.660635: train-step 6467, loss 0.195931, acc 0.97
2016-05-21T06:31:32.859484: train-step 6468, loss 0.21785, acc 0.95
2016-05-21T06:31:39.026998: train-step 6469, loss 0.224087, acc 0.95
2016-05-21T06:31:45.393567: train-step 6470, loss 0.192661, acc 0.96
2016-05-21T06:31:51.733035: train-step 6471, loss 0.146695, acc 0.98
2016-05-21T06:31:57.985250: train-step 6472, loss 0.238169, acc 0.94
2016-05-21T06:32:04.385093: train-step 6473, loss 0.168222, acc 0.98
2016-05-21T06:32:10.927968: train-step 6474, loss 0.215728, acc 0.95
2016-05-21T06:32:17.226596: train-step 6475, loss 0.190782, acc 0.97
2016-05-21T06:32:23.498834: train-step 6476, loss 0.220748, acc 0.96
2016-05-21T06:32:29.655586: train-step 6477, loss 0.188026, acc 0.96
2016-05-21T06:32:36.238111: train-step 6478, loss 0.199696, acc 0.96
2016-05-21T06:32:42.603610: train-step 6479, loss 0.204822, acc 0.96
2016-05-21T06:32:48.833119: train-step 6480, loss 0.209336, acc 0.94
2016-05-21T06:32:55.337344: train-step 6481, loss 0.169843, acc 1
2016-05-21T06:33:01.708986: train-step 6482, loss 0.183111, acc 0.97
2016-05-21T06:33:08.038639: train-step 6483, loss 0.261862, acc 0.93
2016-05-21T06:33:14.248824: train-step 6484, loss 0.19227, acc 0.95
2016-05-21T06:33:20.511541: train-step 6485, loss 0.169352, acc 0.97
2016-05-21T06:33:26.786479: train-step 6486, loss 0.157419, acc 0.99
2016-05-21T06:33:33.043821: train-step 6487, loss 0.159469, acc 0.99
2016-05-21T06:33:39.603675: train-step 6488, loss 0.19913, acc 0.94
2016-05-21T06:33:45.966262: train-step 6489, loss 0.192421, acc 0.95
2016-05-21T06:33:52.336330: train-step 6490, loss 0.160534, acc 1
2016-05-21T06:33:58.588845: train-step 6491, loss 0.212989, acc 0.94
2016-05-21T06:34:04.781147: train-step 6492, loss 0.197359, acc 0.96
2016-05-21T06:34:11.140381: train-step 6493, loss 0.200277, acc 0.97
2016-05-21T06:34:17.338695: train-step 6494, loss 0.202562, acc 0.96
2016-05-21T06:34:23.591984: train-step 6495, loss 0.208358, acc 0.98
2016-05-21T06:34:29.833371: train-step 6496, loss 0.191383, acc 0.96
2016-05-21T06:34:36.415383: train-step 6497, loss 0.219824, acc 0.94
2016-05-21T06:34:42.744851: train-step 6498, loss 0.191535, acc 0.95
2016-05-21T06:34:48.942679: train-step 6499, loss 0.195437, acc 0.96
2016-05-21T06:34:55.177119: train-step 6500, loss 0.216905, acc 0.93
epoch number is: 26
2016-05-21T06:35:01.753475: train-step 6501, loss 0.216113, acc 0.95
2016-05-21T06:35:08.142936: train-step 6502, loss 0.180674, acc 0.96
2016-05-21T06:35:14.519988: train-step 6503, loss 0.188022, acc 0.96
2016-05-21T06:35:21.341674: train-step 6504, loss 0.186681, acc 0.98
2016-05-21T06:35:27.822466: train-step 6505, loss 0.170746, acc 0.98
2016-05-21T06:35:34.156025: train-step 6506, loss 0.196276, acc 0.98
2016-05-21T06:35:40.456301: train-step 6507, loss 0.223285, acc 0.95
2016-05-21T06:35:46.810520: train-step 6508, loss 0.220443, acc 0.94
2016-05-21T06:35:53.013429: train-step 6509, loss 0.17092, acc 0.99
2016-05-21T06:35:59.218359: train-step 6510, loss 0.232287, acc 0.95
2016-05-21T06:36:05.436608: train-step 6511, loss 0.194568, acc 0.96
2016-05-21T06:36:11.761836: train-step 6512, loss 0.168006, acc 1
2016-05-21T06:36:18.324377: train-step 6513, loss 0.197723, acc 0.98
2016-05-21T06:36:24.661743: train-step 6514, loss 0.218032, acc 0.98
2016-05-21T06:36:30.987538: train-step 6515, loss 0.170852, acc 0.99
2016-05-21T06:36:37.210159: train-step 6516, loss 0.176786, acc 0.98
2016-05-21T06:36:43.743650: train-step 6517, loss 0.17054, acc 0.99
2016-05-21T06:36:50.096429: train-step 6518, loss 0.189164, acc 0.96
2016-05-21T06:36:56.457250: train-step 6519, loss 0.182663, acc 0.99
2016-05-21T06:37:02.706344: train-step 6520, loss 0.24662, acc 0.95
2016-05-21T06:37:09.081544: train-step 6521, loss 0.237098, acc 0.94
2016-05-21T06:37:15.582318: train-step 6522, loss 0.142349, acc 1
2016-05-21T06:37:21.821625: train-step 6523, loss 0.166146, acc 0.99
2016-05-21T06:37:28.142434: train-step 6524, loss 0.187757, acc 0.97
2016-05-21T06:37:34.705325: train-step 6525, loss 0.186426, acc 0.97
2016-05-21T06:37:41.266495: train-step 6526, loss 0.197855, acc 0.95
2016-05-21T06:37:47.578103: train-step 6527, loss 0.185806, acc 0.96
2016-05-21T06:37:53.824346: train-step 6528, loss 0.15922, acc 0.99
2016-05-21T06:38:00.017585: train-step 6529, loss 0.179224, acc 0.97
2016-05-21T06:38:06.217148: train-step 6530, loss 0.153254, acc 0.99
2016-05-21T06:38:12.566312: train-step 6531, loss 0.176486, acc 0.98
2016-05-21T06:38:19.083525: train-step 6532, loss 0.189967, acc 0.98
2016-05-21T06:38:25.387654: train-step 6533, loss 0.213, acc 0.95
2016-05-21T06:38:31.675804: train-step 6534, loss 0.182715, acc 0.96
2016-05-21T06:38:38.057712: train-step 6535, loss 0.180823, acc 0.97
2016-05-21T06:38:44.377169: train-step 6536, loss 0.168357, acc 0.98
2016-05-21T06:38:50.647031: train-step 6537, loss 0.205511, acc 0.98
2016-05-21T06:38:56.998829: train-step 6538, loss 0.181002, acc 0.99
2016-05-21T06:39:03.532170: train-step 6539, loss 0.189292, acc 0.97
2016-05-21T06:39:09.892318: train-step 6540, loss 0.172009, acc 1
2016-05-21T06:39:16.213532: train-step 6541, loss 0.162983, acc 0.99
2016-05-21T06:39:22.467870: train-step 6542, loss 0.161751, acc 0.99
2016-05-21T06:39:29.076668: train-step 6543, loss 0.152348, acc 0.98
2016-05-21T06:39:35.426545: train-step 6544, loss 0.177654, acc 0.97
2016-05-21T06:39:41.780412: train-step 6545, loss 0.159684, acc 0.98
2016-05-21T06:39:48.129212: train-step 6546, loss 0.16584, acc 0.99
2016-05-21T06:39:54.500488: train-step 6547, loss 0.191282, acc 0.98
2016-05-21T06:40:00.792309: train-step 6548, loss 0.191793, acc 0.97
2016-05-21T06:40:07.021585: train-step 6549, loss 0.169589, acc 0.98
2016-05-21T06:40:13.250626: train-step 6550, loss 0.195706, acc 0.96
2016-05-21T06:40:19.456663: train-step 6551, loss 0.234691, acc 0.94
2016-05-21T06:40:26.044700: train-step 6552, loss 0.191614, acc 0.99
2016-05-21T06:40:32.325537: train-step 6553, loss 0.204597, acc 0.96
2016-05-21T06:40:38.703106: train-step 6554, loss 0.235264, acc 0.95
2016-05-21T06:40:45.061518: train-step 6555, loss 0.199073, acc 0.96
2016-05-21T06:40:51.256709: train-step 6556, loss 0.180103, acc 0.98
2016-05-21T06:40:57.425947: train-step 6557, loss 0.209965, acc 0.95
2016-05-21T06:41:03.751180: train-step 6558, loss 0.173167, acc 0.99
2016-05-21T06:41:10.125512: train-step 6559, loss 0.17317, acc 0.99
2016-05-21T06:41:16.418908: train-step 6560, loss 0.166469, acc 0.98
2016-05-21T06:41:22.585560: train-step 6561, loss 0.228176, acc 0.93
2016-05-21T06:41:29.174019: train-step 6562, loss 0.214213, acc 0.94
2016-05-21T06:41:35.482237: train-step 6563, loss 0.160989, acc 0.98
2016-05-21T06:41:41.694435: train-step 6564, loss 0.19062, acc 0.97
2016-05-21T06:41:48.251293: train-step 6565, loss 0.215425, acc 0.95
2016-05-21T06:41:54.581251: train-step 6566, loss 0.178791, acc 0.98
2016-05-21T06:42:00.938754: train-step 6567, loss 0.188757, acc 0.95
2016-05-21T06:42:07.325745: train-step 6568, loss 0.193294, acc 0.97
2016-05-21T06:42:13.571205: train-step 6569, loss 0.159004, acc 0.98
2016-05-21T06:42:19.801943: train-step 6570, loss 0.171492, acc 0.99
2016-05-21T06:42:26.052881: train-step 6571, loss 0.180518, acc 0.97
2016-05-21T06:42:32.611024: train-step 6572, loss 0.195049, acc 0.97
2016-05-21T06:42:38.873998: train-step 6573, loss 0.167091, acc 0.97
2016-05-21T06:42:45.271943: train-step 6574, loss 0.22965, acc 0.95
2016-05-21T06:42:51.657059: train-step 6575, loss 0.23729, acc 0.92
2016-05-21T06:42:58.026224: train-step 6576, loss 0.155942, acc 0.98
2016-05-21T06:43:04.387248: train-step 6577, loss 0.165093, acc 0.99
2016-05-21T06:43:10.729520: train-step 6578, loss 0.230742, acc 0.95
2016-05-21T06:43:17.096272: train-step 6579, loss 0.205798, acc 0.95
2016-05-21T06:43:23.503360: train-step 6580, loss 0.187066, acc 0.96
2016-05-21T06:43:29.858594: train-step 6581, loss 0.195741, acc 0.97
2016-05-21T06:43:36.171432: train-step 6582, loss 0.223474, acc 0.96
2016-05-21T06:43:42.568866: train-step 6583, loss 0.209119, acc 0.95
2016-05-21T06:43:48.962932: train-step 6584, loss 0.19227, acc 0.94
2016-05-21T06:43:55.202926: train-step 6585, loss 0.170837, acc 0.96
2016-05-21T06:44:01.574077: train-step 6586, loss 0.176172, acc 0.97
2016-05-21T06:44:08.071539: train-step 6587, loss 0.183146, acc 0.98
2016-05-21T06:44:14.419623: train-step 6588, loss 0.193391, acc 0.97
2016-05-21T06:44:20.729396: train-step 6589, loss 0.219652, acc 0.95
2016-05-21T06:44:26.989022: train-step 6590, loss 0.167203, acc 0.97
2016-05-21T06:44:33.184837: train-step 6591, loss 0.184265, acc 0.96
2016-05-21T06:44:39.348849: train-step 6592, loss 0.189259, acc 0.96
2016-05-21T06:44:45.561784: train-step 6593, loss 0.168566, acc 0.97
2016-05-21T06:44:51.927282: train-step 6594, loss 0.174442, acc 0.97
2016-05-21T06:44:58.175430: train-step 6595, loss 0.173541, acc 0.99
2016-05-21T06:45:04.499145: train-step 6596, loss 0.224127, acc 0.94
2016-05-21T06:45:11.078804: train-step 6597, loss 0.203757, acc 0.94
2016-05-21T06:45:17.634905: train-step 6598, loss 0.181994, acc 0.98
2016-05-21T06:45:23.987126: train-step 6599, loss 0.201598, acc 0.96
2016-05-21T06:45:30.352499: train-step 6600, loss 0.174633, acc 0.97
2016-05-21T06:45:36.568271: train-step 6601, loss 0.215593, acc 0.96
2016-05-21T06:45:43.064804: train-step 6602, loss 0.198114, acc 0.97
2016-05-21T06:45:49.429183: train-step 6603, loss 0.241313, acc 0.94
2016-05-21T06:45:55.709035: train-step 6604, loss 0.198395, acc 0.96
2016-05-21T06:46:01.918091: train-step 6605, loss 0.240526, acc 0.94
2016-05-21T06:46:08.229716: train-step 6606, loss 0.186922, acc 0.96
2016-05-21T06:46:14.756875: train-step 6607, loss 0.169269, acc 0.99
2016-05-21T06:46:21.081081: train-step 6608, loss 0.186174, acc 0.98
2016-05-21T06:46:27.393044: train-step 6609, loss 0.207514, acc 0.97
2016-05-21T06:46:33.611058: train-step 6610, loss 0.22957, acc 0.94
2016-05-21T06:46:39.814548: train-step 6611, loss 0.180891, acc 0.96
2016-05-21T06:46:46.186399: train-step 6612, loss 0.182707, acc 0.96
2016-05-21T06:46:52.569456: train-step 6613, loss 0.192338, acc 0.97
2016-05-21T06:46:58.925408: train-step 6614, loss 0.178174, acc 0.99
2016-05-21T06:47:05.242658: train-step 6615, loss 0.17586, acc 0.98
2016-05-21T06:47:11.380278: train-step 6616, loss 0.198617, acc 0.96
2016-05-21T06:47:17.594749: train-step 6617, loss 0.207345, acc 0.96
2016-05-21T06:47:23.945149: train-step 6618, loss 0.182926, acc 0.95
2016-05-21T06:47:30.196385: train-step 6619, loss 0.17019, acc 0.97
2016-05-21T06:47:36.763619: train-step 6620, loss 0.186871, acc 0.96
2016-05-21T06:47:43.134218: train-step 6621, loss 0.243503, acc 0.92
2016-05-21T06:47:49.508030: train-step 6622, loss 0.249445, acc 0.93
2016-05-21T06:47:55.853994: train-step 6623, loss 0.199347, acc 0.97
2016-05-21T06:48:02.188695: train-step 6624, loss 0.162741, acc 0.98
2016-05-21T06:48:08.448410: train-step 6625, loss 0.184768, acc 0.95
2016-05-21T06:48:14.675878: train-step 6626, loss 0.189248, acc 0.96
2016-05-21T06:48:21.238220: train-step 6627, loss 0.2024, acc 0.96
2016-05-21T06:48:27.544505: train-step 6628, loss 0.229846, acc 0.94
2016-05-21T06:48:33.715530: train-step 6629, loss 0.177362, acc 0.97
2016-05-21T06:48:40.257083: train-step 6630, loss 0.186572, acc 0.99
2016-05-21T06:48:46.648512: train-step 6631, loss 0.170685, acc 0.99
2016-05-21T06:48:53.039911: train-step 6632, loss 0.195624, acc 0.98
2016-05-21T06:48:59.304760: train-step 6633, loss 0.250845, acc 0.93
2016-05-21T06:49:05.732272: train-step 6634, loss 0.189851, acc 0.99
2016-05-21T06:49:12.261635: train-step 6635, loss 0.170441, acc 0.97
2016-05-21T06:49:18.581477: train-step 6636, loss 0.26839, acc 0.92
2016-05-21T06:49:24.872690: train-step 6637, loss 0.185159, acc 0.97
2016-05-21T06:49:31.619453: train-step 6638, loss 0.193862, acc 0.97
2016-05-21T06:49:38.159164: train-step 6639, loss 0.22205, acc 0.96
2016-05-21T06:49:44.499163: train-step 6640, loss 0.16923, acc 0.98
2016-05-21T06:49:50.847020: train-step 6641, loss 0.228202, acc 0.97
2016-05-21T06:49:57.220014: train-step 6642, loss 0.183269, acc 0.96
2016-05-21T06:50:03.457971: train-step 6643, loss 0.176907, acc 0.96
2016-05-21T06:50:09.664113: train-step 6644, loss 0.175142, acc 0.96
2016-05-21T06:50:15.958708: train-step 6645, loss 0.209256, acc 0.96
2016-05-21T06:50:22.300737: train-step 6646, loss 0.163346, acc 0.99
2016-05-21T06:50:28.611063: train-step 6647, loss 0.161222, acc 0.97
2016-05-21T06:50:34.959550: train-step 6648, loss 0.188055, acc 0.97
2016-05-21T06:50:41.477297: train-step 6649, loss 0.199545, acc 0.97
2016-05-21T06:50:48.284175: train-step 6650, loss 0.175647, acc 0.99
2016-05-21T06:50:54.631602: train-step 6651, loss 0.189237, acc 0.95
2016-05-21T06:51:00.972515: train-step 6652, loss 0.202751, acc 0.97
2016-05-21T06:51:07.171390: train-step 6653, loss 0.186158, acc 0.97
2016-05-21T06:51:13.444074: train-step 6654, loss 0.183022, acc 0.96
2016-05-21T06:51:19.586940: train-step 6655, loss 0.168944, acc 0.98
2016-05-21T06:51:25.817342: train-step 6656, loss 0.219063, acc 0.99
2016-05-21T06:51:32.119880: train-step 6657, loss 0.177791, acc 0.98
2016-05-21T06:51:38.496695: train-step 6658, loss 0.173036, acc 0.97
2016-05-21T06:51:44.823990: train-step 6659, loss 0.188329, acc 0.96
2016-05-21T06:51:51.084541: train-step 6660, loss 0.178097, acc 0.97
2016-05-21T06:51:57.347645: train-step 6661, loss 0.173792, acc 0.98
2016-05-21T06:52:03.500304: train-step 6662, loss 0.182592, acc 0.95
2016-05-21T06:52:09.693836: train-step 6663, loss 0.170825, acc 0.98
2016-05-21T06:52:15.956041: train-step 6664, loss 0.192098, acc 0.97
2016-05-21T06:52:22.205647: train-step 6665, loss 0.183393, acc 0.99
2016-05-21T06:52:28.757056: train-step 6666, loss 0.209344, acc 0.98
2016-05-21T06:52:35.075873: train-step 6667, loss 0.156442, acc 1
2016-05-21T06:52:41.468187: train-step 6668, loss 0.194892, acc 0.97
2016-05-21T06:52:47.708894: train-step 6669, loss 0.183923, acc 0.96
2016-05-21T06:52:53.954521: train-step 6670, loss 0.171472, acc 0.98
2016-05-21T06:53:00.157049: train-step 6671, loss 0.175281, acc 0.97
2016-05-21T06:53:06.601405: train-step 6672, loss 0.164972, acc 0.97
2016-05-21T06:53:13.088756: train-step 6673, loss 0.205304, acc 0.96
2016-05-21T06:53:19.400492: train-step 6674, loss 0.181892, acc 0.99
2016-05-21T06:53:26.276570: train-step 6675, loss 0.155313, acc 0.99
2016-05-21T06:53:32.802602: train-step 6676, loss 0.178158, acc 0.98
2016-05-21T06:53:39.018367: train-step 6677, loss 0.208459, acc 0.94
2016-05-21T06:53:45.203292: train-step 6678, loss 0.193138, acc 0.97
2016-05-21T06:53:51.408920: train-step 6679, loss 0.204791, acc 0.96
2016-05-21T06:53:58.026868: train-step 6680, loss 0.157378, acc 0.98
2016-05-21T06:54:04.383743: train-step 6681, loss 0.224956, acc 0.93
2016-05-21T06:54:10.563875: train-step 6682, loss 0.204493, acc 0.94
2016-05-21T06:54:17.145416: train-step 6683, loss 0.222094, acc 0.93
2016-05-21T06:54:23.453658: train-step 6684, loss 0.179619, acc 0.97
2016-05-21T06:54:29.804864: train-step 6685, loss 0.202487, acc 0.97
2016-05-21T06:54:36.154564: train-step 6686, loss 0.185306, acc 0.98
2016-05-21T06:54:42.499913: train-step 6687, loss 0.154424, acc 0.99
2016-05-21T06:54:48.848791: train-step 6688, loss 0.290793, acc 0.92
2016-05-21T06:54:55.194512: train-step 6689, loss 0.190537, acc 0.97
2016-05-21T06:55:01.645596: train-step 6690, loss 0.20754, acc 0.94
2016-05-21T06:55:07.884076: train-step 6691, loss 0.184093, acc 0.96
2016-05-21T06:55:14.272228: train-step 6692, loss 0.163479, acc 0.98
2016-05-21T06:55:20.758855: train-step 6693, loss 0.20996, acc 0.98
2016-05-21T06:55:27.123593: train-step 6694, loss 0.167375, acc 0.99
2016-05-21T06:55:33.440441: train-step 6695, loss 0.174247, acc 0.98
2016-05-21T06:55:39.619257: train-step 6696, loss 0.19364, acc 0.96
2016-05-21T06:55:45.826040: train-step 6697, loss 0.176574, acc 0.97
2016-05-21T06:55:52.054423: train-step 6698, loss 0.20814, acc 0.94
2016-05-21T06:55:58.232650: train-step 6699, loss 0.159993, acc 0.99
2016-05-21T06:56:04.581442: train-step 6700, loss 0.201367, acc 0.97
2016-05-21T06:56:10.765195: train-step 6701, loss 0.169198, acc 0.99
2016-05-21T06:56:16.974489: train-step 6702, loss 0.213383, acc 0.96
2016-05-21T06:56:23.187839: train-step 6703, loss 0.184159, acc 0.98
2016-05-21T06:56:29.729910: train-step 6704, loss 0.172456, acc 0.99
2016-05-21T06:56:36.022304: train-step 6705, loss 0.220051, acc 0.94
2016-05-21T06:56:42.169922: train-step 6706, loss 0.145637, acc 0.98
2016-05-21T06:56:48.706312: train-step 6707, loss 0.196334, acc 0.98
2016-05-21T06:56:55.040114: train-step 6708, loss 0.237773, acc 0.94
2016-05-21T06:57:01.430298: train-step 6709, loss 0.239007, acc 0.95
2016-05-21T06:57:07.632212: train-step 6710, loss 0.221168, acc 0.95
2016-05-21T06:57:14.028611: train-step 6711, loss 0.233886, acc 0.94
2016-05-21T06:57:20.576804: train-step 6712, loss 0.21469, acc 0.95
2016-05-21T06:57:26.912550: train-step 6713, loss 0.240016, acc 0.96
2016-05-21T06:57:33.233836: train-step 6714, loss 0.24384, acc 0.93
2016-05-21T06:57:39.459973: train-step 6715, loss 0.180995, acc 0.98
2016-05-21T06:57:46.015085: train-step 6716, loss 0.163398, acc 0.99
2016-05-21T06:57:52.325076: train-step 6717, loss 0.19784, acc 0.95
2016-05-21T06:57:58.712081: train-step 6718, loss 0.211026, acc 0.97
2016-05-21T06:58:05.074174: train-step 6719, loss 0.188086, acc 0.95
2016-05-21T06:58:11.475519: train-step 6720, loss 0.18507, acc 0.97
2016-05-21T06:58:17.817209: train-step 6721, loss 0.154913, acc 0.98
2016-05-21T06:58:24.114886: train-step 6722, loss 0.190369, acc 0.96
2016-05-21T06:58:30.288200: train-step 6723, loss 0.214824, acc 0.92
2016-05-21T06:58:36.547750: train-step 6724, loss 0.231558, acc 0.95
2016-05-21T06:58:42.993720: train-step 6725, loss 0.198496, acc 0.96
2016-05-21T06:58:49.413544: train-step 6726, loss 0.1459, acc 0.99
2016-05-21T06:58:55.737017: train-step 6727, loss 0.180523, acc 0.97
2016-05-21T06:59:02.623146: train-step 6728, loss 0.227438, acc 0.91
2016-05-21T06:59:09.076335: train-step 6729, loss 0.209594, acc 0.95
2016-05-21T06:59:15.439892: train-step 6730, loss 0.201578, acc 0.95
2016-05-21T06:59:21.729130: train-step 6731, loss 0.179759, acc 0.96
2016-05-21T06:59:27.936470: train-step 6732, loss 0.194491, acc 0.96
2016-05-21T06:59:34.204479: train-step 6733, loss 0.199222, acc 0.96
2016-05-21T06:59:40.530219: train-step 6734, loss 0.145196, acc 0.99
2016-05-21T06:59:46.701688: train-step 6735, loss 0.172816, acc 0.97
2016-05-21T06:59:52.912787: train-step 6736, loss 0.172445, acc 0.95
2016-05-21T06:59:59.287368: train-step 6737, loss 0.205435, acc 0.93
2016-05-21T07:00:05.671017: train-step 6738, loss 0.19065, acc 0.98
2016-05-21T07:00:12.014534: train-step 6739, loss 0.211034, acc 0.95
2016-05-21T07:00:18.234348: train-step 6740, loss 0.249513, acc 0.94
2016-05-21T07:00:24.448878: train-step 6741, loss 0.219248, acc 0.94
2016-05-21T07:00:30.673883: train-step 6742, loss 0.164438, acc 0.99
2016-05-21T07:00:37.034140: train-step 6743, loss 0.175346, acc 0.98
2016-05-21T07:00:43.369594: train-step 6744, loss 0.164907, acc 0.99
2016-05-21T07:00:49.593443: train-step 6745, loss 0.21245, acc 0.95
2016-05-21T07:00:55.852324: train-step 6746, loss 0.228687, acc 0.94
2016-05-21T07:01:02.159782: train-step 6747, loss 0.142075, acc 0.99
2016-05-21T07:01:08.704865: train-step 6748, loss 0.284569, acc 0.93
2016-05-21T07:01:14.943110: train-step 6749, loss 0.197446, acc 0.97
2016-05-21T07:01:21.173462: train-step 6750, loss 0.165238, acc 0.97
epoch number is: 27
2016-05-21T07:01:27.833021: train-step 6751, loss 0.167912, acc 0.98
2016-05-21T07:01:34.104687: train-step 6752, loss 0.227234, acc 0.94
2016-05-21T07:01:40.316113: train-step 6753, loss 0.159609, acc 0.99
2016-05-21T07:01:46.514466: train-step 6754, loss 0.159358, acc 1
2016-05-21T07:01:52.729842: train-step 6755, loss 0.174709, acc 0.96
2016-05-21T07:01:59.011391: train-step 6756, loss 0.175887, acc 0.98
2016-05-21T07:02:05.300188: train-step 6757, loss 0.140189, acc 1
2016-05-21T07:02:11.636618: train-step 6758, loss 0.196761, acc 0.96
2016-05-21T07:02:18.037043: train-step 6759, loss 0.147577, acc 1
2016-05-21T07:02:24.278224: train-step 6760, loss 0.17274, acc 0.96
2016-05-21T07:02:30.446006: train-step 6761, loss 0.1825, acc 0.95
2016-05-21T07:02:36.777887: train-step 6762, loss 0.188636, acc 0.99
2016-05-21T07:02:42.960789: train-step 6763, loss 0.162368, acc 0.97
2016-05-21T07:02:49.246633: train-step 6764, loss 0.17326, acc 0.98
2016-05-21T07:02:55.770007: train-step 6765, loss 0.156885, acc 1
2016-05-21T07:03:01.998574: train-step 6766, loss 0.183188, acc 0.98
2016-05-21T07:03:08.161175: train-step 6767, loss 0.217141, acc 0.95
2016-05-21T07:03:14.460976: train-step 6768, loss 0.210443, acc 0.94
2016-05-21T07:03:20.785344: train-step 6769, loss 0.161159, acc 0.98
2016-05-21T07:03:26.999418: train-step 6770, loss 0.210743, acc 0.96
2016-05-21T07:03:33.257386: train-step 6771, loss 0.176829, acc 0.99
2016-05-21T07:03:39.581304: train-step 6772, loss 0.209623, acc 0.97
2016-05-21T07:03:45.805157: train-step 6773, loss 0.155186, acc 0.99
2016-05-21T07:03:52.320378: train-step 6774, loss 0.151786, acc 0.98
2016-05-21T07:03:58.709686: train-step 6775, loss 0.193985, acc 0.97
2016-05-21T07:04:05.037602: train-step 6776, loss 0.200293, acc 0.97
2016-05-21T07:04:11.362493: train-step 6777, loss 0.186235, acc 0.96
2016-05-21T07:04:17.730722: train-step 6778, loss 0.203632, acc 0.96
2016-05-21T07:04:24.055345: train-step 6779, loss 0.187172, acc 0.96
2016-05-21T07:04:30.458235: train-step 6780, loss 0.177992, acc 0.97
2016-05-21T07:04:36.766979: train-step 6781, loss 0.180438, acc 0.96
2016-05-21T07:04:43.103250: train-step 6782, loss 0.188615, acc 0.97
2016-05-21T07:04:49.391350: train-step 6783, loss 0.21339, acc 0.97
2016-05-21T07:04:55.560385: train-step 6784, loss 0.170583, acc 0.97
2016-05-21T07:05:01.759625: train-step 6785, loss 0.189156, acc 0.97
2016-05-21T07:05:08.057865: train-step 6786, loss 0.173593, acc 0.96
2016-05-21T07:05:14.430034: train-step 6787, loss 0.178258, acc 0.99
2016-05-21T07:05:20.810188: train-step 6788, loss 0.14799, acc 1
2016-05-21T07:05:27.056966: train-step 6789, loss 0.161275, acc 0.99
2016-05-21T07:05:33.370507: train-step 6790, loss 0.21873, acc 0.94
2016-05-21T07:05:39.885074: train-step 6791, loss 0.164395, acc 0.98
2016-05-21T07:05:46.249858: train-step 6792, loss 0.19491, acc 0.95
2016-05-21T07:05:52.620483: train-step 6793, loss 0.173624, acc 0.98
2016-05-21T07:05:58.830962: train-step 6794, loss 0.165106, acc 0.99
2016-05-21T07:06:05.379755: train-step 6795, loss 0.177711, acc 0.95
2016-05-21T07:06:11.708565: train-step 6796, loss 0.189237, acc 0.95
2016-05-21T07:06:18.082968: train-step 6797, loss 0.177228, acc 0.98
2016-05-21T07:06:24.275281: train-step 6798, loss 0.170259, acc 1
2016-05-21T07:06:30.534306: train-step 6799, loss 0.154457, acc 1
2016-05-21T07:06:36.763629: train-step 6800, loss 0.210828, acc 0.97
2016-05-21T07:06:42.934337: train-step 6801, loss 0.209594, acc 0.95
2016-05-21T07:06:49.115312: train-step 6802, loss 0.196122, acc 0.95
2016-05-21T07:06:55.463038: train-step 6803, loss 0.140616, acc 1
2016-05-21T07:07:01.657977: train-step 6804, loss 0.188136, acc 0.96
2016-05-21T07:07:08.109165: train-step 6805, loss 0.185517, acc 1
2016-05-21T07:07:14.476203: train-step 6806, loss 0.218859, acc 0.94
2016-05-21T07:07:20.780136: train-step 6807, loss 0.209989, acc 0.96
2016-05-21T07:07:26.986964: train-step 6808, loss 0.165021, acc 0.98
2016-05-21T07:07:33.194433: train-step 6809, loss 0.15633, acc 0.98
2016-05-21T07:07:39.636586: train-step 6810, loss 0.178434, acc 0.96
2016-05-21T07:07:46.060259: train-step 6811, loss 0.205037, acc 0.94
2016-05-21T07:07:52.423624: train-step 6812, loss 0.181768, acc 0.98
2016-05-21T07:07:59.240880: train-step 6813, loss 0.171374, acc 0.98
2016-05-21T07:08:05.738013: train-step 6814, loss 0.172147, acc 0.98
2016-05-21T07:08:12.105267: train-step 6815, loss 0.209294, acc 0.96
2016-05-21T07:08:18.436543: train-step 6816, loss 0.202944, acc 0.97
2016-05-21T07:08:24.758759: train-step 6817, loss 0.188827, acc 0.95
2016-05-21T07:08:30.972692: train-step 6818, loss 0.167977, acc 0.99
2016-05-21T07:08:37.461598: train-step 6819, loss 0.148092, acc 0.99
2016-05-21T07:08:43.834054: train-step 6820, loss 0.172602, acc 0.98
2016-05-21T07:08:50.160522: train-step 6821, loss 0.169289, acc 0.97
2016-05-21T07:08:56.418210: train-step 6822, loss 0.17791, acc 0.99
2016-05-21T07:09:02.678289: train-step 6823, loss 0.20118, acc 0.95
2016-05-21T07:09:08.895285: train-step 6824, loss 0.15615, acc 0.98
2016-05-21T07:09:15.095809: train-step 6825, loss 0.193838, acc 0.98
2016-05-21T07:09:21.287710: train-step 6826, loss 0.188548, acc 0.98
2016-05-21T07:09:27.532569: train-step 6827, loss 0.225416, acc 0.93
2016-05-21T07:09:33.778422: train-step 6828, loss 0.155234, acc 1
2016-05-21T07:09:39.962774: train-step 6829, loss 0.164947, acc 0.98
2016-05-21T07:09:46.116726: train-step 6830, loss 0.212433, acc 0.97
2016-05-21T07:09:52.609004: train-step 6831, loss 0.162324, acc 0.99
2016-05-21T07:09:58.935699: train-step 6832, loss 0.243833, acc 0.94
2016-05-21T07:10:05.236575: train-step 6833, loss 0.14768, acc 0.98
2016-05-21T07:10:11.475708: train-step 6834, loss 0.144513, acc 0.98
2016-05-21T07:10:17.652233: train-step 6835, loss 0.155658, acc 0.98
2016-05-21T07:10:23.830027: train-step 6836, loss 0.144777, acc 0.99
2016-05-21T07:10:30.016728: train-step 6837, loss 0.176065, acc 0.98
2016-05-21T07:10:36.385089: train-step 6838, loss 0.184846, acc 0.96
2016-05-21T07:10:42.776478: train-step 6839, loss 0.194586, acc 0.96
2016-05-21T07:10:49.136006: train-step 6840, loss 0.207482, acc 0.97
2016-05-21T07:10:55.402407: train-step 6841, loss 0.18944, acc 0.96
2016-05-21T07:11:01.638223: train-step 6842, loss 0.202415, acc 0.96
2016-05-21T07:11:08.226739: train-step 6843, loss 0.188418, acc 0.98
2016-05-21T07:11:14.526462: train-step 6844, loss 0.214368, acc 0.93
2016-05-21T07:11:20.858114: train-step 6845, loss 0.215727, acc 0.95
2016-05-21T07:11:27.126266: train-step 6846, loss 0.213861, acc 0.93
2016-05-21T07:11:33.673797: train-step 6847, loss 0.16153, acc 1
2016-05-21T07:11:40.052004: train-step 6848, loss 0.183001, acc 0.96
2016-05-21T07:11:46.271466: train-step 6849, loss 0.181241, acc 0.98
2016-05-21T07:11:52.796247: train-step 6850, loss 0.187232, acc 0.96
2016-05-21T07:11:59.136853: train-step 6851, loss 0.156316, acc 0.99
2016-05-21T07:12:05.367754: train-step 6852, loss 0.186912, acc 0.97
2016-05-21T07:12:11.801054: train-step 6853, loss 0.190017, acc 0.96
2016-05-21T07:12:18.277892: train-step 6854, loss 0.193808, acc 0.99
2016-05-21T07:12:24.524211: train-step 6855, loss 0.268475, acc 0.91
2016-05-21T07:12:30.914412: train-step 6856, loss 0.184955, acc 0.95
2016-05-21T07:12:37.422204: train-step 6857, loss 0.238414, acc 0.93
2016-05-21T07:12:43.622299: train-step 6858, loss 0.190801, acc 0.95
2016-05-21T07:12:49.872318: train-step 6859, loss 0.185234, acc 0.97
2016-05-21T07:12:56.103300: train-step 6860, loss 0.219562, acc 0.94
2016-05-21T07:13:02.306678: train-step 6861, loss 0.214569, acc 0.95
2016-05-21T07:13:08.784843: train-step 6862, loss 0.212777, acc 0.95
2016-05-21T07:13:15.193020: train-step 6863, loss 0.193845, acc 0.98
2016-05-21T07:13:21.580050: train-step 6864, loss 0.169622, acc 0.97
2016-05-21T07:13:27.931509: train-step 6865, loss 0.216291, acc 0.95
2016-05-21T07:13:34.260131: train-step 6866, loss 0.194234, acc 0.96
2016-05-21T07:13:40.531449: train-step 6867, loss 0.165823, acc 0.98
2016-05-21T07:13:47.075040: train-step 6868, loss 0.174313, acc 0.98
2016-05-21T07:13:53.408009: train-step 6869, loss 0.196166, acc 0.94
2016-05-21T07:13:59.651860: train-step 6870, loss 0.190004, acc 0.99
2016-05-21T07:14:06.170275: train-step 6871, loss 0.166433, acc 0.99
2016-05-21T07:14:12.527061: train-step 6872, loss 0.203325, acc 0.94
2016-05-21T07:14:18.872223: train-step 6873, loss 0.158243, acc 0.98
2016-05-21T07:14:25.130979: train-step 6874, loss 0.192562, acc 0.98
2016-05-21T07:14:31.309699: train-step 6875, loss 0.172329, acc 0.99
2016-05-21T07:14:37.606707: train-step 6876, loss 0.178715, acc 0.98
2016-05-21T07:14:44.002993: train-step 6877, loss 0.175925, acc 0.97
2016-05-21T07:14:50.261523: train-step 6878, loss 0.199093, acc 0.96
2016-05-21T07:14:56.507040: train-step 6879, loss 0.185922, acc 0.96
2016-05-21T07:15:03.109016: train-step 6880, loss 0.183825, acc 0.96
2016-05-21T07:15:09.434048: train-step 6881, loss 0.197606, acc 0.95
2016-05-21T07:15:15.783654: train-step 6882, loss 0.200138, acc 0.96
2016-05-21T07:15:22.008994: train-step 6883, loss 0.199746, acc 0.95
2016-05-21T07:15:28.265808: train-step 6884, loss 0.194816, acc 0.97
2016-05-21T07:15:34.521766: train-step 6885, loss 0.186723, acc 0.96
2016-05-21T07:15:40.753380: train-step 6886, loss 0.205761, acc 0.96
2016-05-21T07:15:47.342260: train-step 6887, loss 0.173765, acc 0.97
2016-05-21T07:15:53.694277: train-step 6888, loss 0.169406, acc 0.98
2016-05-21T07:16:00.092209: train-step 6889, loss 0.165644, acc 0.97
2016-05-21T07:16:06.512247: train-step 6890, loss 0.168808, acc 0.99
2016-05-21T07:16:12.737388: train-step 6891, loss 0.194039, acc 0.97
2016-05-21T07:16:19.071409: train-step 6892, loss 0.201502, acc 0.96
2016-05-21T07:16:25.616476: train-step 6893, loss 0.179973, acc 0.98
2016-05-21T07:16:31.960146: train-step 6894, loss 0.183563, acc 0.97
2016-05-21T07:16:38.241296: train-step 6895, loss 0.161687, acc 0.97
2016-05-21T07:16:44.430088: train-step 6896, loss 0.17844, acc 0.99
2016-05-21T07:16:50.658566: train-step 6897, loss 0.173678, acc 0.99
2016-05-21T07:16:56.874683: train-step 6898, loss 0.209446, acc 0.94
2016-05-21T07:17:03.131425: train-step 6899, loss 0.22322, acc 0.95
2016-05-21T07:17:09.271143: train-step 6900, loss 0.237745, acc 0.93
2016-05-21T07:17:15.444719: train-step 6901, loss 0.167702, acc 0.99
2016-05-21T07:17:21.601228: train-step 6902, loss 0.2387, acc 0.93
2016-05-21T07:17:27.887096: train-step 6903, loss 0.197474, acc 0.94
2016-05-21T07:17:34.202010: train-step 6904, loss 0.176538, acc 0.97
2016-05-21T07:17:40.406781: train-step 6905, loss 0.160139, acc 1
2016-05-21T07:17:46.623963: train-step 6906, loss 0.170479, acc 0.99
2016-05-21T07:17:52.924769: train-step 6907, loss 0.157981, acc 0.98
2016-05-21T07:17:59.300918: train-step 6908, loss 0.166907, acc 0.98
2016-05-21T07:18:05.582818: train-step 6909, loss 0.186375, acc 0.97
2016-05-21T07:18:11.850465: train-step 6910, loss 0.216221, acc 0.97
2016-05-21T07:18:18.111565: train-step 6911, loss 0.220726, acc 0.93
2016-05-21T07:18:24.314230: train-step 6912, loss 0.185481, acc 0.96
2016-05-21T07:18:30.523337: train-step 6913, loss 0.164817, acc 0.99
2016-05-21T07:18:36.870404: train-step 6914, loss 0.153859, acc 0.98
2016-05-21T07:18:43.211909: train-step 6915, loss 0.196029, acc 0.97
2016-05-21T07:18:49.422293: train-step 6916, loss 0.191123, acc 0.95
2016-05-21T07:18:55.565461: train-step 6917, loss 0.191873, acc 0.96
2016-05-21T07:19:01.781889: train-step 6918, loss 0.177892, acc 0.98
2016-05-21T07:19:07.968274: train-step 6919, loss 0.208886, acc 0.95
2016-05-21T07:19:14.199693: train-step 6920, loss 0.185358, acc 0.98
2016-05-21T07:19:20.788968: train-step 6921, loss 0.183245, acc 0.94
2016-05-21T07:19:27.081435: train-step 6922, loss 0.239874, acc 0.94
2016-05-21T07:19:33.384000: train-step 6923, loss 0.226579, acc 0.94
2016-05-21T07:19:39.571622: train-step 6924, loss 0.193574, acc 0.96
2016-05-21T07:19:46.102372: train-step 6925, loss 0.180351, acc 0.98
2016-05-21T07:19:52.431928: train-step 6926, loss 0.178241, acc 0.96
2016-05-21T07:19:58.770403: train-step 6927, loss 0.165921, acc 0.98
2016-05-21T07:20:05.018810: train-step 6928, loss 0.21318, acc 0.96
2016-05-21T07:20:11.390086: train-step 6929, loss 0.200631, acc 0.95
2016-05-21T07:20:17.897234: train-step 6930, loss 0.23278, acc 0.93
2016-05-21T07:20:24.116521: train-step 6931, loss 0.157667, acc 0.99
2016-05-21T07:20:30.408102: train-step 6932, loss 0.206412, acc 0.96
2016-05-21T07:20:36.577592: train-step 6933, loss 0.182065, acc 0.96
2016-05-21T07:20:43.128103: train-step 6934, loss 0.171618, acc 0.98
2016-05-21T07:20:49.489632: train-step 6935, loss 0.165372, acc 0.99
2016-05-21T07:20:55.659838: train-step 6936, loss 0.194378, acc 0.95
2016-05-21T07:21:02.217549: train-step 6937, loss 0.221657, acc 0.96
2016-05-21T07:21:08.585957: train-step 6938, loss 0.174812, acc 0.98
2016-05-21T07:21:14.744085: train-step 6939, loss 0.158436, acc 0.98
2016-05-21T07:21:21.227510: train-step 6940, loss 0.22929, acc 0.94
2016-05-21T07:21:27.601900: train-step 6941, loss 0.217621, acc 0.96
2016-05-21T07:21:33.941956: train-step 6942, loss 0.165653, acc 0.98
2016-05-21T07:21:40.168625: train-step 6943, loss 0.189977, acc 0.96
2016-05-21T07:21:46.501486: train-step 6944, loss 0.221392, acc 0.97
2016-05-21T07:21:53.054522: train-step 6945, loss 0.14286, acc 0.98
2016-05-21T07:21:59.346970: train-step 6946, loss 0.212494, acc 0.97
2016-05-21T07:22:05.588376: train-step 6947, loss 0.198292, acc 0.96
2016-05-21T07:22:11.756944: train-step 6948, loss 0.211087, acc 0.98
2016-05-21T07:22:18.294127: train-step 6949, loss 0.190103, acc 0.97
2016-05-21T07:22:24.613353: train-step 6950, loss 0.19348, acc 0.98
2016-05-21T07:22:30.788441: train-step 6951, loss 0.148052, acc 1
2016-05-21T07:22:37.015723: train-step 6952, loss 0.178061, acc 0.98
2016-05-21T07:22:43.421651: train-step 6953, loss 0.177001, acc 0.98
2016-05-21T07:22:49.974671: train-step 6954, loss 0.152728, acc 0.99
2016-05-21T07:22:56.221502: train-step 6955, loss 0.214328, acc 0.94
2016-05-21T07:23:02.503283: train-step 6956, loss 0.192848, acc 0.96
2016-05-21T07:23:09.026915: train-step 6957, loss 0.233926, acc 0.93
2016-05-21T07:23:15.301951: train-step 6958, loss 0.207547, acc 0.95
2016-05-21T07:23:21.448929: train-step 6959, loss 0.148044, acc 0.98
2016-05-21T07:23:27.759092: train-step 6960, loss 0.180302, acc 0.99
2016-05-21T07:23:34.115522: train-step 6961, loss 0.173027, acc 0.97
2016-05-21T07:23:40.465836: train-step 6962, loss 0.211954, acc 0.96
2016-05-21T07:23:46.845546: train-step 6963, loss 0.173804, acc 0.97
2016-05-21T07:23:53.221521: train-step 6964, loss 0.181221, acc 0.98
2016-05-21T07:23:59.452688: train-step 6965, loss 0.186351, acc 0.97
2016-05-21T07:24:05.603858: train-step 6966, loss 0.172734, acc 0.98
2016-05-21T07:24:11.755764: train-step 6967, loss 0.189369, acc 0.97
2016-05-21T07:24:17.945403: train-step 6968, loss 0.204037, acc 0.96
2016-05-21T07:24:24.105933: train-step 6969, loss 0.184617, acc 0.98
2016-05-21T07:24:30.257533: train-step 6970, loss 0.182465, acc 0.99
2016-05-21T07:24:36.462102: train-step 6971, loss 0.201288, acc 0.97
2016-05-21T07:24:42.695859: train-step 6972, loss 0.166283, acc 0.99
2016-05-21T07:24:48.886770: train-step 6973, loss 0.243409, acc 0.93
2016-05-21T07:24:55.112816: train-step 6974, loss 0.182016, acc 0.98
2016-05-21T07:25:01.320682: train-step 6975, loss 0.181676, acc 0.96
2016-05-21T07:25:07.839927: train-step 6976, loss 0.212627, acc 0.94
2016-05-21T07:25:14.217123: train-step 6977, loss 0.172721, acc 1
2016-05-21T07:25:20.401973: train-step 6978, loss 0.159054, acc 0.98
2016-05-21T07:25:26.958597: train-step 6979, loss 0.182245, acc 0.96
2016-05-21T07:25:33.298002: train-step 6980, loss 0.185111, acc 0.98
2016-05-21T07:25:39.629686: train-step 6981, loss 0.188272, acc 0.96
2016-05-21T07:25:46.042823: train-step 6982, loss 0.16946, acc 0.97
2016-05-21T07:25:52.226315: train-step 6983, loss 0.195261, acc 0.95
2016-05-21T07:25:58.475177: train-step 6984, loss 0.201444, acc 0.96
2016-05-21T07:26:04.676810: train-step 6985, loss 0.242808, acc 0.95
2016-05-21T07:26:11.240314: train-step 6986, loss 0.205228, acc 0.97
2016-05-21T07:26:17.606770: train-step 6987, loss 0.299655, acc 0.95
2016-05-21T07:26:23.834172: train-step 6988, loss 0.150157, acc 0.99
2016-05-21T07:26:29.989996: train-step 6989, loss 0.230576, acc 0.94
2016-05-21T07:26:36.151816: train-step 6990, loss 0.152611, acc 0.98
2016-05-21T07:26:42.360701: train-step 6991, loss 0.188286, acc 0.96
2016-05-21T07:26:48.582549: train-step 6992, loss 0.198145, acc 0.97
2016-05-21T07:26:54.738233: train-step 6993, loss 0.157418, acc 0.98
2016-05-21T07:27:01.014873: train-step 6994, loss 0.187186, acc 0.97
2016-05-21T07:27:07.318879: train-step 6995, loss 0.188591, acc 0.95
2016-05-21T07:27:13.492651: train-step 6996, loss 0.147151, acc 0.99
2016-05-21T07:27:19.678210: train-step 6997, loss 0.173309, acc 0.98
2016-05-21T07:27:25.888315: train-step 6998, loss 0.194759, acc 0.98
2016-05-21T07:27:32.090693: train-step 6999, loss 0.219444, acc 0.94
2016-05-21T07:27:38.239278: train-step 7000, loss 0.159376, acc 0.99
epoch number is: 28
2016-05-21T07:27:44.617254: train-step 7001, loss 0.189583, acc 0.95
2016-05-21T07:27:50.850879: train-step 7002, loss 0.186266, acc 0.98
2016-05-21T07:27:57.083410: train-step 7003, loss 0.157888, acc 0.99
2016-05-21T07:28:03.698645: train-step 7004, loss 0.176195, acc 0.97
2016-05-21T07:28:10.081831: train-step 7005, loss 0.142984, acc 0.99
2016-05-21T07:28:16.320661: train-step 7006, loss 0.209866, acc 0.97
2016-05-21T07:28:22.529540: train-step 7007, loss 0.193961, acc 0.96
2016-05-21T07:28:29.036886: train-step 7008, loss 0.170491, acc 0.97
2016-05-21T07:28:35.484284: train-step 7009, loss 0.2157, acc 0.97
2016-05-21T07:28:41.686486: train-step 7010, loss 0.169771, acc 0.96
2016-05-21T07:28:47.901825: train-step 7011, loss 0.189374, acc 0.97
2016-05-21T07:28:54.139310: train-step 7012, loss 0.176495, acc 0.99
2016-05-21T07:29:00.365266: train-step 7013, loss 0.193726, acc 0.97
2016-05-21T07:29:06.526081: train-step 7014, loss 0.182953, acc 0.96
2016-05-21T07:29:12.863143: train-step 7015, loss 0.159461, acc 0.98
2016-05-21T07:29:19.119964: train-step 7016, loss 0.157021, acc 0.98
2016-05-21T07:29:25.377554: train-step 7017, loss 0.171212, acc 0.97
2016-05-21T07:29:31.727891: train-step 7018, loss 0.182712, acc 0.97
2016-05-21T07:29:38.256496: train-step 7019, loss 0.186177, acc 0.97
2016-05-21T07:29:44.618609: train-step 7020, loss 0.163334, acc 0.99
2016-05-21T07:29:50.975899: train-step 7021, loss 0.159047, acc 0.98
2016-05-21T07:29:57.343101: train-step 7022, loss 0.201462, acc 0.97
2016-05-21T07:30:03.733554: train-step 7023, loss 0.226542, acc 0.95
2016-05-21T07:30:09.976674: train-step 7024, loss 0.181171, acc 0.97
2016-05-21T07:30:16.145543: train-step 7025, loss 0.201078, acc 0.98
2016-05-21T07:30:22.515307: train-step 7026, loss 0.170243, acc 0.97
2016-05-21T07:30:28.745969: train-step 7027, loss 0.190277, acc 0.98
2016-05-21T07:30:35.091993: train-step 7028, loss 0.171669, acc 0.98
2016-05-21T07:30:41.631204: train-step 7029, loss 0.148971, acc 1
2016-05-21T07:30:47.966641: train-step 7030, loss 0.202459, acc 0.96
2016-05-21T07:30:54.306344: train-step 7031, loss 0.160576, acc 0.94
2016-05-21T07:31:00.724899: train-step 7032, loss 0.167173, acc 0.97
2016-05-21T07:31:06.951790: train-step 7033, loss 0.187912, acc 0.97
2016-05-21T07:31:13.211441: train-step 7034, loss 0.190883, acc 0.96
2016-05-21T07:31:19.469434: train-step 7035, loss 0.186745, acc 0.97
2016-05-21T07:31:25.636180: train-step 7036, loss 0.184915, acc 0.96
2016-05-21T07:31:31.834349: train-step 7037, loss 0.171421, acc 0.98
2016-05-21T07:31:38.055958: train-step 7038, loss 0.194593, acc 0.96
2016-05-21T07:31:44.252217: train-step 7039, loss 0.164349, acc 0.97
2016-05-21T07:31:50.429790: train-step 7040, loss 0.166796, acc 1
2016-05-21T07:31:56.832666: train-step 7041, loss 0.169593, acc 0.99
2016-05-21T07:32:03.206891: train-step 7042, loss 0.150689, acc 0.98
2016-05-21T07:32:09.471625: train-step 7043, loss 0.175758, acc 0.96
2016-05-21T07:32:15.840369: train-step 7044, loss 0.199514, acc 0.95
2016-05-21T07:32:22.408802: train-step 7045, loss 0.27067, acc 0.94
2016-05-21T07:32:28.664565: train-step 7046, loss 0.194719, acc 0.97
2016-05-21T07:32:34.882865: train-step 7047, loss 0.202711, acc 0.95
2016-05-21T07:32:41.420542: train-step 7048, loss 0.185684, acc 0.97
2016-05-21T07:32:47.728069: train-step 7049, loss 0.179683, acc 0.99
2016-05-21T07:32:53.938938: train-step 7050, loss 0.159635, acc 0.99
2016-05-21T07:33:00.091589: train-step 7051, loss 0.152725, acc 0.99
2016-05-21T07:33:06.293882: train-step 7052, loss 0.147947, acc 1
2016-05-21T07:33:12.512249: train-step 7053, loss 0.170272, acc 0.96
2016-05-21T07:33:18.862450: train-step 7054, loss 0.16454, acc 0.99
2016-05-21T07:33:25.122140: train-step 7055, loss 0.1443, acc 0.98
2016-05-21T07:33:31.330807: train-step 7056, loss 0.177487, acc 0.97
2016-05-21T07:33:37.519986: train-step 7057, loss 0.174583, acc 0.95
2016-05-21T07:33:43.706680: train-step 7058, loss 0.185672, acc 0.96
2016-05-21T07:33:49.879614: train-step 7059, loss 0.185756, acc 0.99
2016-05-21T07:33:56.062515: train-step 7060, loss 0.19273, acc 0.94
2016-05-21T07:34:02.243319: train-step 7061, loss 0.15728, acc 0.98
2016-05-21T07:34:08.441611: train-step 7062, loss 0.195911, acc 0.95
2016-05-21T07:34:14.794119: train-step 7063, loss 0.164895, acc 0.98
2016-05-21T07:34:20.995437: train-step 7064, loss 0.193861, acc 0.96
2016-05-21T07:34:27.266156: train-step 7065, loss 0.168496, acc 0.97
2016-05-21T07:34:33.834310: train-step 7066, loss 0.169706, acc 0.98
2016-05-21T07:34:40.186689: train-step 7067, loss 0.176296, acc 0.97
2016-05-21T07:34:46.531980: train-step 7068, loss 0.17664, acc 0.97
2016-05-21T07:34:52.923252: train-step 7069, loss 0.213433, acc 0.96
2016-05-21T07:34:59.095989: train-step 7070, loss 0.184869, acc 0.95
2016-05-21T07:35:06.125764: train-step 7071, loss 0.157297, acc 0.99
2016-05-21T07:35:13.373104: train-step 7072, loss 0.176604, acc 0.96
2016-05-21T07:35:20.088135: train-step 7073, loss 0.16255, acc 0.97
2016-05-21T07:35:26.436541: train-step 7074, loss 0.202801, acc 0.97
2016-05-21T07:35:32.582990: train-step 7075, loss 0.189649, acc 0.98
2016-05-21T07:35:38.794983: train-step 7076, loss 0.156525, acc 0.99
2016-05-21T07:35:45.187033: train-step 7077, loss 0.153562, acc 0.97
2016-05-21T07:35:51.420770: train-step 7078, loss 0.229594, acc 0.95
2016-05-21T07:35:57.673491: train-step 7079, loss 0.163423, acc 0.97
2016-05-21T07:36:03.943895: train-step 7080, loss 0.192202, acc 0.95
2016-05-21T07:36:10.140074: train-step 7081, loss 0.221992, acc 0.97
2016-05-21T07:36:16.314050: train-step 7082, loss 0.167468, acc 0.99
2016-05-21T07:36:22.500543: train-step 7083, loss 0.193218, acc 0.95
2016-05-21T07:36:28.717677: train-step 7084, loss 0.175262, acc 0.98
2016-05-21T07:36:35.267744: train-step 7085, loss 0.178154, acc 0.97
2016-05-21T07:36:41.592264: train-step 7086, loss 0.210949, acc 0.94
2016-05-21T07:36:47.988095: train-step 7087, loss 0.194687, acc 0.96
2016-05-21T07:36:54.265812: train-step 7088, loss 0.186475, acc 0.97
2016-05-21T07:37:00.495406: train-step 7089, loss 0.213774, acc 0.95
2016-05-21T07:37:06.819092: train-step 7090, loss 0.276849, acc 0.92
2016-05-21T07:37:13.085701: train-step 7091, loss 0.142163, acc 0.99
2016-05-21T07:37:19.366172: train-step 7092, loss 0.220741, acc 0.94
2016-05-21T07:37:25.584037: train-step 7093, loss 0.164617, acc 0.98
2016-05-21T07:37:32.144069: train-step 7094, loss 0.183499, acc 0.95
2016-05-21T07:37:38.457237: train-step 7095, loss 0.199719, acc 0.96
2016-05-21T07:37:44.677016: train-step 7096, loss 0.179289, acc 0.99
2016-05-21T07:37:50.937285: train-step 7097, loss 0.148724, acc 0.99
2016-05-21T07:37:57.173717: train-step 7098, loss 0.167611, acc 0.97
2016-05-21T07:38:03.430252: train-step 7099, loss 0.150549, acc 0.99
2016-05-21T07:38:09.639065: train-step 7100, loss 0.189221, acc 0.99
2016-05-21T07:38:15.863442: train-step 7101, loss 0.204962, acc 0.97
2016-05-21T07:38:22.600069: train-step 7102, loss 0.156863, acc 0.97
2016-05-21T07:38:29.251832: train-step 7103, loss 0.211875, acc 0.97
2016-05-21T07:38:35.570882: train-step 7104, loss 0.179918, acc 0.98
2016-05-21T07:38:41.737007: train-step 7105, loss 0.15769, acc 0.99
2016-05-21T07:38:47.932252: train-step 7106, loss 0.158987, acc 0.98
2016-05-21T07:38:54.141973: train-step 7107, loss 0.218201, acc 0.95
2016-05-21T07:39:00.338388: train-step 7108, loss 0.217151, acc 0.94
2016-05-21T07:39:06.488937: train-step 7109, loss 0.16846, acc 0.98
2016-05-21T07:39:12.630417: train-step 7110, loss 0.20554, acc 0.96
2016-05-21T07:39:18.748197: train-step 7111, loss 0.190647, acc 0.95
2016-05-21T07:39:24.979566: train-step 7112, loss 0.223249, acc 0.95
2016-05-21T07:39:31.168954: train-step 7113, loss 0.175591, acc 0.99
2016-05-21T07:39:37.345577: train-step 7114, loss 0.239057, acc 0.92
2016-05-21T07:39:43.555698: train-step 7115, loss 0.163414, acc 0.99
2016-05-21T07:39:49.756897: train-step 7116, loss 0.176534, acc 0.98
2016-05-21T07:39:55.968285: train-step 7117, loss 0.169663, acc 0.96
2016-05-21T07:40:02.169790: train-step 7118, loss 0.169187, acc 0.99
2016-05-21T07:40:08.325879: train-step 7119, loss 0.17715, acc 0.97
2016-05-21T07:40:14.528017: train-step 7120, loss 0.213823, acc 0.94
2016-05-21T07:40:20.747687: train-step 7121, loss 0.193574, acc 0.98
2016-05-21T07:40:26.961003: train-step 7122, loss 0.238542, acc 0.93
2016-05-21T07:40:33.185485: train-step 7123, loss 0.168247, acc 0.99
2016-05-21T07:40:39.434728: train-step 7124, loss 0.22187, acc 0.95
2016-05-21T07:40:45.604097: train-step 7125, loss 0.168169, acc 0.96
2016-05-21T07:40:52.216813: train-step 7126, loss 0.208816, acc 0.96
2016-05-21T07:40:58.576501: train-step 7127, loss 0.19176, acc 0.98
2016-05-21T07:41:04.755231: train-step 7128, loss 0.176767, acc 0.98
2016-05-21T07:41:11.301791: train-step 7129, loss 0.167995, acc 0.99
2016-05-21T07:41:17.622199: train-step 7130, loss 0.20729, acc 0.96
2016-05-21T07:41:23.800913: train-step 7131, loss 0.173135, acc 1
2016-05-21T07:41:30.075567: train-step 7132, loss 0.18177, acc 0.96
2016-05-21T07:41:36.281644: train-step 7133, loss 0.219138, acc 0.95
2016-05-21T07:41:42.464252: train-step 7134, loss 0.17563, acc 0.98
2016-05-21T07:41:48.688175: train-step 7135, loss 0.144033, acc 0.99
2016-05-21T07:41:54.876262: train-step 7136, loss 0.214234, acc 0.95
2016-05-21T07:42:01.136247: train-step 7137, loss 0.191551, acc 0.97
2016-05-21T07:42:07.384112: train-step 7138, loss 0.177708, acc 0.96
2016-05-21T07:42:13.619786: train-step 7139, loss 0.180469, acc 0.97
2016-05-21T07:42:19.822039: train-step 7140, loss 0.190884, acc 0.97
2016-05-21T07:42:25.988308: train-step 7141, loss 0.198071, acc 0.97
2016-05-21T07:42:32.214030: train-step 7142, loss 0.211017, acc 0.95
2016-05-21T07:42:38.380188: train-step 7143, loss 0.177198, acc 0.99
2016-05-21T07:42:44.531009: train-step 7144, loss 0.196924, acc 0.96
2016-05-21T07:42:50.714095: train-step 7145, loss 0.192442, acc 0.97
2016-05-21T07:42:56.928858: train-step 7146, loss 0.197744, acc 0.97
2016-05-21T07:43:03.138788: train-step 7147, loss 0.160853, acc 0.98
2016-05-21T07:43:09.348631: train-step 7148, loss 0.165668, acc 0.97
2016-05-21T07:43:15.708039: train-step 7149, loss 0.181309, acc 0.97
2016-05-21T07:43:21.885433: train-step 7150, loss 0.163825, acc 0.97
2016-05-21T07:43:28.155362: train-step 7151, loss 0.189297, acc 0.96
2016-05-21T07:43:34.362392: train-step 7152, loss 0.213228, acc 0.97
2016-05-21T07:43:40.967800: train-step 7153, loss 0.186045, acc 0.99
2016-05-21T07:43:47.307828: train-step 7154, loss 0.244415, acc 0.93
2016-05-21T07:43:53.557686: train-step 7155, loss 0.161195, acc 0.99
2016-05-21T07:43:59.803110: train-step 7156, loss 0.191565, acc 0.96
2016-05-21T07:44:06.041116: train-step 7157, loss 0.192651, acc 0.96
2016-05-21T07:44:12.298288: train-step 7158, loss 0.186017, acc 0.99
2016-05-21T07:44:18.521918: train-step 7159, loss 0.237821, acc 0.92
2016-05-21T07:44:24.792569: train-step 7160, loss 0.174177, acc 0.98
2016-05-21T07:44:31.095683: train-step 7161, loss 0.174235, acc 0.99
2016-05-21T07:44:37.286837: train-step 7162, loss 0.196632, acc 0.95
2016-05-21T07:44:43.484179: train-step 7163, loss 0.198217, acc 0.93
2016-05-21T07:44:49.684386: train-step 7164, loss 0.248197, acc 0.94
2016-05-21T07:44:56.002527: train-step 7165, loss 0.220185, acc 0.94
2016-05-21T07:45:02.181845: train-step 7166, loss 0.155205, acc 0.99
2016-05-21T07:45:08.376574: train-step 7167, loss 0.175392, acc 0.96
2016-05-21T07:45:14.605057: train-step 7168, loss 0.141064, acc 1
2016-05-21T07:45:20.812289: train-step 7169, loss 0.163885, acc 0.99
2016-05-21T07:45:27.127645: train-step 7170, loss 0.202236, acc 0.96
2016-05-21T07:45:33.304840: train-step 7171, loss 0.215357, acc 0.95
2016-05-21T07:45:39.495901: train-step 7172, loss 0.193912, acc 0.96
2016-05-21T07:45:45.749039: train-step 7173, loss 0.147696, acc 0.99
2016-05-21T07:45:51.986349: train-step 7174, loss 0.18152, acc 0.97
2016-05-21T07:45:58.169528: train-step 7175, loss 0.169963, acc 0.97
2016-05-21T07:46:04.415814: train-step 7176, loss 0.15941, acc 0.98
2016-05-21T07:46:10.744567: train-step 7177, loss 0.229452, acc 0.94
2016-05-21T07:46:16.923515: train-step 7178, loss 0.198324, acc 0.95
2016-05-21T07:46:23.120208: train-step 7179, loss 0.165456, acc 0.99
2016-05-21T07:46:29.328011: train-step 7180, loss 0.165151, acc 0.99
2016-05-21T07:46:35.658776: train-step 7181, loss 0.166005, acc 0.98
2016-05-21T07:46:41.916432: train-step 7182, loss 0.217591, acc 0.95
2016-05-21T07:46:48.168130: train-step 7183, loss 0.188031, acc 0.97
2016-05-21T07:46:54.385698: train-step 7184, loss 0.165329, acc 0.98
2016-05-21T07:47:00.628311: train-step 7185, loss 0.189429, acc 1
2016-05-21T07:47:07.012881: train-step 7186, loss 0.171202, acc 0.98
2016-05-21T07:47:13.225913: train-step 7187, loss 0.20868, acc 0.95
2016-05-21T07:47:19.477103: train-step 7188, loss 0.150124, acc 0.97
2016-05-21T07:47:25.716244: train-step 7189, loss 0.135261, acc 0.99
2016-05-21T07:47:32.024894: train-step 7190, loss 0.143137, acc 0.99
2016-05-21T07:47:38.597025: train-step 7191, loss 0.174329, acc 0.99
2016-05-21T07:47:44.875266: train-step 7192, loss 0.189077, acc 0.98
2016-05-21T07:47:51.091470: train-step 7193, loss 0.181517, acc 0.98
2016-05-21T07:47:57.251809: train-step 7194, loss 0.154587, acc 0.97
2016-05-21T07:48:03.488952: train-step 7195, loss 0.174998, acc 0.97
2016-05-21T07:48:09.690765: train-step 7196, loss 0.191668, acc 0.97
2016-05-21T07:48:15.833430: train-step 7197, loss 0.171831, acc 0.99
2016-05-21T07:48:22.046548: train-step 7198, loss 0.201612, acc 0.97
2016-05-21T07:48:28.180537: train-step 7199, loss 0.191681, acc 0.97
2016-05-21T07:48:34.635464: train-step 7200, loss 0.211906, acc 0.95
2016-05-21T07:48:41.084934: train-step 7201, loss 0.224926, acc 0.96
2016-05-21T07:48:47.329920: train-step 7202, loss 0.185854, acc 0.97
2016-05-21T07:48:53.579050: train-step 7203, loss 0.221669, acc 0.94
2016-05-21T07:48:59.824236: train-step 7204, loss 0.202935, acc 0.96
2016-05-21T07:49:05.979882: train-step 7205, loss 0.211526, acc 0.96
2016-05-21T07:49:12.308406: train-step 7206, loss 0.174298, acc 0.98
2016-05-21T07:49:18.909128: train-step 7207, loss 0.215763, acc 0.95
2016-05-21T07:49:25.192978: train-step 7208, loss 0.229083, acc 0.96
2016-05-21T07:49:31.452271: train-step 7209, loss 0.164522, acc 1
2016-05-21T07:49:37.581853: train-step 7210, loss 0.157923, acc 0.97
2016-05-21T07:49:43.753468: train-step 7211, loss 0.21075, acc 0.96
2016-05-21T07:49:49.977693: train-step 7212, loss 0.152093, acc 0.99
2016-05-21T07:49:56.143262: train-step 7213, loss 0.212626, acc 0.96
2016-05-21T07:50:02.368077: train-step 7214, loss 0.185754, acc 0.98
2016-05-21T07:50:08.524708: train-step 7215, loss 0.185498, acc 0.97
2016-05-21T07:50:14.681333: train-step 7216, loss 0.184971, acc 0.96
2016-05-21T07:50:20.921231: train-step 7217, loss 0.149836, acc 0.99
2016-05-21T07:50:27.321523: train-step 7218, loss 0.181928, acc 0.98
2016-05-21T07:50:33.523422: train-step 7219, loss 0.189959, acc 0.97
2016-05-21T07:50:39.723261: train-step 7220, loss 0.154253, acc 0.98
2016-05-21T07:50:46.131438: train-step 7221, loss 0.225427, acc 0.93
2016-05-21T07:50:52.468953: train-step 7222, loss 0.174658, acc 0.99
2016-05-21T07:50:58.666395: train-step 7223, loss 0.192501, acc 0.97
2016-05-21T07:51:04.897077: train-step 7224, loss 0.178889, acc 0.97
2016-05-21T07:51:11.037465: train-step 7225, loss 0.165609, acc 0.97
2016-05-21T07:51:17.177072: train-step 7226, loss 0.176463, acc 0.95
2016-05-21T07:51:23.373012: train-step 7227, loss 0.176019, acc 0.96
2016-05-21T07:51:29.601043: train-step 7228, loss 0.194664, acc 0.96
2016-05-21T07:51:35.854344: train-step 7229, loss 0.203601, acc 0.96
2016-05-21T07:51:41.983364: train-step 7230, loss 0.193477, acc 0.97
2016-05-21T07:51:48.336945: train-step 7231, loss 0.206917, acc 0.96
2016-05-21T07:51:54.603661: train-step 7232, loss 0.194443, acc 0.99
2016-05-21T07:52:00.798189: train-step 7233, loss 0.184328, acc 0.95
2016-05-21T07:52:06.982754: train-step 7234, loss 0.195596, acc 0.97
2016-05-21T07:52:13.163970: train-step 7235, loss 0.179841, acc 0.97
2016-05-21T07:52:19.331187: train-step 7236, loss 0.248687, acc 0.94
2016-05-21T07:52:25.477326: train-step 7237, loss 0.199934, acc 0.96
2016-05-21T07:52:31.651145: train-step 7238, loss 0.196052, acc 0.98
2016-05-21T07:52:37.982696: train-step 7239, loss 0.165244, acc 0.98
2016-05-21T07:52:44.232592: train-step 7240, loss 0.205986, acc 0.96
2016-05-21T07:52:50.469344: train-step 7241, loss 0.15753, acc 0.97
2016-05-21T07:52:56.648657: train-step 7242, loss 0.168182, acc 0.98
2016-05-21T07:53:02.855867: train-step 7243, loss 0.150206, acc 0.98
2016-05-21T07:53:09.071089: train-step 7244, loss 0.235235, acc 0.93
2016-05-21T07:53:15.382331: train-step 7245, loss 0.213613, acc 0.97
2016-05-21T07:53:21.531334: train-step 7246, loss 0.160941, acc 0.98
2016-05-21T07:53:27.758797: train-step 7247, loss 0.169817, acc 0.99
2016-05-21T07:53:33.935911: train-step 7248, loss 0.180783, acc 0.96
2016-05-21T07:53:40.332359: train-step 7249, loss 0.22842, acc 0.93
2016-05-21T07:53:46.575155: train-step 7250, loss 0.247657, acc 0.91
epoch number is: 29
2016-05-21T07:53:52.939650: train-step 7251, loss 0.161279, acc 0.98
2016-05-21T07:53:59.140940: train-step 7252, loss 0.19408, acc 0.98
2016-05-21T07:54:05.304964: train-step 7253, loss 0.148459, acc 0.99
2016-05-21T07:54:11.565801: train-step 7254, loss 0.179756, acc 0.97
2016-05-21T07:54:17.788362: train-step 7255, loss 0.162729, acc 0.97
2016-05-21T07:54:24.046433: train-step 7256, loss 0.174367, acc 0.97
2016-05-21T07:54:30.246192: train-step 7257, loss 0.181201, acc 0.96
2016-05-21T07:54:36.490074: train-step 7258, loss 0.207881, acc 0.95
2016-05-21T07:54:42.717081: train-step 7259, loss 0.149155, acc 0.98
2016-05-21T07:54:49.322721: train-step 7260, loss 0.162746, acc 0.98
2016-05-21T07:54:55.648029: train-step 7261, loss 0.177378, acc 0.97
2016-05-21T07:55:01.890231: train-step 7262, loss 0.18927, acc 0.97
2016-05-21T07:55:08.084858: train-step 7263, loss 0.163244, acc 0.99
2016-05-21T07:55:14.278402: train-step 7264, loss 0.174488, acc 0.95
2016-05-21T07:55:20.650987: train-step 7265, loss 0.178333, acc 0.98
2016-05-21T07:55:27.187885: train-step 7266, loss 0.198462, acc 0.97
2016-05-21T07:55:33.462352: train-step 7267, loss 0.160311, acc 0.99
2016-05-21T07:55:39.680087: train-step 7268, loss 0.255474, acc 0.92
2016-05-21T07:55:45.865712: train-step 7269, loss 0.160242, acc 0.99
2016-05-21T07:55:52.039829: train-step 7270, loss 0.186332, acc 0.95
2016-05-21T07:55:58.266924: train-step 7271, loss 0.156422, acc 0.99
2016-05-21T07:56:04.529103: train-step 7272, loss 0.13093, acc 0.99
2016-05-21T07:56:10.685725: train-step 7273, loss 0.14797, acc 0.99
2016-05-21T07:56:16.921626: train-step 7274, loss 0.172385, acc 0.99
2016-05-21T07:56:23.295360: train-step 7275, loss 0.185053, acc 0.96
2016-05-21T07:56:29.491047: train-step 7276, loss 0.178676, acc 0.99
2016-05-21T07:56:35.635062: train-step 7277, loss 0.164515, acc 0.98
2016-05-21T07:56:41.768923: train-step 7278, loss 0.187763, acc 0.95
2016-05-21T07:56:48.123454: train-step 7279, loss 0.185043, acc 0.97
2016-05-21T07:56:54.322935: train-step 7280, loss 0.179548, acc 0.97
2016-05-21T07:57:00.522316: train-step 7281, loss 0.195889, acc 0.96
2016-05-21T07:57:06.767543: train-step 7282, loss 0.180055, acc 0.98
2016-05-21T07:57:12.971619: train-step 7283, loss 0.22521, acc 0.94
2016-05-21T07:57:19.463232: train-step 7284, loss 0.194934, acc 0.95
2016-05-21T07:57:25.847086: train-step 7285, loss 0.184996, acc 0.96
2016-05-21T07:57:32.099754: train-step 7286, loss 0.176536, acc 0.96
2016-05-21T07:57:38.312094: train-step 7287, loss 0.189879, acc 0.95
2016-05-21T07:57:44.479896: train-step 7288, loss 0.183019, acc 0.97
2016-05-21T07:57:50.667004: train-step 7289, loss 0.167106, acc 0.99
2016-05-21T07:57:56.854245: train-step 7290, loss 0.167081, acc 0.99
2016-05-21T07:58:03.028757: train-step 7291, loss 0.17079, acc 0.98
2016-05-21T07:58:09.222988: train-step 7292, loss 0.205747, acc 0.98
2016-05-21T07:58:15.383592: train-step 7293, loss 0.165058, acc 0.97
2016-05-21T07:58:21.576983: train-step 7294, loss 0.199277, acc 0.96
2016-05-21T07:58:27.824145: train-step 7295, loss 0.166204, acc 0.99
2016-05-21T07:58:34.023912: train-step 7296, loss 0.158421, acc 0.98
2016-05-21T07:58:40.195377: train-step 7297, loss 0.153322, acc 0.99
2016-05-21T07:58:46.447435: train-step 7298, loss 0.162943, acc 0.97
2016-05-21T07:58:52.641262: train-step 7299, loss 0.163702, acc 0.98
2016-05-21T07:58:58.841633: train-step 7300, loss 0.215684, acc 0.96
2016-05-21T07:59:05.094499: train-step 7301, loss 0.139714, acc 1
2016-05-21T07:59:11.404640: train-step 7302, loss 0.184834, acc 0.96
2016-05-21T07:59:17.782408: train-step 7303, loss 0.19193, acc 0.96
2016-05-21T07:59:24.021188: train-step 7304, loss 0.210677, acc 0.95
2016-05-21T07:59:30.202555: train-step 7305, loss 0.161658, acc 0.98
2016-05-21T07:59:36.407492: train-step 7306, loss 0.155708, acc 1
2016-05-21T07:59:42.624522: train-step 7307, loss 0.191135, acc 0.96
2016-05-21T07:59:48.844636: train-step 7308, loss 0.156842, acc 0.99
2016-05-21T07:59:55.055067: train-step 7309, loss 0.188979, acc 0.97
2016-05-21T08:00:01.228844: train-step 7310, loss 0.209155, acc 0.96
2016-05-21T08:00:07.402617: train-step 7311, loss 0.150695, acc 0.99
2016-05-21T08:00:13.609605: train-step 7312, loss 0.235547, acc 0.94
2016-05-21T08:00:19.824602: train-step 7313, loss 0.161218, acc 0.98
2016-05-21T08:00:26.042705: train-step 7314, loss 0.200632, acc 0.97
2016-05-21T08:00:32.268000: train-step 7315, loss 0.160731, acc 0.99
2016-05-21T08:00:38.477793: train-step 7316, loss 0.17491, acc 0.98
2016-05-21T08:00:44.908312: train-step 7317, loss 0.189533, acc 0.96
2016-05-21T08:00:51.258111: train-step 7318, loss 0.168786, acc 0.96
2016-05-21T08:00:57.649624: train-step 7319, loss 0.185609, acc 0.97
2016-05-21T08:01:03.857642: train-step 7320, loss 0.190403, acc 0.96
2016-05-21T08:01:10.084706: train-step 7321, loss 0.197709, acc 0.97
2016-05-21T08:01:16.440821: train-step 7322, loss 0.145668, acc 0.99
2016-05-21T08:01:22.697357: train-step 7323, loss 0.163853, acc 0.98
2016-05-21T08:01:28.908977: train-step 7324, loss 0.154927, acc 0.95
2016-05-21T08:01:35.072003: train-step 7325, loss 0.171363, acc 0.99
2016-05-21T08:01:41.267618: train-step 7326, loss 0.19434, acc 0.98
2016-05-21T08:01:47.646367: train-step 7327, loss 0.150645, acc 0.97
2016-05-21T08:01:53.848081: train-step 7328, loss 0.148974, acc 0.99
2016-05-21T08:02:00.061578: train-step 7329, loss 0.156366, acc 0.98
2016-05-21T08:02:06.248684: train-step 7330, loss 0.196019, acc 0.97
2016-05-21T08:02:12.455628: train-step 7331, loss 0.154661, acc 0.99
2016-05-21T08:02:18.692431: train-step 7332, loss 0.163608, acc 0.98
2016-05-21T08:02:24.890441: train-step 7333, loss 0.143978, acc 1
2016-05-21T08:02:31.117505: train-step 7334, loss 0.185733, acc 0.97
2016-05-21T08:02:37.296428: train-step 7335, loss 0.176333, acc 0.99
2016-05-21T08:02:43.533464: train-step 7336, loss 0.160399, acc 0.98
2016-05-21T08:02:49.795012: train-step 7337, loss 0.18766, acc 0.96
2016-05-21T08:02:55.993771: train-step 7338, loss 0.191223, acc 0.95
2016-05-21T08:03:02.187527: train-step 7339, loss 0.234546, acc 0.95
2016-05-21T08:03:08.367634: train-step 7340, loss 0.216225, acc 0.94
2016-05-21T08:03:14.601199: train-step 7341, loss 0.208144, acc 0.95
2016-05-21T08:03:20.820095: train-step 7342, loss 0.142045, acc 0.99
2016-05-21T08:03:27.067447: train-step 7343, loss 0.146627, acc 0.99
2016-05-21T08:03:33.264616: train-step 7344, loss 0.148812, acc 0.97
2016-05-21T08:03:39.486770: train-step 7345, loss 0.239881, acc 0.92
2016-05-21T08:03:45.714388: train-step 7346, loss 0.231908, acc 0.93
2016-05-21T08:03:51.904774: train-step 7347, loss 0.163041, acc 0.98
2016-05-21T08:03:58.097220: train-step 7348, loss 0.181396, acc 0.97
2016-05-21T08:04:04.396324: train-step 7349, loss 0.185733, acc 0.97
2016-05-21T08:04:10.731373: train-step 7350, loss 0.21941, acc 0.94
2016-05-21T08:04:16.903276: train-step 7351, loss 0.177699, acc 0.95
2016-05-21T08:04:23.151263: train-step 7352, loss 0.192517, acc 0.98
2016-05-21T08:04:29.432455: train-step 7353, loss 0.190142, acc 0.97
2016-05-21T08:04:35.625970: train-step 7354, loss 0.159118, acc 0.98
2016-05-21T08:04:41.831692: train-step 7355, loss 0.147792, acc 0.97
2016-05-21T08:04:48.235584: train-step 7356, loss 0.182501, acc 0.95
2016-05-21T08:04:54.468327: train-step 7357, loss 0.197305, acc 0.98
2016-05-21T08:05:00.687393: train-step 7358, loss 0.180051, acc 0.98
2016-05-21T08:05:07.129000: train-step 7359, loss 0.188305, acc 0.94
2016-05-21T08:05:13.593664: train-step 7360, loss 0.223716, acc 0.93
2016-05-21T08:05:19.751662: train-step 7361, loss 0.140861, acc 1
2016-05-21T08:05:26.017140: train-step 7362, loss 0.162473, acc 0.98
2016-05-21T08:05:32.236537: train-step 7363, loss 0.23685, acc 0.93
2016-05-21T08:05:38.467396: train-step 7364, loss 0.16204, acc 0.98
2016-05-21T08:05:44.604446: train-step 7365, loss 0.240986, acc 0.94
2016-05-21T08:05:50.920276: train-step 7366, loss 0.15272, acc 1
2016-05-21T08:05:57.151435: train-step 7367, loss 0.15585, acc 0.96
2016-05-21T08:06:03.372912: train-step 7368, loss 0.185574, acc 0.98
2016-05-21T08:06:09.582515: train-step 7369, loss 0.138266, acc 0.99
2016-05-21T08:06:15.761226: train-step 7370, loss 0.177316, acc 0.98
2016-05-21T08:06:21.997590: train-step 7371, loss 0.138778, acc 0.99
2016-05-21T08:06:28.336225: train-step 7372, loss 0.185044, acc 0.98
2016-05-21T08:06:34.564740: train-step 7373, loss 0.184215, acc 0.95
2016-05-21T08:06:40.796929: train-step 7374, loss 0.229506, acc 0.93
2016-05-21T08:06:47.191970: train-step 7375, loss 0.206131, acc 0.96
2016-05-21T08:06:53.341304: train-step 7376, loss 0.198137, acc 0.96
2016-05-21T08:06:59.539005: train-step 7377, loss 0.223582, acc 0.95
2016-05-21T08:07:05.773852: train-step 7378, loss 0.190863, acc 0.95
2016-05-21T08:07:11.947829: train-step 7379, loss 0.185481, acc 0.98
2016-05-21T08:07:18.169150: train-step 7380, loss 0.157301, acc 0.98
2016-05-21T08:07:24.385105: train-step 7381, loss 0.185823, acc 0.97
2016-05-21T08:07:30.550668: train-step 7382, loss 0.174385, acc 0.98
2016-05-21T08:07:36.727988: train-step 7383, loss 0.160035, acc 0.99
2016-05-21T08:07:42.931351: train-step 7384, loss 0.162551, acc 0.96
2016-05-21T08:07:49.170721: train-step 7385, loss 0.162307, acc 0.98
2016-05-21T08:07:55.356043: train-step 7386, loss 0.201229, acc 0.97
2016-05-21T08:08:01.592021: train-step 7387, loss 0.156906, acc 0.98
2016-05-21T08:08:07.799617: train-step 7388, loss 0.143868, acc 0.99
2016-05-21T08:08:13.969554: train-step 7389, loss 0.171085, acc 0.97
2016-05-21T08:08:20.191578: train-step 7390, loss 0.201257, acc 0.94
2016-05-21T08:08:26.337508: train-step 7391, loss 0.21079, acc 0.94
2016-05-21T08:08:32.494952: train-step 7392, loss 0.166355, acc 0.99
2016-05-21T08:08:38.650797: train-step 7393, loss 0.262243, acc 0.89
2016-05-21T08:08:44.891536: train-step 7394, loss 0.201775, acc 0.95
2016-05-21T08:08:51.099878: train-step 7395, loss 0.195512, acc 0.93
2016-05-21T08:08:57.302394: train-step 7396, loss 0.167888, acc 1
2016-05-21T08:09:03.521088: train-step 7397, loss 0.173064, acc 0.97
2016-05-21T08:09:09.677943: train-step 7398, loss 0.160402, acc 0.98
2016-05-21T08:09:15.899455: train-step 7399, loss 0.178226, acc 0.97
2016-05-21T08:09:22.114766: train-step 7400, loss 0.164905, acc 0.98
2016-05-21T08:09:28.290809: train-step 7401, loss 0.142763, acc 1
2016-05-21T08:09:34.497768: train-step 7402, loss 0.193667, acc 0.97
2016-05-21T08:09:40.655955: train-step 7403, loss 0.240159, acc 0.93
2016-05-21T08:09:46.839881: train-step 7404, loss 0.165306, acc 0.96
2016-05-21T08:09:52.947127: train-step 7405, loss 0.155102, acc 0.99
2016-05-21T08:09:59.167564: train-step 7406, loss 0.169695, acc 0.98
2016-05-21T08:10:05.405163: train-step 7407, loss 0.19705, acc 0.96
2016-05-21T08:10:11.633918: train-step 7408, loss 0.201954, acc 0.96
2016-05-21T08:10:17.799178: train-step 7409, loss 0.208106, acc 0.96
2016-05-21T08:10:23.935524: train-step 7410, loss 0.149732, acc 0.98
2016-05-21T08:10:30.150451: train-step 7411, loss 0.171649, acc 0.98
2016-05-21T08:10:36.359882: train-step 7412, loss 0.186867, acc 0.96
2016-05-21T08:10:42.555007: train-step 7413, loss 0.199352, acc 0.96
2016-05-21T08:10:48.721717: train-step 7414, loss 0.186279, acc 0.97
2016-05-21T08:10:54.940419: train-step 7415, loss 0.201294, acc 0.96
2016-05-21T08:11:01.134676: train-step 7416, loss 0.171633, acc 0.97
2016-05-21T08:11:07.392018: train-step 7417, loss 0.174234, acc 0.97
2016-05-21T08:11:13.599303: train-step 7418, loss 0.241672, acc 0.93
2016-05-21T08:11:19.815810: train-step 7419, loss 0.146153, acc 0.98
2016-05-21T08:11:26.062594: train-step 7420, loss 0.171631, acc 0.98
2016-05-21T08:11:32.254174: train-step 7421, loss 0.196146, acc 0.97
2016-05-21T08:11:38.456170: train-step 7422, loss 0.142006, acc 0.99
2016-05-21T08:11:44.588085: train-step 7423, loss 0.204684, acc 0.96
2016-05-21T08:11:50.764399: train-step 7424, loss 0.177426, acc 0.98
2016-05-21T08:11:56.922979: train-step 7425, loss 0.218575, acc 0.92
2016-05-21T08:12:03.153542: train-step 7426, loss 0.172554, acc 0.98
2016-05-21T08:12:10.190963: train-step 7427, loss 0.176796, acc 0.95
2016-05-21T08:12:17.003704: train-step 7428, loss 0.176248, acc 0.98
2016-05-21T08:12:23.396378: train-step 7429, loss 0.172282, acc 0.98
2016-05-21T08:12:29.587932: train-step 7430, loss 0.172181, acc 0.98
2016-05-21T08:12:35.792303: train-step 7431, loss 0.224982, acc 0.92
2016-05-21T08:12:42.117990: train-step 7432, loss 0.173726, acc 0.98
2016-05-21T08:12:48.306237: train-step 7433, loss 0.181529, acc 0.96
2016-05-21T08:12:54.555531: train-step 7434, loss 0.162147, acc 0.99
2016-05-21T08:13:00.775277: train-step 7435, loss 0.174753, acc 0.98
2016-05-21T08:13:06.990460: train-step 7436, loss 0.185999, acc 0.96
2016-05-21T08:13:13.211148: train-step 7437, loss 0.19607, acc 0.96
2016-05-21T08:13:19.599064: train-step 7438, loss 0.166212, acc 0.98
2016-05-21T08:13:25.774322: train-step 7439, loss 0.259374, acc 0.93
2016-05-21T08:13:32.025206: train-step 7440, loss 0.159485, acc 0.98
2016-05-21T08:13:38.316697: train-step 7441, loss 0.1566, acc 0.98
2016-05-21T08:13:44.505835: train-step 7442, loss 0.213076, acc 0.95
2016-05-21T08:13:50.710778: train-step 7443, loss 0.180871, acc 0.96
2016-05-21T08:13:56.964406: train-step 7444, loss 0.178616, acc 0.97
2016-05-21T08:14:03.154591: train-step 7445, loss 0.195967, acc 0.95
2016-05-21T08:14:09.297048: train-step 7446, loss 0.190182, acc 0.96
2016-05-21T08:14:15.688610: train-step 7447, loss 0.159608, acc 0.99
2016-05-21T08:14:21.873762: train-step 7448, loss 0.186137, acc 0.98
2016-05-21T08:14:28.103959: train-step 7449, loss 0.150716, acc 0.99
2016-05-21T08:14:34.360091: train-step 7450, loss 0.221494, acc 0.96
2016-05-21T08:14:40.547831: train-step 7451, loss 0.204986, acc 0.95
2016-05-21T08:14:46.711283: train-step 7452, loss 0.185919, acc 0.97
2016-05-21T08:14:52.934682: train-step 7453, loss 0.172483, acc 0.97
2016-05-21T08:14:59.143339: train-step 7454, loss 0.236478, acc 0.93
2016-05-21T08:15:05.509143: train-step 7455, loss 0.232395, acc 0.94
2016-05-21T08:15:11.752052: train-step 7456, loss 0.211134, acc 0.95
2016-05-21T08:15:18.003880: train-step 7457, loss 0.157282, acc 0.97
2016-05-21T08:15:24.159610: train-step 7458, loss 0.206102, acc 0.95
2016-05-21T08:15:30.435697: train-step 7459, loss 0.175015, acc 0.97
2016-05-21T08:15:36.805598: train-step 7460, loss 0.154872, acc 0.98
2016-05-21T08:15:43.028317: train-step 7461, loss 0.236641, acc 0.93
2016-05-21T08:15:49.188816: train-step 7462, loss 0.198639, acc 0.96
2016-05-21T08:15:55.574127: train-step 7463, loss 0.191836, acc 0.97
2016-05-21T08:16:01.746925: train-step 7464, loss 0.201469, acc 0.94
2016-05-21T08:16:07.950604: train-step 7465, loss 0.153845, acc 0.99
2016-05-21T08:16:14.149752: train-step 7466, loss 0.206342, acc 0.96
2016-05-21T08:16:20.394715: train-step 7467, loss 0.181334, acc 0.97
2016-05-21T08:16:26.690086: train-step 7468, loss 0.188207, acc 0.95
2016-05-21T08:16:32.906542: train-step 7469, loss 0.159006, acc 0.99
2016-05-21T08:16:39.124390: train-step 7470, loss 0.189957, acc 0.98
2016-05-21T08:16:45.699646: train-step 7471, loss 0.168759, acc 0.98
2016-05-21T08:16:52.080607: train-step 7472, loss 0.228707, acc 0.94
2016-05-21T08:16:58.433208: train-step 7473, loss 0.184827, acc 0.97
2016-05-21T08:17:04.653149: train-step 7474, loss 0.165603, acc 0.99
2016-05-21T08:17:10.911859: train-step 7475, loss 0.207052, acc 0.97
2016-05-21T08:17:17.133120: train-step 7476, loss 0.181828, acc 0.94
2016-05-21T08:17:23.299768: train-step 7477, loss 0.236326, acc 0.95
2016-05-21T08:17:29.508629: train-step 7478, loss 0.174685, acc 0.97
2016-05-21T08:17:35.769256: train-step 7479, loss 0.143411, acc 0.98
2016-05-21T08:17:42.137739: train-step 7480, loss 0.190559, acc 0.96
2016-05-21T08:17:48.377185: train-step 7481, loss 0.203818, acc 0.97
2016-05-21T08:17:54.578032: train-step 7482, loss 0.169406, acc 0.99
2016-05-21T08:18:00.764103: train-step 7483, loss 0.202562, acc 0.96
2016-05-21T08:18:06.969162: train-step 7484, loss 0.145708, acc 0.99
2016-05-21T08:18:13.164976: train-step 7485, loss 0.171774, acc 0.97
2016-05-21T08:18:19.457628: train-step 7486, loss 0.186948, acc 0.97
2016-05-21T08:18:25.775183: train-step 7487, loss 0.215759, acc 0.94
2016-05-21T08:18:31.977272: train-step 7488, loss 0.193132, acc 0.97
2016-05-21T08:18:38.213844: train-step 7489, loss 0.193428, acc 0.96
2016-05-21T08:18:44.993699: train-step 7490, loss 0.187083, acc 0.97
2016-05-21T08:18:51.605048: train-step 7491, loss 0.155872, acc 0.99
2016-05-21T08:18:57.912543: train-step 7492, loss 0.181762, acc 0.96
2016-05-21T08:19:04.137736: train-step 7493, loss 0.18025, acc 0.97
2016-05-21T08:19:10.756781: train-step 7494, loss 0.20935, acc 0.96
2016-05-21T08:19:17.041413: train-step 7495, loss 0.17376, acc 0.99
2016-05-21T08:19:24.546616: train-step 7496, loss 0.196668, acc 0.97
2016-05-21T08:19:30.761227: train-step 7497, loss 0.185914, acc 0.97
2016-05-21T08:19:37.072259: train-step 7498, loss 0.190667, acc 0.97
2016-05-21T08:19:43.306946: train-step 7499, loss 0.177347, acc 0.98
2016-05-21T08:19:49.469873: train-step 7500, loss 0.165367, acc 0.99
2016-05-21T08:19:52.457580  dev-step: 1  acc: 0.89
2016-05-21T08:19:55.301133  dev-step: 2  acc: 0.93
2016-05-21T08:19:58.083516  dev-step: 3  acc: 0.96
2016-05-21T08:20:00.766627  dev-step: 4  acc: 0.94
2016-05-21T08:20:03.347011  dev-step: 5  acc: 0.92
2016-05-21T08:20:05.913130  dev-step: 6  acc: 0.95
2016-05-21T08:20:08.464669  dev-step: 7  acc: 0.92
2016-05-21T08:20:11.081651  dev-step: 8  acc: 0.93
2016-05-21T08:20:13.659270  dev-step: 9  acc: 0.96
2016-05-21T08:20:16.218588  dev-step: 10  acc: 0.97
2016-05-21T08:20:18.686814  dev-step: 11  acc: 0.91
2016-05-21T08:20:21.160218  dev-step: 12  acc: 0.97
2016-05-21T08:20:23.689565  dev-step: 13  acc: 0.93
2016-05-21T08:20:26.194060  dev-step: 14  acc: 0.89
2016-05-21T08:20:28.696207  dev-step: 15  acc: 0.89
2016-05-21T08:20:31.249284  dev-step: 16  acc: 0.92
2016-05-21T08:20:33.864761  dev-step: 17  acc: 0.89
2016-05-21T08:20:36.392894  dev-step: 18  acc: 0.95
2016-05-21T08:20:38.940218  dev-step: 19  acc: 0.92
2016-05-21T08:20:41.525404  dev-step: 20  acc: 0.92
2016-05-21T08:20:44.037640  dev-step: 21  acc: 0.94
2016-05-21T08:20:46.631648  dev-step: 22  acc: 0.9
2016-05-21T08:20:49.212678  dev-step: 23  acc: 0.93
2016-05-21T08:20:51.723991  dev-step: 24  acc: 0.94
2016-05-21T08:20:54.204237  dev-step: 25  acc: 0.86
2016-05-21T08:20:56.670833  dev-step: 26  acc: 0.94
2016-05-21T08:20:59.227474  dev-step: 27  acc: 0.82
2016-05-21T08:21:01.821288  dev-step: 28  acc: 0.91
2016-05-21T08:21:04.567650  dev-step: 29  acc: 0.95
2016-05-21T08:21:07.229467  dev-step: 30  acc: 0.88
2016-05-21T08:21:09.858911  dev-step: 31  acc: 0.92
2016-05-21T08:21:12.443257  dev-step: 32  acc: 0.84
2016-05-21T08:21:15.075570  dev-step: 33  acc: 0.96
2016-05-21T08:21:17.711443  dev-step: 34  acc: 0.92
2016-05-21T08:21:20.267653  dev-step: 35  acc: 0.95
2016-05-21T08:21:22.761240  dev-step: 36  acc: 0.91
2016-05-21T08:21:25.284575  dev-step: 37  acc: 0.95
2016-05-21T08:21:27.815643  dev-step: 38  acc: 0.9
2016-05-21T08:21:30.335268  dev-step: 39  acc: 0.92
2016-05-21T08:21:32.842243  dev-step: 40  acc: 0.95
2016-05-21T08:21:35.415503  dev-step: 41  acc: 0.93
2016-05-21T08:21:37.966569  dev-step: 42  acc: 0.92
2016-05-21T08:21:40.495248  dev-step: 43  acc: 0.88
2016-05-21T08:21:43.070037  dev-step: 44  acc: 0.9
2016-05-21T08:21:45.633207  dev-step: 45  acc: 0.91
2016-05-21T08:21:48.156615  dev-step: 46  acc: 0.88
2016-05-21T08:21:50.695708  dev-step: 47  acc: 0.85
2016-05-21T08:21:53.217394  dev-step: 48  acc: 0.92
2016-05-21T08:21:55.759151  dev-step: 49  acc: 0.98
2016-05-21T08:21:58.234112  dev-step: 50  acc: 0.92
2016-05-21T08:22:00.758869  dev-step: 51  acc: 0.95
2016-05-21T08:22:03.300591  dev-step: 52  acc: 0.94
2016-05-21T08:22:05.906719  dev-step: 53  acc: 0.92
2016-05-21T08:22:08.434683  dev-step: 54  acc: 0.98
2016-05-21T08:22:11.036634  dev-step: 55  acc: 0.88
2016-05-21T08:22:13.664707  dev-step: 56  acc: 0.9
2016-05-21T08:22:16.240219  dev-step: 57  acc: 0.9
2016-05-21T08:22:18.845916  dev-step: 58  acc: 0.91
2016-05-21T08:22:21.483003  dev-step: 59  acc: 0.94
2016-05-21T08:22:24.039162  dev-step: 60  acc: 0.91
2016-05-21T08:22:26.605903  dev-step: 61  acc: 0.91
2016-05-21T08:22:29.247020  dev-step: 62  acc: 0.88
2016-05-21T08:22:31.805230  dev-step: 63  acc: 0.89
2016-05-21T08:22:34.350003  dev-step: 64  acc: 0.97
2016-05-21T08:22:36.877618  dev-step: 65  acc: 0.85
2016-05-21T08:22:39.394995  dev-step: 66  acc: 0.9
2016-05-21T08:22:41.937543  dev-step: 67  acc: 0.92
2016-05-21T08:22:44.443305  dev-step: 68  acc: 0.93
2016-05-21T08:22:46.949107  dev-step: 69  acc: 0.97
2016-05-21T08:22:49.436147  dev-step: 70  acc: 0.96
2016-05-21T08:22:51.889684  dev-step: 71  acc: 0.94
2016-05-21T08:22:54.428394  dev-step: 72  acc: 0.94
2016-05-21T08:22:57.004555  dev-step: 73  acc: 0.94
2016-05-21T08:22:59.540133  dev-step: 74  acc: 0.92
2016-05-21T08:23:02.019208  dev-step: 75  acc: 0.95
2016-05-21T08:23:04.524644  dev-step: 76  acc: 0.92
2016-05-21T08:23:07.187654  dev-step: 77  acc: 0.91
2016-05-21T08:23:09.906008  dev-step: 78  acc: 0.92
2016-05-21T08:23:12.554393  dev-step: 79  acc: 0.91
2016-05-21T08:23:15.087884  dev-step: 80  acc: 0.94
2016-05-21T08:23:17.601567  dev-step: 81  acc: 0.95
2016-05-21T08:23:20.074764  dev-step: 82  acc: 0.9
2016-05-21T08:23:22.608738  dev-step: 83  acc: 0.93
2016-05-21T08:23:25.169436  dev-step: 84  acc: 0.9
2016-05-21T08:23:27.694559  dev-step: 85  acc: 0.94
2016-05-21T08:23:30.233693  dev-step: 86  acc: 0.97
2016-05-21T08:23:32.799448  dev-step: 87  acc: 0.9
2016-05-21T08:23:35.481174  dev-step: 88  acc: 0.95
2016-05-21T08:23:38.205874  dev-step: 89  acc: 0.91
2016-05-21T08:23:40.804407  dev-step: 90  acc: 0.94
2016-05-21T08:23:43.345219  dev-step: 91  acc: 0.87
2016-05-21T08:23:45.865779  dev-step: 92  acc: 0.94
2016-05-21T08:23:48.396305  dev-step: 93  acc: 0.91
2016-05-21T08:23:50.897200  dev-step: 94  acc: 0.96
2016-05-21T08:23:53.449693  dev-step: 95  acc: 0.98
2016-05-21T08:23:55.964950  dev-step: 96  acc: 0.92
2016-05-21T08:23:58.481519  dev-step: 97  acc: 0.85
2016-05-21T08:24:01.052837  dev-step: 98  acc: 0.88
2016-05-21T08:24:03.501562  dev-step: 99  acc: 0.87
2016-05-21T08:24:06.034228  dev-step: 100  acc: 0.96
2016-05-21T08:24:08.593506  dev-step: 101  acc: 0.96
2016-05-21T08:24:11.302210  dev-step: 102  acc: 0.93
2016-05-21T08:24:14.045910  dev-step: 103  acc: 0.95
2016-05-21T08:24:16.677013  dev-step: 104  acc: 0.89
2016-05-21T08:24:19.197874  dev-step: 105  acc: 0.93
2016-05-21T08:24:21.697184  dev-step: 106  acc: 0.95
2016-05-21T08:24:24.328883  dev-step: 107  acc: 0.93
2016-05-21T08:24:26.928643  dev-step: 108  acc: 0.92
2016-05-21T08:24:29.494244  dev-step: 109  acc: 0.87
2016-05-21T08:24:31.974750  dev-step: 110  acc: 0.86
2016-05-21T08:24:34.506813  dev-step: 111  acc: 0.91
2016-05-21T08:24:37.028935  dev-step: 112  acc: 0.92
2016-05-21T08:24:39.515546  dev-step: 113  acc: 0.95
2016-05-21T08:24:41.997807  dev-step: 114  acc: 0.9
2016-05-21T08:24:44.526055  dev-step: 115  acc: 0.91
2016-05-21T08:24:47.010613  dev-step: 116  acc: 0.94
2016-05-21T08:24:49.524599  dev-step: 117  acc: 0.92
2016-05-21T08:24:52.007396  dev-step: 118  acc: 0.91
2016-05-21T08:24:54.526271  dev-step: 119  acc: 0.87
2016-05-21T08:24:57.062740  dev-step: 120  acc: 0.87
2016-05-21T08:24:59.533990  dev-step: 121  acc: 0.89
2016-05-21T08:25:02.013890  dev-step: 122  acc: 0.83
2016-05-21T08:25:04.494966  dev-step: 123  acc: 0.93
2016-05-21T08:25:07.077655  dev-step: 124  acc: 0.9
2016-05-21T08:25:09.676668  dev-step: 125  acc: 0.95
2016-05-21T08:25:12.192957  dev-step: 126  acc: 0.91
2016-05-21T08:25:14.663296  dev-step: 127  acc: 0.9
2016-05-21T08:25:17.197916  dev-step: 128  acc: 0.91
2016-05-21T08:25:19.718275  dev-step: 129  acc: 0.88
2016-05-21T08:25:22.233867  dev-step: 130  acc: 0.91
2016-05-21T08:25:24.735717  dev-step: 131  acc: 0.96
2016-05-21T08:25:27.281377  dev-step: 132  acc: 0.96
2016-05-21T08:25:29.920633  dev-step: 133  acc: 0.97
2016-05-21T08:25:32.431121  dev-step: 134  acc: 0.86
2016-05-21T08:25:34.963134  dev-step: 135  acc: 0.96
2016-05-21T08:25:37.451748  dev-step: 136  acc: 0.91
2016-05-21T08:25:39.974398  dev-step: 137  acc: 0.95
2016-05-21T08:25:42.454879  dev-step: 138  acc: 0.94
2016-05-21T08:25:44.974606  dev-step: 139  acc: 0.85
2016-05-21T08:25:47.596770  dev-step: 140  acc: 0.85
2016-05-21T08:25:50.218799  dev-step: 141  acc: 0.89
2016-05-21T08:25:52.776417  dev-step: 142  acc: 0.85
2016-05-21T08:25:55.396934  dev-step: 143  acc: 0.91
2016-05-21T08:25:57.983515  dev-step: 144  acc: 0.91
2016-05-21T08:26:00.547439  dev-step: 145  acc: 0.84
2016-05-21T08:26:03.175569  dev-step: 146  acc: 0.97
2016-05-21T08:26:05.810427  dev-step: 147  acc: 0.88
2016-05-21T08:26:08.363117  dev-step: 148  acc: 0.91
2016-05-21T08:26:10.838296  dev-step: 149  acc: 0.84
2016-05-21T08:26:13.340211  dev-step: 150  acc: 0.91
2016-05-21T08:26:15.825488  dev-step: 151  acc: 0.92
2016-05-21T08:26:18.355405  dev-step: 152  acc: 0.87
2016-05-21T08:26:20.831143  dev-step: 153  acc: 0.92
2016-05-21T08:26:23.395185  dev-step: 154  acc: 0.88
2016-05-21T08:26:26.026481  dev-step: 155  acc: 0.9
2016-05-21T08:26:28.538082  dev-step: 156  acc: 0.83
2016-05-21T08:26:31.061614  dev-step: 157  acc: 0.91
2016-05-21T08:26:33.519649  dev-step: 158  acc: 0.9
2016-05-21T08:26:36.035338  dev-step: 159  acc: 0.92
2016-05-21T08:26:38.502825  dev-step: 160  acc: 0.91
2016-05-21T08:26:41.022109  dev-step: 161  acc: 0.94
2016-05-21T08:26:43.636877  dev-step: 162  acc: 0.96
2016-05-21T08:26:46.204937  dev-step: 163  acc: 0.93
2016-05-21T08:26:48.767308  dev-step: 164  acc: 0.85
2016-05-21T08:26:51.375310  dev-step: 165  acc: 0.93
2016-05-21T08:26:53.955199  dev-step: 166  acc: 0.84
2016-05-21T08:26:56.531373  dev-step: 167  acc: 0.86
2016-05-21T08:26:59.136374  dev-step: 168  acc: 0.92
2016-05-21T08:27:01.765604  dev-step: 169  acc: 0.87
2016-05-21T08:27:04.345253  dev-step: 170  acc: 0.92
2016-05-21T08:27:06.873258  dev-step: 171  acc: 0.93
2016-05-21T08:27:09.354788  dev-step: 172  acc: 0.9
2016-05-21T08:27:11.872885  dev-step: 173  acc: 0.94
2016-05-21T08:27:14.343126  dev-step: 174  acc: 0.96
2016-05-21T08:27:16.855808  dev-step: 175  acc: 0.89
2016-05-21T08:27:19.474682  dev-step: 176  acc: 0.86
2016-05-21T08:27:22.109640  dev-step: 177  acc: 0.93
2016-05-21T08:27:24.672436  dev-step: 178  acc: 0.89
2016-05-21T08:27:27.279135  dev-step: 179  acc: 0.84
2016-05-21T08:27:29.905850  dev-step: 180  acc: 0.89
2016-05-21T08:27:32.463419  dev-step: 181  acc: 0.89
2016-05-21T08:27:35.004172  dev-step: 182  acc: 0.96
2016-05-21T08:27:37.528055  dev-step: 183  acc: 0.93
2016-05-21T08:27:40.059432  dev-step: 184  acc: 0.92
2016-05-21T08:27:42.593356  dev-step: 185  acc: 0.95
2016-05-21T08:27:45.110743  dev-step: 186  acc: 0.86
2016-05-21T08:27:47.716279  dev-step: 187  acc: 0.92
2016-05-21T08:27:50.328305  dev-step: 188  acc: 0.89
2016-05-21T08:27:52.901683  dev-step: 189  acc: 0.86
2016-05-21T08:27:55.363631  dev-step: 190  acc: 0.91
2016-05-21T08:27:57.883966  dev-step: 191  acc: 0.89
2016-05-21T08:28:00.459445  dev-step: 192  acc: 0.94
2016-05-21T08:28:02.989387  dev-step: 193  acc: 0.88
2016-05-21T08:28:05.473423  dev-step: 194  acc: 0.94
2016-05-21T08:28:08.004470  dev-step: 195  acc: 0.94
2016-05-21T08:28:10.534752  dev-step: 196  acc: 0.91
2016-05-21T08:28:13.011455  dev-step: 197  acc: 0.9
2016-05-21T08:28:15.531238  dev-step: 198  acc: 0.95
2016-05-21T08:28:18.066787  dev-step: 199  acc: 0.93
2016-05-21T08:28:20.533679  dev-step: 200  acc: 0.88
2016-05-21T08:28:23.111904  dev-step: 201  acc: 0.89
2016-05-21T08:28:25.703552  dev-step: 202  acc: 0.94
2016-05-21T08:28:28.268598  dev-step: 203  acc: 0.94
2016-05-21T08:28:30.819866  dev-step: 204  acc: 0.91
2016-05-21T08:28:33.340839  dev-step: 205  acc: 0.92
2016-05-21T08:28:35.852548  dev-step: 206  acc: 0.92
2016-05-21T08:28:38.385168  dev-step: 207  acc: 0.87
2016-05-21T08:28:40.866714  dev-step: 208  acc: 0.91
2016-05-21T08:28:43.474279  dev-step: 209  acc: 0.87
2016-05-21T08:28:46.114339  dev-step: 210  acc: 0.91
2016-05-21T08:28:48.640411  dev-step: 211  acc: 0.89
2016-05-21T08:28:51.127392  dev-step: 212  acc: 0.91
2016-05-21T08:28:53.648897  dev-step: 213  acc: 0.9
2016-05-21T08:28:56.163250  dev-step: 214  acc: 0.85
2016-05-21T08:28:58.680945  dev-step: 215  acc: 0.83
2016-05-21T08:29:01.205214  dev-step: 216  acc: 0.9
2016-05-21T08:29:03.816532  dev-step: 217  acc: 0.89
2016-05-21T08:29:06.421914  dev-step: 218  acc: 0.81
2016-05-21T08:29:08.978121  dev-step: 219  acc: 0.86
2016-05-21T08:29:11.534761  dev-step: 220  acc: 0.92
2016-05-21T08:29:14.125303  dev-step: 221  acc: 0.9
2016-05-21T08:29:16.691363  dev-step: 222  acc: 0.94
2016-05-21T08:29:19.250043  dev-step: 223  acc: 0.94
2016-05-21T08:29:21.881110  dev-step: 224  acc: 0.86
2016-05-21T08:29:24.446052  dev-step: 225  acc: 0.94
2016-05-21T08:29:26.974379  dev-step: 226  acc: 0.87
2016-05-21T08:29:29.480763  dev-step: 227  acc: 0.91
2016-05-21T08:29:32.001519  dev-step: 228  acc: 0.86
2016-05-21T08:29:34.523007  dev-step: 229  acc: 0.91
2016-05-21T08:29:36.999934  dev-step: 230  acc: 0.9
2016-05-21T08:29:39.475355  dev-step: 231  acc: 0.89
2016-05-21T08:29:41.994509  dev-step: 232  acc: 0.87
2016-05-21T08:29:44.459945  dev-step: 233  acc: 0.9
2016-05-21T08:29:46.997208  dev-step: 234  acc: 0.95
2016-05-21T08:29:49.622774  dev-step: 235  acc: 0.94
2016-05-21T08:29:52.186861  dev-step: 236  acc: 0.86
2016-05-21T08:29:54.688749  dev-step: 237  acc: 0.86
2016-05-21T08:29:57.228392  dev-step: 238  acc: 0.92
2016-05-21T08:29:59.723524  dev-step: 239  acc: 0.91
2016-05-21T08:30:02.285474  dev-step: 240  acc: 0.81
2016-05-21T08:30:04.798149  dev-step: 241  acc: 0.84
2016-05-21T08:30:07.298709  dev-step: 242  acc: 0.86
2016-05-21T08:30:09.820196  dev-step: 243  acc: 0.84
2016-05-21T08:30:12.329610  dev-step: 244  acc: 0.91
2016-05-21T08:30:14.876039  dev-step: 245  acc: 0.83
2016-05-21T08:30:17.433702  dev-step: 246  acc: 0.87
2016-05-21T08:30:19.903521  dev-step: 247  acc: 0.93
2016-05-21T08:30:22.443575  dev-step: 248  acc: 0.89
2016-05-21T08:30:24.972978  dev-step: 249  acc: 0.78
2016-05-21T08:30:27.633223  dev-step: 250  acc: 0.88
avg_loss 0.287456, avg_acc 0.90864, real_acc 0.90864
epoch number is: 30
2016-05-21T08:30:34.282813: train-step 7501, loss 0.185413, acc 0.96
2016-05-21T08:30:40.644617: train-step 7502, loss 0.149872, acc 0.99
2016-05-21T08:30:47.013029: train-step 7503, loss 0.205563, acc 0.94
2016-05-21T08:30:53.322797: train-step 7504, loss 0.172163, acc 0.98
2016-05-21T08:30:59.498996: train-step 7505, loss 0.159396, acc 1
2016-05-21T08:31:05.749081: train-step 7506, loss 0.12532, acc 1
2016-05-21T08:31:11.961905: train-step 7507, loss 0.177681, acc 0.97
2016-05-21T08:31:18.263306: train-step 7508, loss 0.159637, acc 0.98
2016-05-21T08:31:24.625009: train-step 7509, loss 0.174468, acc 0.99
2016-05-21T08:31:30.960007: train-step 7510, loss 0.214869, acc 0.96
2016-05-21T08:31:37.190373: train-step 7511, loss 0.165799, acc 0.99
2016-05-21T08:31:43.360653: train-step 7512, loss 0.156414, acc 0.98
2016-05-21T08:31:49.550641: train-step 7513, loss 0.149423, acc 0.97
2016-05-21T08:31:55.772101: train-step 7514, loss 0.191604, acc 0.97
2016-05-21T08:32:01.928863: train-step 7515, loss 0.206249, acc 0.95
2016-05-21T08:32:08.119699: train-step 7516, loss 0.170608, acc 0.96
2016-05-21T08:32:14.313261: train-step 7517, loss 0.159601, acc 0.98
2016-05-21T08:32:20.488507: train-step 7518, loss 0.184283, acc 0.95
2016-05-21T08:32:26.711743: train-step 7519, loss 0.178753, acc 0.99
2016-05-21T08:32:32.880917: train-step 7520, loss 0.177906, acc 0.95
2016-05-21T08:32:39.101922: train-step 7521, loss 0.140514, acc 0.99
2016-05-21T08:32:45.311193: train-step 7522, loss 0.198376, acc 0.97
2016-05-21T08:32:51.554731: train-step 7523, loss 0.174932, acc 0.97
2016-05-21T08:32:57.764762: train-step 7524, loss 0.190152, acc 0.95
2016-05-21T08:33:03.972779: train-step 7525, loss 0.214027, acc 0.95
2016-05-21T08:33:10.315555: train-step 7526, loss 0.150097, acc 0.98
2016-05-21T08:33:16.507520: train-step 7527, loss 0.152266, acc 0.99
2016-05-21T08:33:22.711446: train-step 7528, loss 0.193526, acc 0.95
2016-05-21T08:33:28.939957: train-step 7529, loss 0.187069, acc 0.98
2016-05-21T08:33:35.201371: train-step 7530, loss 0.183567, acc 0.97
2016-05-21T08:33:41.505511: train-step 7531, loss 0.20368, acc 0.96
2016-05-21T08:33:47.676190: train-step 7532, loss 0.167235, acc 0.98
2016-05-21T08:33:53.885145: train-step 7533, loss 0.166659, acc 0.99
2016-05-21T08:34:00.048228: train-step 7534, loss 0.188483, acc 0.97
2016-05-21T08:34:06.347125: train-step 7535, loss 0.208587, acc 0.96
2016-05-21T08:34:12.527129: train-step 7536, loss 0.15307, acc 0.96
2016-05-21T08:34:18.788518: train-step 7537, loss 0.173705, acc 0.95
2016-05-21T08:34:24.965496: train-step 7538, loss 0.140006, acc 1
2016-05-21T08:34:31.161944: train-step 7539, loss 0.243966, acc 0.91
2016-05-21T08:34:37.502393: train-step 7540, loss 0.19886, acc 0.98
2016-05-21T08:34:43.698698: train-step 7541, loss 0.207347, acc 0.94
2016-05-21T08:34:49.970617: train-step 7542, loss 0.168228, acc 0.99
2016-05-21T08:34:56.214372: train-step 7543, loss 0.161707, acc 0.97
2016-05-21T08:35:02.578614: train-step 7544, loss 0.160885, acc 0.99
2016-05-21T08:35:08.825262: train-step 7545, loss 0.163016, acc 0.98
2016-05-21T08:35:15.084877: train-step 7546, loss 0.165485, acc 0.99
2016-05-21T08:35:21.309307: train-step 7547, loss 0.177645, acc 0.97
2016-05-21T08:35:27.489268: train-step 7548, loss 0.206138, acc 0.93
2016-05-21T08:35:33.714935: train-step 7549, loss 0.192664, acc 0.97
2016-05-21T08:35:39.917211: train-step 7550, loss 0.195255, acc 0.97
2016-05-21T08:35:46.137180: train-step 7551, loss 0.214167, acc 0.95
2016-05-21T08:35:52.306720: train-step 7552, loss 0.175714, acc 0.98
2016-05-21T08:35:58.472030: train-step 7553, loss 0.213115, acc 0.93
2016-05-21T08:36:04.688157: train-step 7554, loss 0.178509, acc 0.98
2016-05-21T08:36:10.894517: train-step 7555, loss 0.182006, acc 0.97
2016-05-21T08:36:17.085927: train-step 7556, loss 0.15254, acc 0.99
2016-05-21T08:36:23.251398: train-step 7557, loss 0.146248, acc 0.99
2016-05-21T08:36:29.430001: train-step 7558, loss 0.178745, acc 0.96
2016-05-21T08:36:35.640471: train-step 7559, loss 0.199151, acc 0.94
2016-05-21T08:36:41.902993: train-step 7560, loss 0.161829, acc 0.98
2016-05-21T08:36:48.041402: train-step 7561, loss 0.20709, acc 0.98
2016-05-21T08:36:54.211714: train-step 7562, loss 0.142206, acc 1
2016-05-21T08:37:00.408259: train-step 7563, loss 0.179825, acc 0.97
2016-05-21T08:37:06.620319: train-step 7564, loss 0.160342, acc 0.98
2016-05-21T08:37:12.854531: train-step 7565, loss 0.128268, acc 1
2016-05-21T08:37:19.056456: train-step 7566, loss 0.202149, acc 0.96
2016-05-21T08:37:25.287065: train-step 7567, loss 0.198265, acc 0.95
2016-05-21T08:37:31.475545: train-step 7568, loss 0.156119, acc 0.99
2016-05-21T08:37:37.664273: train-step 7569, loss 0.226446, acc 0.94
2016-05-21T08:37:43.814966: train-step 7570, loss 0.155049, acc 0.98
2016-05-21T08:37:50.012089: train-step 7571, loss 0.170307, acc 0.98
2016-05-21T08:37:56.202749: train-step 7572, loss 0.173134, acc 0.98
2016-05-21T08:38:02.410430: train-step 7573, loss 0.178251, acc 0.98
2016-05-21T08:38:08.624407: train-step 7574, loss 0.142952, acc 0.98
2016-05-21T08:38:14.800321: train-step 7575, loss 0.20513, acc 0.95
2016-05-21T08:38:21.022945: train-step 7576, loss 0.175077, acc 0.98
2016-05-21T08:38:27.390515: train-step 7577, loss 0.176477, acc 0.96
2016-05-21T08:38:33.609392: train-step 7578, loss 0.203602, acc 0.97
2016-05-21T08:38:39.843214: train-step 7579, loss 0.181031, acc 0.97
2016-05-21T08:38:46.043417: train-step 7580, loss 0.174128, acc 0.97
2016-05-21T08:38:52.258353: train-step 7581, loss 0.166532, acc 0.98
2016-05-21T08:38:58.425101: train-step 7582, loss 0.155515, acc 0.99
2016-05-21T08:39:04.585611: train-step 7583, loss 0.172621, acc 0.99
2016-05-21T08:39:10.830044: train-step 7584, loss 0.15458, acc 0.98
2016-05-21T08:39:17.222136: train-step 7585, loss 0.184485, acc 0.96
2016-05-21T08:39:23.472690: train-step 7586, loss 0.175459, acc 0.97
2016-05-21T08:39:29.741448: train-step 7587, loss 0.146383, acc 0.98
2016-05-21T08:39:35.924979: train-step 7588, loss 0.187668, acc 0.96
2016-05-21T08:39:42.126602: train-step 7589, loss 0.165151, acc 0.98
2016-05-21T08:39:48.327968: train-step 7590, loss 0.173958, acc 0.97
2016-05-21T08:39:54.531492: train-step 7591, loss 0.194208, acc 0.96
2016-05-21T08:40:00.817944: train-step 7592, loss 0.185793, acc 0.96
2016-05-21T08:40:07.129896: train-step 7593, loss 0.154277, acc 0.99
2016-05-21T08:40:13.315894: train-step 7594, loss 0.158401, acc 0.98
2016-05-21T08:40:19.508892: train-step 7595, loss 0.193299, acc 0.96
2016-05-21T08:40:25.702420: train-step 7596, loss 0.193664, acc 0.98
2016-05-21T08:40:32.024169: train-step 7597, loss 0.203037, acc 0.97
2016-05-21T08:40:38.233559: train-step 7598, loss 0.176302, acc 0.97
2016-05-21T08:40:44.426816: train-step 7599, loss 0.174985, acc 0.98
2016-05-21T08:40:50.789056: train-step 7600, loss 0.149005, acc 0.97
2016-05-21T08:40:57.035918: train-step 7601, loss 0.184016, acc 0.97
2016-05-21T08:41:03.284275: train-step 7602, loss 0.210607, acc 0.94
2016-05-21T08:41:09.504566: train-step 7603, loss 0.149527, acc 0.99
2016-05-21T08:41:16.103156: train-step 7604, loss 0.16051, acc 0.98
2016-05-21T08:41:22.426250: train-step 7605, loss 0.166708, acc 0.98
2016-05-21T08:41:28.657453: train-step 7606, loss 0.155021, acc 0.99
2016-05-21T08:41:34.898852: train-step 7607, loss 0.204775, acc 0.95
2016-05-21T08:41:41.058657: train-step 7608, loss 0.190736, acc 0.94
2016-05-21T08:41:47.274245: train-step 7609, loss 0.186694, acc 0.98
2016-05-21T08:41:53.456857: train-step 7610, loss 0.203361, acc 0.95
2016-05-21T08:41:59.679224: train-step 7611, loss 0.194571, acc 0.95
2016-05-21T08:42:05.890757: train-step 7612, loss 0.17986, acc 0.96
2016-05-21T08:42:12.041430: train-step 7613, loss 0.146511, acc 0.99
2016-05-21T08:42:18.259762: train-step 7614, loss 0.169074, acc 0.99
2016-05-21T08:42:24.447399: train-step 7615, loss 0.201782, acc 0.96
2016-05-21T08:42:30.625242: train-step 7616, loss 0.150579, acc 0.98
2016-05-21T08:42:36.855260: train-step 7617, loss 0.176058, acc 0.97
2016-05-21T08:42:43.098782: train-step 7618, loss 0.195615, acc 0.97
2016-05-21T08:42:49.297164: train-step 7619, loss 0.154934, acc 0.99
2016-05-21T08:42:55.494186: train-step 7620, loss 0.205517, acc 0.94
2016-05-21T08:43:01.717321: train-step 7621, loss 0.173007, acc 0.98
2016-05-21T08:43:07.945538: train-step 7622, loss 0.154759, acc 0.98
2016-05-21T08:43:14.148752: train-step 7623, loss 0.159002, acc 0.96
2016-05-21T08:43:20.347054: train-step 7624, loss 0.198999, acc 0.95
2016-05-21T08:43:26.699573: train-step 7625, loss 0.156185, acc 0.98
2016-05-21T08:43:32.902273: train-step 7626, loss 0.180108, acc 0.97
2016-05-21T08:43:39.141926: train-step 7627, loss 0.163249, acc 0.99
2016-05-21T08:43:45.332423: train-step 7628, loss 0.144741, acc 0.99
2016-05-21T08:43:51.576944: train-step 7629, loss 0.213583, acc 0.97
2016-05-21T08:43:57.961160: train-step 7630, loss 0.169279, acc 0.97
2016-05-21T08:44:04.344552: train-step 7631, loss 0.148453, acc 1
2016-05-21T08:44:10.569297: train-step 7632, loss 0.191298, acc 0.96
2016-05-21T08:44:16.747094: train-step 7633, loss 0.167616, acc 0.98
2016-05-21T08:44:22.960634: train-step 7634, loss 0.192513, acc 0.97
2016-05-21T08:44:29.165665: train-step 7635, loss 0.198015, acc 0.95
2016-05-21T08:44:35.410811: train-step 7636, loss 0.181712, acc 0.98
2016-05-21T08:44:41.658650: train-step 7637, loss 0.185489, acc 0.97
2016-05-21T08:44:48.189364: train-step 7638, loss 0.158175, acc 0.98
2016-05-21T08:44:54.569460: train-step 7639, loss 0.180637, acc 0.97
2016-05-21T08:45:00.738882: train-step 7640, loss 0.156064, acc 0.98
2016-05-21T08:45:06.953480: train-step 7641, loss 0.176964, acc 0.99
2016-05-21T08:45:13.148130: train-step 7642, loss 0.174509, acc 0.98
2016-05-21T08:45:19.349205: train-step 7643, loss 0.187474, acc 0.97
2016-05-21T08:45:25.627372: train-step 7644, loss 0.1924, acc 0.97
2016-05-21T08:45:31.908537: train-step 7645, loss 0.171241, acc 0.98
2016-05-21T08:45:38.066973: train-step 7646, loss 0.145346, acc 1
2016-05-21T08:45:44.285773: train-step 7647, loss 0.198973, acc 0.96
2016-05-21T08:45:50.506028: train-step 7648, loss 0.176065, acc 0.97
2016-05-21T08:45:56.738742: train-step 7649, loss 0.187534, acc 0.96
2016-05-21T08:46:02.865058: train-step 7650, loss 0.155125, acc 0.99
2016-05-21T08:46:09.275494: train-step 7651, loss 0.169956, acc 0.99
2016-05-21T08:46:15.747756: train-step 7652, loss 0.148919, acc 0.99
2016-05-21T08:46:21.995916: train-step 7653, loss 0.173708, acc 0.96
2016-05-21T08:46:28.377093: train-step 7654, loss 0.170757, acc 0.97
2016-05-21T08:46:34.910278: train-step 7655, loss 0.174117, acc 0.96
2016-05-21T08:46:41.173326: train-step 7656, loss 0.184759, acc 0.96
2016-05-21T08:46:47.423622: train-step 7657, loss 0.165345, acc 0.98
2016-05-21T08:46:53.649879: train-step 7658, loss 0.204737, acc 0.97
2016-05-21T08:46:59.813479: train-step 7659, loss 0.157903, acc 0.99
2016-05-21T08:47:06.002556: train-step 7660, loss 0.179083, acc 0.97
2016-05-21T08:47:12.195903: train-step 7661, loss 0.172867, acc 0.98
2016-05-21T08:47:18.447157: train-step 7662, loss 0.169227, acc 0.98
2016-05-21T08:47:24.688860: train-step 7663, loss 0.144932, acc 1
2016-05-21T08:47:30.903989: train-step 7664, loss 0.176689, acc 0.97
2016-05-21T08:47:37.159283: train-step 7665, loss 0.212966, acc 0.95
2016-05-21T08:47:43.332791: train-step 7666, loss 0.180995, acc 0.96
2016-05-21T08:47:49.593370: train-step 7667, loss 0.172707, acc 0.98
2016-05-21T08:47:55.798761: train-step 7668, loss 0.18028, acc 0.97
2016-05-21T08:48:02.057218: train-step 7669, loss 0.16999, acc 0.99
2016-05-21T08:48:08.336503: train-step 7670, loss 0.173002, acc 0.96
2016-05-21T08:48:14.518765: train-step 7671, loss 0.189556, acc 0.97
2016-05-21T08:48:20.736000: train-step 7672, loss 0.19788, acc 0.96
2016-05-21T08:48:26.959875: train-step 7673, loss 0.231852, acc 0.92
2016-05-21T08:48:33.196489: train-step 7674, loss 0.152983, acc 0.98
2016-05-21T08:48:39.538546: train-step 7675, loss 0.184487, acc 0.98
2016-05-21T08:48:45.751714: train-step 7676, loss 0.201778, acc 0.95
2016-05-21T08:48:51.988716: train-step 7677, loss 0.196807, acc 0.97
2016-05-21T08:48:58.352768: train-step 7678, loss 0.221749, acc 0.93
2016-05-21T08:49:04.739843: train-step 7679, loss 0.213362, acc 0.96
2016-05-21T08:49:10.979496: train-step 7680, loss 0.160214, acc 0.98
2016-05-21T08:49:17.256508: train-step 7681, loss 0.19011, acc 0.97
2016-05-21T08:49:23.515513: train-step 7682, loss 0.154288, acc 0.99
2016-05-21T08:49:29.697047: train-step 7683, loss 0.168613, acc 0.99
2016-05-21T08:49:35.927984: train-step 7684, loss 0.151932, acc 0.99
2016-05-21T08:49:42.247896: train-step 7685, loss 0.168254, acc 0.99
2016-05-21T08:49:48.643081: train-step 7686, loss 0.170461, acc 0.96
2016-05-21T08:49:54.881963: train-step 7687, loss 0.166861, acc 0.96
2016-05-21T08:50:01.076051: train-step 7688, loss 0.170592, acc 0.96
2016-05-21T08:50:07.298001: train-step 7689, loss 0.151363, acc 0.97
2016-05-21T08:50:13.509824: train-step 7690, loss 0.161719, acc 0.98
2016-05-21T08:50:19.800739: train-step 7691, loss 0.169343, acc 0.97
2016-05-21T08:50:26.007521: train-step 7692, loss 0.134941, acc 0.99
2016-05-21T08:50:32.181177: train-step 7693, loss 0.166063, acc 0.99
2016-05-21T08:50:38.336010: train-step 7694, loss 0.174662, acc 0.98
2016-05-21T08:50:44.536962: train-step 7695, loss 0.194557, acc 0.94
2016-05-21T08:50:50.798374: train-step 7696, loss 0.176098, acc 0.97
2016-05-21T08:50:57.028466: train-step 7697, loss 0.206591, acc 0.97
2016-05-21T08:51:03.270713: train-step 7698, loss 0.158938, acc 0.98
2016-05-21T08:51:09.479989: train-step 7699, loss 0.152346, acc 0.98
2016-05-21T08:51:15.767488: train-step 7700, loss 0.170101, acc 0.96
2016-05-21T08:51:22.344337: train-step 7701, loss 0.156216, acc 0.98
2016-05-21T08:51:28.946645: train-step 7702, loss 0.182551, acc 0.98
2016-05-21T08:51:35.282973: train-step 7703, loss 0.186695, acc 0.98
2016-05-21T08:51:41.491903: train-step 7704, loss 0.173165, acc 0.97
2016-05-21T08:51:47.665245: train-step 7705, loss 0.143054, acc 0.99
2016-05-21T08:51:54.030518: train-step 7706, loss 0.17401, acc 1
2016-05-21T08:52:00.428660: train-step 7707, loss 0.175348, acc 0.98
2016-05-21T08:52:06.615332: train-step 7708, loss 0.179685, acc 0.98
2016-05-21T08:52:12.816840: train-step 7709, loss 0.180157, acc 0.97
2016-05-21T08:52:19.160243: train-step 7710, loss 0.180103, acc 0.99
2016-05-21T08:52:25.439214: train-step 7711, loss 0.152069, acc 0.97
2016-05-21T08:52:31.634393: train-step 7712, loss 0.182225, acc 0.95
2016-05-21T08:52:37.868823: train-step 7713, loss 0.163245, acc 0.99
2016-05-21T08:52:44.128788: train-step 7714, loss 0.164322, acc 0.96
2016-05-21T08:52:50.303626: train-step 7715, loss 0.174881, acc 0.98
2016-05-21T08:52:56.679281: train-step 7716, loss 0.182465, acc 0.96
2016-05-21T08:53:03.061815: train-step 7717, loss 0.178614, acc 0.98
2016-05-21T08:53:09.355499: train-step 7718, loss 0.182169, acc 0.96
2016-05-21T08:53:15.543022: train-step 7719, loss 0.28027, acc 0.92
2016-05-21T08:53:21.672325: train-step 7720, loss 0.165057, acc 0.98
2016-05-21T08:53:27.816213: train-step 7721, loss 0.147349, acc 0.99
2016-05-21T08:53:33.960127: train-step 7722, loss 0.221957, acc 0.93
2016-05-21T08:53:40.199584: train-step 7723, loss 0.162281, acc 0.99
2016-05-21T08:53:46.404528: train-step 7724, loss 0.172731, acc 0.98
2016-05-21T08:53:52.648251: train-step 7725, loss 0.192776, acc 0.96
2016-05-21T08:53:58.866775: train-step 7726, loss 0.180382, acc 0.97
2016-05-21T08:54:05.030454: train-step 7727, loss 0.186493, acc 0.97
2016-05-21T08:54:11.236352: train-step 7728, loss 0.206141, acc 0.94
2016-05-21T08:54:17.359063: train-step 7729, loss 0.200297, acc 0.92
2016-05-21T08:54:23.579690: train-step 7730, loss 0.152489, acc 0.98
2016-05-21T08:54:29.817262: train-step 7731, loss 0.142288, acc 0.99
2016-05-21T08:54:36.063289: train-step 7732, loss 0.173203, acc 0.96
2016-05-21T08:54:42.232077: train-step 7733, loss 0.161468, acc 0.98
2016-05-21T08:54:48.457892: train-step 7734, loss 0.184652, acc 0.96
2016-05-21T08:54:54.672683: train-step 7735, loss 0.136034, acc 0.99
2016-05-21T08:55:00.922776: train-step 7736, loss 0.20568, acc 0.96
2016-05-21T08:55:07.143044: train-step 7737, loss 0.16735, acc 0.98
2016-05-21T08:55:13.285099: train-step 7738, loss 0.201856, acc 0.96
2016-05-21T08:55:19.503867: train-step 7739, loss 0.178207, acc 0.97
2016-05-21T08:55:25.709680: train-step 7740, loss 0.197173, acc 0.96
2016-05-21T08:55:31.891294: train-step 7741, loss 0.145097, acc 0.99
2016-05-21T08:55:38.101717: train-step 7742, loss 0.246787, acc 0.93
2016-05-21T08:55:44.356095: train-step 7743, loss 0.167685, acc 0.99
2016-05-21T08:55:50.557510: train-step 7744, loss 0.168291, acc 0.99
2016-05-21T08:55:56.752960: train-step 7745, loss 0.160129, acc 0.99
2016-05-21T08:56:02.969836: train-step 7746, loss 0.175418, acc 0.97
2016-05-21T08:56:09.160794: train-step 7747, loss 0.175064, acc 0.99
2016-05-21T08:56:15.339133: train-step 7748, loss 0.190003, acc 0.95
2016-05-21T08:56:21.543560: train-step 7749, loss 0.18581, acc 0.96
2016-05-21T08:56:27.670503: train-step 7750, loss 0.168279, acc 0.97
epoch number is: 31
2016-05-21T08:56:34.157807: train-step 7751, loss 0.162265, acc 0.98
2016-05-21T08:56:40.353763: train-step 7752, loss 0.17493, acc 0.96
2016-05-21T08:56:46.528542: train-step 7753, loss 0.172512, acc 0.98
2016-05-21T08:56:52.726688: train-step 7754, loss 0.16849, acc 0.98
2016-05-21T08:56:58.875346: train-step 7755, loss 0.176086, acc 0.96
2016-05-21T08:57:05.039232: train-step 7756, loss 0.167957, acc 0.98
2016-05-21T08:57:11.232766: train-step 7757, loss 0.169595, acc 1
2016-05-21T08:57:17.423393: train-step 7758, loss 0.185833, acc 0.96
2016-05-21T08:57:23.639483: train-step 7759, loss 0.188373, acc 0.97
2016-05-21T08:57:29.789266: train-step 7760, loss 0.168588, acc 0.99
2016-05-21T08:57:35.956715: train-step 7761, loss 0.186912, acc 0.98
2016-05-21T08:57:42.175282: train-step 7762, loss 0.148785, acc 0.99
2016-05-21T08:57:48.366398: train-step 7763, loss 0.13628, acc 0.99
2016-05-21T08:57:54.609529: train-step 7764, loss 0.158217, acc 0.99
2016-05-21T08:58:00.958525: train-step 7765, loss 0.161232, acc 0.99
2016-05-21T08:58:07.156128: train-step 7766, loss 0.172672, acc 0.98
2016-05-21T08:58:13.340612: train-step 7767, loss 0.16346, acc 0.98
2016-05-21T08:58:19.545969: train-step 7768, loss 0.148066, acc 0.99
2016-05-21T08:58:25.794571: train-step 7769, loss 0.168106, acc 0.97
2016-05-21T08:58:32.060412: train-step 7770, loss 0.13933, acc 1
2016-05-21T08:58:38.272563: train-step 7771, loss 0.172058, acc 0.98
2016-05-21T08:58:44.465305: train-step 7772, loss 0.161119, acc 0.98
2016-05-21T08:58:50.842207: train-step 7773, loss 0.181823, acc 0.96
2016-05-21T08:58:57.047800: train-step 7774, loss 0.186502, acc 0.97
2016-05-21T08:59:03.288229: train-step 7775, loss 0.141163, acc 0.99
2016-05-21T08:59:09.438140: train-step 7776, loss 0.162037, acc 0.98
2016-05-21T08:59:15.759437: train-step 7777, loss 0.153666, acc 0.98
2016-05-21T08:59:21.942201: train-step 7778, loss 0.178441, acc 0.96
2016-05-21T08:59:28.194373: train-step 7779, loss 0.184249, acc 0.96
2016-05-21T08:59:34.411242: train-step 7780, loss 0.195523, acc 0.97
2016-05-21T08:59:40.605482: train-step 7781, loss 0.15705, acc 0.99
2016-05-21T08:59:46.780238: train-step 7782, loss 0.180313, acc 0.98
2016-05-21T08:59:52.940121: train-step 7783, loss 0.214655, acc 0.95
2016-05-21T08:59:59.177220: train-step 7784, loss 0.144571, acc 0.99
2016-05-21T09:00:05.393881: train-step 7785, loss 0.176203, acc 0.98
2016-05-21T09:00:11.700642: train-step 7786, loss 0.157942, acc 1
2016-05-21T09:00:17.907248: train-step 7787, loss 0.163903, acc 0.97
2016-05-21T09:00:24.155832: train-step 7788, loss 0.164035, acc 1
2016-05-21T09:00:30.379855: train-step 7789, loss 0.158331, acc 0.98
2016-05-21T09:00:36.583948: train-step 7790, loss 0.202898, acc 0.96
2016-05-21T09:00:42.922440: train-step 7791, loss 0.164896, acc 0.98
2016-05-21T09:00:49.305900: train-step 7792, loss 0.171399, acc 0.98
2016-05-21T09:00:55.527970: train-step 7793, loss 0.188919, acc 0.97
2016-05-21T09:01:01.763384: train-step 7794, loss 0.171843, acc 0.98
2016-05-21T09:01:08.071453: train-step 7795, loss 0.157426, acc 0.98
2016-05-21T09:01:14.386959: train-step 7796, loss 0.166203, acc 0.99
2016-05-21T09:01:20.688260: train-step 7797, loss 0.194738, acc 0.97
2016-05-21T09:01:27.026405: train-step 7798, loss 0.137804, acc 0.99
2016-05-21T09:01:33.405850: train-step 7799, loss 0.181638, acc 0.98
2016-05-21T09:01:39.575201: train-step 7800, loss 0.190729, acc 0.97
2016-05-21T09:01:45.714690: train-step 7801, loss 0.191418, acc 0.95
2016-05-21T09:01:52.089232: train-step 7802, loss 0.155822, acc 0.99
2016-05-21T09:01:58.276855: train-step 7803, loss 0.18832, acc 0.96
2016-05-21T09:02:04.621806: train-step 7804, loss 0.136638, acc 1
2016-05-21T09:02:11.149496: train-step 7805, loss 0.135461, acc 0.99
2016-05-21T09:02:17.366142: train-step 7806, loss 0.155034, acc 0.98
2016-05-21T09:02:23.566519: train-step 7807, loss 0.156329, acc 0.98
2016-05-21T09:02:29.808566: train-step 7808, loss 0.189718, acc 0.97
2016-05-21T09:02:36.036536: train-step 7809, loss 0.183396, acc 0.95
2016-05-21T09:02:42.203101: train-step 7810, loss 0.168118, acc 0.98
2016-05-21T09:02:48.414292: train-step 7811, loss 0.151079, acc 0.98
2016-05-21T09:02:54.631213: train-step 7812, loss 0.14649, acc 0.98
2016-05-21T09:03:00.864924: train-step 7813, loss 0.172446, acc 0.99
2016-05-21T09:03:07.069909: train-step 7814, loss 0.166604, acc 0.98
2016-05-21T09:03:13.235604: train-step 7815, loss 0.173965, acc 0.97
2016-05-21T09:03:19.418133: train-step 7816, loss 0.131013, acc 1
2016-05-21T09:03:25.580620: train-step 7817, loss 0.169253, acc 0.98
2016-05-21T09:03:31.761651: train-step 7818, loss 0.18863, acc 0.97
2016-05-21T09:03:38.080104: train-step 7819, loss 0.207559, acc 0.95
2016-05-21T09:03:44.506243: train-step 7820, loss 0.155239, acc 0.97
2016-05-21T09:03:50.914397: train-step 7821, loss 0.153854, acc 0.97
2016-05-21T09:03:57.184354: train-step 7822, loss 0.15775, acc 0.97
2016-05-21T09:04:03.371886: train-step 7823, loss 0.137927, acc 0.99
2016-05-21T09:04:09.607196: train-step 7824, loss 0.141358, acc 0.98
2016-05-21T09:04:15.834842: train-step 7825, loss 0.151983, acc 0.98
2016-05-21T09:04:22.023051: train-step 7826, loss 0.159481, acc 0.99
2016-05-21T09:04:28.238141: train-step 7827, loss 0.161644, acc 0.98
2016-05-21T09:04:34.435243: train-step 7828, loss 0.172456, acc 0.98
2016-05-21T09:04:40.638271: train-step 7829, loss 0.132911, acc 1
2016-05-21T09:04:46.884940: train-step 7830, loss 0.170958, acc 0.98
2016-05-21T09:04:53.048623: train-step 7831, loss 0.155303, acc 0.98
2016-05-21T09:04:59.245559: train-step 7832, loss 0.169219, acc 0.99
2016-05-21T09:05:05.473601: train-step 7833, loss 0.144102, acc 0.99
2016-05-21T09:05:11.605705: train-step 7834, loss 0.163694, acc 0.98
2016-05-21T09:05:17.774255: train-step 7835, loss 0.154498, acc 0.97
2016-05-21T09:05:23.967839: train-step 7836, loss 0.163694, acc 0.97
2016-05-21T09:05:30.131264: train-step 7837, loss 0.205209, acc 0.97
2016-05-21T09:05:36.262406: train-step 7838, loss 0.183365, acc 0.99
2016-05-21T09:05:42.468942: train-step 7839, loss 0.147355, acc 0.99
2016-05-21T09:05:48.679186: train-step 7840, loss 0.180595, acc 0.97
2016-05-21T09:05:54.841466: train-step 7841, loss 0.151641, acc 0.99
2016-05-21T09:06:01.076404: train-step 7842, loss 0.174616, acc 0.98
2016-05-21T09:06:07.280091: train-step 7843, loss 0.134569, acc 1
2016-05-21T09:06:13.447089: train-step 7844, loss 0.169141, acc 0.97
2016-05-21T09:06:19.602706: train-step 7845, loss 0.153336, acc 0.98
2016-05-21T09:06:25.817904: train-step 7846, loss 0.18465, acc 0.97
2016-05-21T09:06:32.008219: train-step 7847, loss 0.185091, acc 0.97
2016-05-21T09:06:38.181302: train-step 7848, loss 0.168835, acc 0.95
2016-05-21T09:06:44.645796: train-step 7849, loss 0.176085, acc 0.95
2016-05-21T09:06:51.028651: train-step 7850, loss 0.158644, acc 0.99
2016-05-21T09:06:57.446823: train-step 7851, loss 0.19465, acc 0.95
2016-05-21T09:07:03.688085: train-step 7852, loss 0.165785, acc 0.98
2016-05-21T09:07:09.904511: train-step 7853, loss 0.143471, acc 0.99
2016-05-21T09:07:16.084066: train-step 7854, loss 0.17615, acc 0.96
2016-05-21T09:07:22.274463: train-step 7855, loss 0.195168, acc 0.97
2016-05-21T09:07:28.447343: train-step 7856, loss 0.176572, acc 0.98
2016-05-21T09:07:34.609874: train-step 7857, loss 0.149615, acc 0.98
2016-05-21T09:07:40.825827: train-step 7858, loss 0.171944, acc 0.98
2016-05-21T09:07:46.997222: train-step 7859, loss 0.170631, acc 0.98
2016-05-21T09:07:53.151001: train-step 7860, loss 0.211887, acc 0.97
2016-05-21T09:07:59.651894: train-step 7861, loss 0.199025, acc 0.96
2016-05-21T09:08:05.986918: train-step 7862, loss 0.209104, acc 0.98
2016-05-21T09:08:12.382556: train-step 7863, loss 0.192484, acc 0.98
2016-05-21T09:08:18.612669: train-step 7864, loss 0.15638, acc 0.99
2016-05-21T09:08:24.928393: train-step 7865, loss 0.173995, acc 0.97
2016-05-21T09:08:31.457231: train-step 7866, loss 0.229403, acc 0.93
2016-05-21T09:08:37.676513: train-step 7867, loss 0.221072, acc 0.94
2016-05-21T09:08:43.934960: train-step 7868, loss 0.181322, acc 0.99
2016-05-21T09:08:50.168544: train-step 7869, loss 0.162233, acc 0.98
2016-05-21T09:08:56.363868: train-step 7870, loss 0.23579, acc 0.93
2016-05-21T09:09:02.587098: train-step 7871, loss 0.182041, acc 0.97
2016-05-21T09:09:08.938365: train-step 7872, loss 0.141567, acc 0.99
2016-05-21T09:09:15.186370: train-step 7873, loss 0.251918, acc 0.93
2016-05-21T09:09:21.470077: train-step 7874, loss 0.148937, acc 0.97
2016-05-21T09:09:27.651644: train-step 7875, loss 0.18404, acc 0.97
2016-05-21T09:09:33.807139: train-step 7876, loss 0.170797, acc 0.97
2016-05-21T09:09:39.985293: train-step 7877, loss 0.197315, acc 0.96
2016-05-21T09:09:46.184664: train-step 7878, loss 0.171554, acc 0.96
2016-05-21T09:09:52.398523: train-step 7879, loss 0.182972, acc 0.96
2016-05-21T09:09:58.632966: train-step 7880, loss 0.16572, acc 0.96
2016-05-21T09:10:04.854842: train-step 7881, loss 0.22622, acc 0.94
2016-05-21T09:10:11.041748: train-step 7882, loss 0.188407, acc 0.97
2016-05-21T09:10:17.178405: train-step 7883, loss 0.203222, acc 0.96
2016-05-21T09:10:23.365711: train-step 7884, loss 0.129794, acc 0.99
2016-05-21T09:10:29.583435: train-step 7885, loss 0.168276, acc 0.99
2016-05-21T09:10:35.797841: train-step 7886, loss 0.175892, acc 0.98
2016-05-21T09:10:42.010660: train-step 7887, loss 0.175436, acc 0.97
2016-05-21T09:10:48.234446: train-step 7888, loss 0.169096, acc 0.97
2016-05-21T09:10:54.437856: train-step 7889, loss 0.151881, acc 0.98
2016-05-21T09:11:00.676979: train-step 7890, loss 0.161691, acc 0.98
2016-05-21T09:11:06.903093: train-step 7891, loss 0.163257, acc 0.98
2016-05-21T09:11:13.128379: train-step 7892, loss 0.223606, acc 0.93
2016-05-21T09:11:19.323678: train-step 7893, loss 0.143816, acc 0.98
2016-05-21T09:11:25.572120: train-step 7894, loss 0.147875, acc 0.99
2016-05-21T09:11:31.831209: train-step 7895, loss 0.17043, acc 0.97
2016-05-21T09:11:38.052034: train-step 7896, loss 0.179521, acc 0.98
2016-05-21T09:11:44.260661: train-step 7897, loss 0.226804, acc 0.97
2016-05-21T09:11:50.430128: train-step 7898, loss 0.165513, acc 0.96
2016-05-21T09:11:56.646782: train-step 7899, loss 0.159428, acc 0.98
2016-05-21T09:12:02.811178: train-step 7900, loss 0.176016, acc 0.96
2016-05-21T09:12:09.092743: train-step 7901, loss 0.180207, acc 0.97
2016-05-21T09:12:15.617138: train-step 7902, loss 0.164434, acc 0.99
2016-05-21T09:12:21.822014: train-step 7903, loss 0.18284, acc 0.98
2016-05-21T09:12:28.069120: train-step 7904, loss 0.186353, acc 0.95
2016-05-21T09:12:34.288245: train-step 7905, loss 0.148315, acc 0.99
2016-05-21T09:12:40.484293: train-step 7906, loss 0.156789, acc 0.99
2016-05-21T09:12:46.871202: train-step 7907, loss 0.177951, acc 0.96
2016-05-21T09:12:53.094131: train-step 7908, loss 0.211147, acc 0.95
2016-05-21T09:12:59.308425: train-step 7909, loss 0.186064, acc 0.95
2016-05-21T09:13:05.714128: train-step 7910, loss 0.158255, acc 0.99
2016-05-21T09:13:11.958289: train-step 7911, loss 0.192476, acc 0.96
2016-05-21T09:13:18.183884: train-step 7912, loss 0.18526, acc 0.96
2016-05-21T09:13:24.421987: train-step 7913, loss 0.19579, acc 0.95
2016-05-21T09:13:30.621743: train-step 7914, loss 0.15421, acc 0.99
2016-05-21T09:13:36.815240: train-step 7915, loss 0.164429, acc 0.98
2016-05-21T09:13:43.027143: train-step 7916, loss 0.15897, acc 0.97
2016-05-21T09:13:49.216610: train-step 7917, loss 0.154172, acc 1
2016-05-21T09:13:55.447398: train-step 7918, loss 0.186313, acc 0.96
2016-05-21T09:14:01.638153: train-step 7919, loss 0.243471, acc 0.96
2016-05-21T09:14:07.834003: train-step 7920, loss 0.225637, acc 0.93
2016-05-21T09:14:14.034838: train-step 7921, loss 0.16756, acc 0.97
2016-05-21T09:14:20.380028: train-step 7922, loss 0.161211, acc 0.97
2016-05-21T09:14:26.608971: train-step 7923, loss 0.169686, acc 0.98
2016-05-21T09:14:32.861372: train-step 7924, loss 0.170368, acc 0.98
2016-05-21T09:14:39.031455: train-step 7925, loss 0.176701, acc 0.97
2016-05-21T09:14:45.238631: train-step 7926, loss 0.153751, acc 0.97
2016-05-21T09:14:51.388674: train-step 7927, loss 0.15685, acc 0.99
2016-05-21T09:14:57.707166: train-step 7928, loss 0.162499, acc 0.96
2016-05-21T09:15:03.974334: train-step 7929, loss 0.205267, acc 0.95
2016-05-21T09:15:10.188496: train-step 7930, loss 0.144421, acc 0.98
2016-05-21T09:15:16.414315: train-step 7931, loss 0.180263, acc 0.97
2016-05-21T09:15:22.586019: train-step 7932, loss 0.221638, acc 0.96
2016-05-21T09:15:28.752249: train-step 7933, loss 0.189433, acc 0.96
2016-05-21T09:15:34.887988: train-step 7934, loss 0.168766, acc 0.98
2016-05-21T09:15:41.094507: train-step 7935, loss 0.184722, acc 0.97
2016-05-21T09:15:47.279819: train-step 7936, loss 0.22931, acc 0.93
2016-05-21T09:15:53.469701: train-step 7937, loss 0.172638, acc 0.98
2016-05-21T09:15:59.609021: train-step 7938, loss 0.235394, acc 0.93
2016-05-21T09:16:05.840592: train-step 7939, loss 0.172591, acc 0.99
2016-05-21T09:16:12.050405: train-step 7940, loss 0.172703, acc 0.98
2016-05-21T09:16:18.259303: train-step 7941, loss 0.183808, acc 0.96
2016-05-21T09:16:24.612109: train-step 7942, loss 0.167487, acc 0.99
2016-05-21T09:16:30.854436: train-step 7943, loss 0.16773, acc 0.99
2016-05-21T09:16:37.049650: train-step 7944, loss 0.183308, acc 0.96
2016-05-21T09:16:43.282410: train-step 7945, loss 0.170267, acc 0.98
2016-05-21T09:16:49.499891: train-step 7946, loss 0.175436, acc 0.97
2016-05-21T09:16:55.839973: train-step 7947, loss 0.159708, acc 0.97
2016-05-21T09:17:02.250069: train-step 7948, loss 0.173313, acc 0.97
2016-05-21T09:17:08.489746: train-step 7949, loss 0.181304, acc 0.97
2016-05-21T09:17:14.664746: train-step 7950, loss 0.170582, acc 0.98
2016-05-21T09:17:20.852667: train-step 7951, loss 0.188111, acc 0.94
2016-05-21T09:17:27.053435: train-step 7952, loss 0.15937, acc 0.98
2016-05-21T09:17:33.172699: train-step 7953, loss 0.162708, acc 0.99
2016-05-21T09:17:39.467161: train-step 7954, loss 0.172032, acc 0.98
2016-05-21T09:17:45.722452: train-step 7955, loss 0.196688, acc 0.96
2016-05-21T09:17:51.871439: train-step 7956, loss 0.162573, acc 0.97
2016-05-21T09:17:58.450316: train-step 7957, loss 0.198754, acc 0.99
2016-05-21T09:18:04.793399: train-step 7958, loss 0.210562, acc 0.96
2016-05-21T09:18:11.013685: train-step 7959, loss 0.187512, acc 0.95
2016-05-21T09:18:17.536545: train-step 7960, loss 0.163304, acc 0.98
2016-05-21T09:18:23.873159: train-step 7961, loss 0.183985, acc 0.97
2016-05-21T09:18:30.239992: train-step 7962, loss 0.182648, acc 0.96
2016-05-21T09:18:36.503628: train-step 7963, loss 0.192193, acc 0.97
2016-05-21T09:18:42.678521: train-step 7964, loss 0.236836, acc 0.93
2016-05-21T09:18:48.990353: train-step 7965, loss 0.206658, acc 0.97
2016-05-21T09:18:55.223832: train-step 7966, loss 0.159376, acc 0.97
2016-05-21T09:19:01.472863: train-step 7967, loss 0.155402, acc 0.98
2016-05-21T09:19:07.692608: train-step 7968, loss 0.195621, acc 0.95
2016-05-21T09:19:13.891757: train-step 7969, loss 0.146335, acc 0.98
2016-05-21T09:19:20.079391: train-step 7970, loss 0.168978, acc 1
2016-05-21T09:19:26.285246: train-step 7971, loss 0.155733, acc 0.97
2016-05-21T09:19:32.646596: train-step 7972, loss 0.165989, acc 0.99
2016-05-21T09:19:38.918116: train-step 7973, loss 0.164675, acc 0.98
2016-05-21T09:19:45.184653: train-step 7974, loss 0.150645, acc 1
2016-05-21T09:19:51.372666: train-step 7975, loss 0.202178, acc 0.96
2016-05-21T09:19:57.570829: train-step 7976, loss 0.189148, acc 0.98
2016-05-21T09:20:03.728050: train-step 7977, loss 0.172626, acc 0.97
2016-05-21T09:20:10.044927: train-step 7978, loss 0.153178, acc 1
2016-05-21T09:20:16.278756: train-step 7979, loss 0.155043, acc 0.99
2016-05-21T09:20:22.533402: train-step 7980, loss 0.142023, acc 1
2016-05-21T09:20:28.760452: train-step 7981, loss 0.172022, acc 0.98
2016-05-21T09:20:34.987654: train-step 7982, loss 0.15149, acc 0.99
2016-05-21T09:20:41.125187: train-step 7983, loss 0.182881, acc 0.96
2016-05-21T09:20:47.455616: train-step 7984, loss 0.161451, acc 0.98
2016-05-21T09:20:53.695182: train-step 7985, loss 0.164599, acc 0.97
2016-05-21T09:20:59.956602: train-step 7986, loss 0.143953, acc 0.98
2016-05-21T09:21:06.171673: train-step 7987, loss 0.196515, acc 0.97
2016-05-21T09:21:12.341044: train-step 7988, loss 0.143359, acc 0.99
2016-05-21T09:21:18.571748: train-step 7989, loss 0.196221, acc 0.96
2016-05-21T09:21:24.807422: train-step 7990, loss 0.176237, acc 0.97
2016-05-21T09:21:31.032602: train-step 7991, loss 0.211333, acc 0.97
2016-05-21T09:21:37.189547: train-step 7992, loss 0.169359, acc 0.98
2016-05-21T09:21:43.533656: train-step 7993, loss 0.182452, acc 0.98
2016-05-21T09:21:49.749945: train-step 7994, loss 0.181739, acc 0.98
2016-05-21T09:21:56.009336: train-step 7995, loss 0.240814, acc 0.96
2016-05-21T09:22:02.382344: train-step 7996, loss 0.150095, acc 0.99
2016-05-21T09:22:08.943107: train-step 7997, loss 0.160626, acc 0.98
2016-05-21T09:22:15.193431: train-step 7998, loss 0.173232, acc 0.97
2016-05-21T09:22:21.389888: train-step 7999, loss 0.154603, acc 1
2016-05-21T09:22:27.644603: train-step 8000, loss 0.166088, acc 0.98
epoch number is: 32
2016-05-21T09:22:34.010197: train-step 8001, loss 0.174314, acc 0.96
2016-05-21T09:22:40.229373: train-step 8002, loss 0.158131, acc 0.98
2016-05-21T09:22:46.368448: train-step 8003, loss 0.198224, acc 0.94
2016-05-21T09:22:52.577605: train-step 8004, loss 0.17328, acc 1
2016-05-21T09:22:58.717983: train-step 8005, loss 0.162513, acc 0.99
2016-05-21T09:23:04.909028: train-step 8006, loss 0.153526, acc 0.99
2016-05-21T09:23:11.125796: train-step 8007, loss 0.187634, acc 0.97
2016-05-21T09:23:17.314250: train-step 8008, loss 0.188267, acc 0.94
2016-05-21T09:23:23.455939: train-step 8009, loss 0.190624, acc 0.96
2016-05-21T09:23:29.629738: train-step 8010, loss 0.149052, acc 0.98
2016-05-21T09:23:35.860843: train-step 8011, loss 0.18555, acc 0.97
2016-05-21T09:23:42.078600: train-step 8012, loss 0.186407, acc 0.97
2016-05-21T09:23:48.239674: train-step 8013, loss 0.13112, acc 0.99
2016-05-21T09:23:54.454130: train-step 8014, loss 0.140192, acc 0.99
2016-05-21T09:24:00.606938: train-step 8015, loss 0.16371, acc 0.97
2016-05-21T09:24:06.776627: train-step 8016, loss 0.197155, acc 0.96
2016-05-21T09:24:12.971956: train-step 8017, loss 0.129971, acc 1
2016-05-21T09:24:19.166301: train-step 8018, loss 0.167198, acc 0.97
2016-05-21T09:24:25.312406: train-step 8019, loss 0.166339, acc 0.99
2016-05-21T09:24:31.538207: train-step 8020, loss 0.162473, acc 0.97
2016-05-21T09:24:37.786586: train-step 8021, loss 0.165018, acc 0.98
2016-05-21T09:24:44.024435: train-step 8022, loss 0.159668, acc 0.98
2016-05-21T09:24:50.243238: train-step 8023, loss 0.182756, acc 0.97
2016-05-21T09:24:56.404766: train-step 8024, loss 0.175627, acc 0.97
2016-05-21T09:25:02.625526: train-step 8025, loss 0.181012, acc 0.96
2016-05-21T09:25:08.983065: train-step 8026, loss 0.179544, acc 0.98
2016-05-21T09:25:15.190721: train-step 8027, loss 0.184509, acc 0.98
2016-05-21T09:25:21.424944: train-step 8028, loss 0.148376, acc 0.99
2016-05-21T09:25:27.818292: train-step 8029, loss 0.189717, acc 0.97
2016-05-21T09:25:34.039363: train-step 8030, loss 0.180711, acc 0.97
2016-05-21T09:25:40.307345: train-step 8031, loss 0.18615, acc 0.97
2016-05-21T09:25:46.572513: train-step 8032, loss 0.173989, acc 0.98
2016-05-21T09:25:52.755860: train-step 8033, loss 0.149832, acc 0.98
2016-05-21T09:25:58.929607: train-step 8034, loss 0.148593, acc 0.99
2016-05-21T09:26:05.177762: train-step 8035, loss 0.175015, acc 0.96
2016-05-21T09:26:11.397092: train-step 8036, loss 0.136527, acc 0.99
2016-05-21T09:26:17.618732: train-step 8037, loss 0.141616, acc 0.99
2016-05-21T09:26:23.922291: train-step 8038, loss 0.183856, acc 0.97
2016-05-21T09:26:30.110775: train-step 8039, loss 0.140979, acc 1
2016-05-21T09:26:36.641799: train-step 8040, loss 0.142975, acc 0.99
2016-05-21T09:26:43.033036: train-step 8041, loss 0.210419, acc 0.94
2016-05-21T09:26:49.225270: train-step 8042, loss 0.157777, acc 0.97
2016-05-21T09:26:55.408180: train-step 8043, loss 0.163877, acc 0.97
2016-05-21T09:27:01.754346: train-step 8044, loss 0.205314, acc 0.97
2016-05-21T09:27:07.985331: train-step 8045, loss 0.1702, acc 0.95
2016-05-21T09:27:14.334506: train-step 8046, loss 0.163868, acc 0.97
2016-05-21T09:27:20.899597: train-step 8047, loss 0.169458, acc 0.96
2016-05-21T09:27:27.211025: train-step 8048, loss 0.164622, acc 0.98
2016-05-21T09:27:33.555433: train-step 8049, loss 0.147213, acc 1
2016-05-21T09:27:39.787781: train-step 8050, loss 0.137845, acc 1
2016-05-21T09:27:46.001635: train-step 8051, loss 0.197448, acc 0.98
2016-05-21T09:27:52.159411: train-step 8052, loss 0.143758, acc 0.98
2016-05-21T09:27:58.356467: train-step 8053, loss 0.179761, acc 0.95
2016-05-21T09:28:04.549572: train-step 8054, loss 0.162478, acc 1
2016-05-21T09:28:10.717971: train-step 8055, loss 0.163923, acc 0.97
2016-05-21T09:28:16.881510: train-step 8056, loss 0.181184, acc 0.97
2016-05-21T09:28:23.086901: train-step 8057, loss 0.178872, acc 0.98
2016-05-21T09:28:29.251621: train-step 8058, loss 0.167735, acc 0.97
2016-05-21T09:28:35.379010: train-step 8059, loss 0.155434, acc 0.99
2016-05-21T09:28:41.602542: train-step 8060, loss 0.179306, acc 1
2016-05-21T09:28:47.838549: train-step 8061, loss 0.155838, acc 0.98
2016-05-21T09:28:54.006451: train-step 8062, loss 0.181119, acc 0.99
2016-05-21T09:29:00.505554: train-step 8063, loss 0.195918, acc 0.98
2016-05-21T09:29:06.935030: train-step 8064, loss 0.193318, acc 0.94
2016-05-21T09:29:13.177289: train-step 8065, loss 0.176771, acc 0.98
2016-05-21T09:29:19.371718: train-step 8066, loss 0.176374, acc 0.97
2016-05-21T09:29:25.568755: train-step 8067, loss 0.152711, acc 0.97
2016-05-21T09:29:31.791933: train-step 8068, loss 0.196621, acc 0.95
2016-05-21T09:29:38.006398: train-step 8069, loss 0.205863, acc 0.97
2016-05-21T09:29:44.183436: train-step 8070, loss 0.222907, acc 0.94
2016-05-21T09:29:50.353944: train-step 8071, loss 0.155923, acc 0.98
2016-05-21T09:29:56.582302: train-step 8072, loss 0.16662, acc 0.99
2016-05-21T09:30:02.760913: train-step 8073, loss 0.190686, acc 0.97
2016-05-21T09:30:08.960332: train-step 8074, loss 0.187355, acc 0.94
2016-05-21T09:30:15.171788: train-step 8075, loss 0.191717, acc 0.96
2016-05-21T09:30:21.364408: train-step 8076, loss 0.140169, acc 0.98
2016-05-21T09:30:27.575134: train-step 8077, loss 0.152592, acc 1
2016-05-21T09:30:33.828764: train-step 8078, loss 0.215413, acc 0.97
2016-05-21T09:30:40.031279: train-step 8079, loss 0.149487, acc 0.99
2016-05-21T09:30:46.235347: train-step 8080, loss 0.178091, acc 0.98
2016-05-21T09:30:52.423633: train-step 8081, loss 0.1719, acc 0.99
2016-05-21T09:30:58.613307: train-step 8082, loss 0.17271, acc 0.98
2016-05-21T09:31:04.787218: train-step 8083, loss 0.141344, acc 0.98
2016-05-21T09:31:11.003555: train-step 8084, loss 0.141532, acc 0.99
2016-05-21T09:31:17.195247: train-step 8085, loss 0.148163, acc 1
2016-05-21T09:31:23.380476: train-step 8086, loss 0.153085, acc 0.95
2016-05-21T09:31:29.544640: train-step 8087, loss 0.1467, acc 1
2016-05-21T09:31:35.809964: train-step 8088, loss 0.187831, acc 0.96
2016-05-21T09:31:42.128753: train-step 8089, loss 0.168331, acc 0.99
2016-05-21T09:31:48.333286: train-step 8090, loss 0.156959, acc 0.99
2016-05-21T09:31:54.524878: train-step 8091, loss 0.153651, acc 0.98
2016-05-21T09:32:00.685533: train-step 8092, loss 0.168384, acc 0.99
2016-05-21T09:32:06.823384: train-step 8093, loss 0.186703, acc 0.97
2016-05-21T09:32:13.063081: train-step 8094, loss 0.157491, acc 0.98
2016-05-21T09:32:19.277107: train-step 8095, loss 0.152751, acc 0.98
2016-05-21T09:32:25.450573: train-step 8096, loss 0.154122, acc 0.98
2016-05-21T09:32:31.623593: train-step 8097, loss 0.146952, acc 0.99
2016-05-21T09:32:37.824693: train-step 8098, loss 0.161644, acc 0.97
2016-05-21T09:32:44.051065: train-step 8099, loss 0.172681, acc 0.98
2016-05-21T09:32:50.240255: train-step 8100, loss 0.137129, acc 0.98
2016-05-21T09:32:56.456779: train-step 8101, loss 0.150903, acc 0.99
2016-05-21T09:33:02.674888: train-step 8102, loss 0.146458, acc 0.99
2016-05-21T09:33:08.869952: train-step 8103, loss 0.165965, acc 0.98
2016-05-21T09:33:15.101208: train-step 8104, loss 0.154682, acc 1
2016-05-21T09:33:21.246705: train-step 8105, loss 0.168665, acc 0.98
2016-05-21T09:33:27.390866: train-step 8106, loss 0.159765, acc 0.99
2016-05-21T09:33:33.570072: train-step 8107, loss 0.125246, acc 1
2016-05-21T09:33:39.782959: train-step 8108, loss 0.133692, acc 0.98
2016-05-21T09:33:45.930933: train-step 8109, loss 0.20542, acc 0.97
2016-05-21T09:33:52.162212: train-step 8110, loss 0.203758, acc 0.97
2016-05-21T09:33:58.334689: train-step 8111, loss 0.193028, acc 0.95
2016-05-21T09:34:04.548668: train-step 8112, loss 0.197374, acc 0.96
2016-05-21T09:34:10.713198: train-step 8113, loss 0.151897, acc 0.98
2016-05-21T09:34:16.877247: train-step 8114, loss 0.174761, acc 0.96
2016-05-21T09:34:23.088512: train-step 8115, loss 0.157033, acc 0.97
2016-05-21T09:34:29.289274: train-step 8116, loss 0.146529, acc 1
2016-05-21T09:34:35.512191: train-step 8117, loss 0.186846, acc 0.94
2016-05-21T09:34:41.672221: train-step 8118, loss 0.159321, acc 0.98
2016-05-21T09:34:47.849695: train-step 8119, loss 0.135362, acc 0.98
2016-05-21T09:34:54.016737: train-step 8120, loss 0.156479, acc 0.97
2016-05-21T09:35:00.254109: train-step 8121, loss 0.180963, acc 0.97
2016-05-21T09:35:06.719702: train-step 8122, loss 0.184742, acc 0.98
2016-05-21T09:35:13.020296: train-step 8123, loss 0.166985, acc 0.97
2016-05-21T09:35:19.372378: train-step 8124, loss 0.183998, acc 0.96
2016-05-21T09:35:25.673996: train-step 8125, loss 0.193844, acc 0.95
2016-05-21T09:35:31.834347: train-step 8126, loss 0.235233, acc 0.92
2016-05-21T09:35:38.198280: train-step 8127, loss 0.137474, acc 1
2016-05-21T09:35:44.389805: train-step 8128, loss 0.186237, acc 0.96
2016-05-21T09:35:50.603308: train-step 8129, loss 0.198059, acc 0.95
2016-05-21T09:35:56.792218: train-step 8130, loss 0.19172, acc 0.96
2016-05-21T09:36:03.325008: train-step 8131, loss 0.161153, acc 0.99
2016-05-21T09:36:09.648385: train-step 8132, loss 0.137737, acc 0.99
2016-05-21T09:36:15.811313: train-step 8133, loss 0.152349, acc 0.99
2016-05-21T09:36:22.021485: train-step 8134, loss 0.156148, acc 0.97
2016-05-21T09:36:28.169669: train-step 8135, loss 0.174575, acc 0.98
2016-05-21T09:36:34.388584: train-step 8136, loss 0.163997, acc 0.98
2016-05-21T09:36:40.572679: train-step 8137, loss 0.188361, acc 0.95
2016-05-21T09:36:46.771086: train-step 8138, loss 0.169462, acc 0.97
2016-05-21T09:36:52.920143: train-step 8139, loss 0.168829, acc 0.97
2016-05-21T09:36:59.191389: train-step 8140, loss 0.173728, acc 0.97
2016-05-21T09:37:05.520868: train-step 8141, loss 0.159034, acc 0.99
2016-05-21T09:37:11.707909: train-step 8142, loss 0.20104, acc 0.96
2016-05-21T09:37:17.895949: train-step 8143, loss 0.170881, acc 0.98
2016-05-21T09:37:24.045670: train-step 8144, loss 0.193508, acc 0.98
2016-05-21T09:37:30.214781: train-step 8145, loss 0.160794, acc 0.98
2016-05-21T09:37:36.457062: train-step 8146, loss 0.177469, acc 0.96
2016-05-21T09:37:42.709089: train-step 8147, loss 0.156283, acc 0.97
2016-05-21T09:37:48.929366: train-step 8148, loss 0.198639, acc 0.97
2016-05-21T09:37:55.100209: train-step 8149, loss 0.177167, acc 0.97
2016-05-21T09:38:01.266122: train-step 8150, loss 0.148657, acc 0.99
2016-05-21T09:38:07.420457: train-step 8151, loss 0.140352, acc 0.98
2016-05-21T09:38:13.615291: train-step 8152, loss 0.177954, acc 0.96
2016-05-21T09:38:19.852986: train-step 8153, loss 0.140437, acc 1
2016-05-21T09:38:26.044460: train-step 8154, loss 0.178968, acc 0.98
2016-05-21T09:38:32.221302: train-step 8155, loss 0.174088, acc 0.98
2016-05-21T09:38:38.415411: train-step 8156, loss 0.166537, acc 0.97
2016-05-21T09:38:44.640075: train-step 8157, loss 0.176972, acc 0.97
2016-05-21T09:38:50.813976: train-step 8158, loss 0.147975, acc 1
2016-05-21T09:38:57.039216: train-step 8159, loss 0.159201, acc 0.98
2016-05-21T09:39:03.221539: train-step 8160, loss 0.133543, acc 1
2016-05-21T09:39:09.446449: train-step 8161, loss 0.171659, acc 0.98
2016-05-21T09:39:15.669285: train-step 8162, loss 0.148687, acc 0.99
2016-05-21T09:39:21.883702: train-step 8163, loss 0.226317, acc 0.95
2016-05-21T09:39:28.104652: train-step 8164, loss 0.191158, acc 0.97
2016-05-21T09:39:34.296675: train-step 8165, loss 0.178042, acc 0.98
2016-05-21T09:39:40.467133: train-step 8166, loss 0.164031, acc 0.98
2016-05-21T09:39:46.694238: train-step 8167, loss 0.147968, acc 0.98
2016-05-21T09:39:52.911788: train-step 8168, loss 0.140921, acc 0.99
2016-05-21T09:39:59.077275: train-step 8169, loss 0.163865, acc 0.97
2016-05-21T09:40:05.277399: train-step 8170, loss 0.204366, acc 0.96
2016-05-21T09:40:11.483551: train-step 8171, loss 0.135857, acc 0.99
2016-05-21T09:40:17.626806: train-step 8172, loss 0.148384, acc 0.99
2016-05-21T09:40:23.842886: train-step 8173, loss 0.177414, acc 0.96
2016-05-21T09:40:30.008903: train-step 8174, loss 0.186394, acc 0.96
2016-05-21T09:40:36.238267: train-step 8175, loss 0.187478, acc 0.96
2016-05-21T09:40:42.400622: train-step 8176, loss 0.139225, acc 0.99
2016-05-21T09:40:48.565009: train-step 8177, loss 0.191565, acc 0.97
2016-05-21T09:40:54.698295: train-step 8178, loss 0.152248, acc 0.98
2016-05-21T09:41:00.891681: train-step 8179, loss 0.139725, acc 0.98
2016-05-21T09:41:07.128152: train-step 8180, loss 0.163075, acc 0.97
2016-05-21T09:41:13.350859: train-step 8181, loss 0.213698, acc 0.97
2016-05-21T09:41:19.559352: train-step 8182, loss 0.162826, acc 0.96
2016-05-21T09:41:25.757179: train-step 8183, loss 0.173576, acc 0.99
2016-05-21T09:41:31.890558: train-step 8184, loss 0.162845, acc 0.97
2016-05-21T09:41:38.077972: train-step 8185, loss 0.158511, acc 0.97
2016-05-21T09:41:44.280528: train-step 8186, loss 0.169015, acc 0.96
2016-05-21T09:41:50.501688: train-step 8187, loss 0.14859, acc 0.97
2016-05-21T09:41:56.665356: train-step 8188, loss 0.172776, acc 0.97
2016-05-21T09:42:02.873636: train-step 8189, loss 0.214105, acc 0.94
2016-05-21T09:42:09.034184: train-step 8190, loss 0.192144, acc 0.98
2016-05-21T09:42:15.221930: train-step 8191, loss 0.155947, acc 0.98
2016-05-21T09:42:21.400587: train-step 8192, loss 0.133453, acc 0.98
2016-05-21T09:42:27.579025: train-step 8193, loss 0.141288, acc 0.98
2016-05-21T09:42:33.747288: train-step 8194, loss 0.156308, acc 0.96
2016-05-21T09:42:39.947782: train-step 8195, loss 0.165761, acc 0.96
2016-05-21T09:42:46.161359: train-step 8196, loss 0.17496, acc 0.98
2016-05-21T09:42:52.391089: train-step 8197, loss 0.158458, acc 0.99
2016-05-21T09:42:58.527883: train-step 8198, loss 0.197919, acc 0.98
2016-05-21T09:43:04.679753: train-step 8199, loss 0.186, acc 0.96
2016-05-21T09:43:10.875959: train-step 8200, loss 0.170362, acc 0.95
2016-05-21T09:43:17.090924: train-step 8201, loss 0.176433, acc 0.98
2016-05-21T09:43:23.272264: train-step 8202, loss 0.182772, acc 0.98
2016-05-21T09:43:29.435733: train-step 8203, loss 0.172052, acc 0.97
2016-05-21T09:43:35.645977: train-step 8204, loss 0.160621, acc 0.98
2016-05-21T09:43:41.863162: train-step 8205, loss 0.14772, acc 0.99
2016-05-21T09:43:48.033767: train-step 8206, loss 0.150248, acc 0.98
2016-05-21T09:43:54.246131: train-step 8207, loss 0.188058, acc 0.96
2016-05-21T09:44:00.445519: train-step 8208, loss 0.258303, acc 0.9
2016-05-21T09:44:06.640993: train-step 8209, loss 0.163772, acc 0.98
2016-05-21T09:44:12.871025: train-step 8210, loss 0.193502, acc 0.96
2016-05-21T09:44:19.409725: train-step 8211, loss 0.177402, acc 0.98
2016-05-21T09:44:25.715785: train-step 8212, loss 0.161405, acc 0.97
2016-05-21T09:44:31.935617: train-step 8213, loss 0.140196, acc 0.99
2016-05-21T09:44:38.191871: train-step 8214, loss 0.206322, acc 0.94
2016-05-21T09:44:44.360717: train-step 8215, loss 0.18188, acc 0.98
2016-05-21T09:44:50.575708: train-step 8216, loss 0.184062, acc 0.98
2016-05-21T09:44:56.806771: train-step 8217, loss 0.19908, acc 0.95
2016-05-21T09:45:03.048515: train-step 8218, loss 0.179441, acc 0.97
2016-05-21T09:45:09.292717: train-step 8219, loss 0.14893, acc 1
2016-05-21T09:45:15.494598: train-step 8220, loss 0.176608, acc 0.99
2016-05-21T09:45:21.660653: train-step 8221, loss 0.175433, acc 0.97
2016-05-21T09:45:27.901998: train-step 8222, loss 0.177324, acc 0.98
2016-05-21T09:45:34.141611: train-step 8223, loss 0.156542, acc 0.98
2016-05-21T09:45:40.376919: train-step 8224, loss 0.14689, acc 1
2016-05-21T09:45:46.579500: train-step 8225, loss 0.2113, acc 0.95
2016-05-21T09:45:52.813608: train-step 8226, loss 0.156426, acc 0.99
2016-05-21T09:45:59.021435: train-step 8227, loss 0.168219, acc 0.99
2016-05-21T09:46:05.231922: train-step 8228, loss 0.132673, acc 1
2016-05-21T09:46:11.401325: train-step 8229, loss 0.160778, acc 0.98
2016-05-21T09:46:17.560140: train-step 8230, loss 0.18036, acc 0.98
2016-05-21T09:46:23.743919: train-step 8231, loss 0.187602, acc 0.96
2016-05-21T09:46:29.959273: train-step 8232, loss 0.207244, acc 0.97
2016-05-21T09:46:36.166678: train-step 8233, loss 0.244824, acc 0.96
2016-05-21T09:46:42.313204: train-step 8234, loss 0.1604, acc 0.99
2016-05-21T09:46:48.547963: train-step 8235, loss 0.145541, acc 0.99
2016-05-21T09:46:54.700125: train-step 8236, loss 0.175422, acc 0.98
2016-05-21T09:47:00.921055: train-step 8237, loss 0.17521, acc 0.99
2016-05-21T09:47:07.124888: train-step 8238, loss 0.176333, acc 0.98
2016-05-21T09:47:13.334017: train-step 8239, loss 0.151567, acc 0.98
2016-05-21T09:47:19.528744: train-step 8240, loss 0.143306, acc 0.97
2016-05-21T09:47:25.757859: train-step 8241, loss 0.154519, acc 0.97
2016-05-21T09:47:31.957779: train-step 8242, loss 0.139237, acc 0.99
2016-05-21T09:47:38.143871: train-step 8243, loss 0.13993, acc 1
2016-05-21T09:47:44.359520: train-step 8244, loss 0.186441, acc 0.96
2016-05-21T09:47:50.550999: train-step 8245, loss 0.15982, acc 0.99
2016-05-21T09:47:57.075630: train-step 8246, loss 0.134575, acc 0.99
2016-05-21T09:48:03.792006: train-step 8247, loss 0.167478, acc 0.98
2016-05-21T09:48:10.166703: train-step 8248, loss 0.176492, acc 0.98
2016-05-21T09:48:16.356691: train-step 8249, loss 0.176688, acc 0.95
2016-05-21T09:48:22.913261: train-step 8250, loss 0.158227, acc 0.99
epoch number is: 33
2016-05-21T09:48:29.317410: train-step 8251, loss 0.182768, acc 0.96
2016-05-21T09:48:35.533699: train-step 8252, loss 0.160035, acc 0.97
2016-05-21T09:48:41.705397: train-step 8253, loss 0.19194, acc 0.95
2016-05-21T09:48:47.896390: train-step 8254, loss 0.142641, acc 0.99
2016-05-21T09:48:54.159862: train-step 8255, loss 0.16546, acc 0.97
2016-05-21T09:49:00.400327: train-step 8256, loss 0.190681, acc 0.96
2016-05-21T09:49:06.754405: train-step 8257, loss 0.174749, acc 0.96
2016-05-21T09:49:13.012315: train-step 8258, loss 0.136176, acc 0.99
2016-05-21T09:49:19.257149: train-step 8259, loss 0.1642, acc 0.99
2016-05-21T09:49:25.478557: train-step 8260, loss 0.125044, acc 0.99
2016-05-21T09:49:31.715211: train-step 8261, loss 0.156684, acc 0.98
2016-05-21T09:49:37.918739: train-step 8262, loss 0.152532, acc 0.99
2016-05-21T09:49:44.171069: train-step 8263, loss 0.166518, acc 0.99
2016-05-21T09:49:50.314863: train-step 8264, loss 0.132935, acc 0.99
2016-05-21T09:49:56.526888: train-step 8265, loss 0.140864, acc 0.99
2016-05-21T09:50:02.756777: train-step 8266, loss 0.206717, acc 0.95
2016-05-21T09:50:08.913705: train-step 8267, loss 0.136979, acc 1
2016-05-21T09:50:15.144180: train-step 8268, loss 0.180688, acc 0.96
2016-05-21T09:50:21.305359: train-step 8269, loss 0.135833, acc 0.99
2016-05-21T09:50:27.459785: train-step 8270, loss 0.191895, acc 0.97
2016-05-21T09:50:33.666881: train-step 8271, loss 0.171426, acc 0.97
2016-05-21T09:50:39.833498: train-step 8272, loss 0.19756, acc 0.94
2016-05-21T09:50:46.023550: train-step 8273, loss 0.137636, acc 1
2016-05-21T09:50:52.168711: train-step 8274, loss 0.167268, acc 0.98
2016-05-21T09:50:58.384271: train-step 8275, loss 0.188788, acc 0.97
2016-05-21T09:51:04.571337: train-step 8276, loss 0.16103, acc 0.98
2016-05-21T09:51:10.793255: train-step 8277, loss 0.172575, acc 0.97
2016-05-21T09:51:16.966662: train-step 8278, loss 0.159516, acc 0.98
2016-05-21T09:51:23.160108: train-step 8279, loss 0.189705, acc 0.95
2016-05-21T09:51:29.483964: train-step 8280, loss 0.173434, acc 0.97
2016-05-21T09:51:35.663921: train-step 8281, loss 0.152121, acc 0.99
2016-05-21T09:51:41.926133: train-step 8282, loss 0.137187, acc 0.99
2016-05-21T09:51:48.120020: train-step 8283, loss 0.183615, acc 0.98
2016-05-21T09:51:54.319241: train-step 8284, loss 0.133542, acc 0.99
2016-05-21T09:52:00.459548: train-step 8285, loss 0.143137, acc 0.99
2016-05-21T09:52:06.655054: train-step 8286, loss 0.152092, acc 0.99
2016-05-21T09:52:12.861525: train-step 8287, loss 0.181541, acc 0.97
2016-05-21T09:52:19.067840: train-step 8288, loss 0.159509, acc 0.97
2016-05-21T09:52:25.238013: train-step 8289, loss 0.172958, acc 0.98
2016-05-21T09:52:31.457070: train-step 8290, loss 0.15157, acc 0.99
2016-05-21T09:52:37.804385: train-step 8291, loss 0.149745, acc 0.99
2016-05-21T09:52:44.021004: train-step 8292, loss 0.150073, acc 0.98
2016-05-21T09:52:50.234343: train-step 8293, loss 0.182078, acc 0.97
2016-05-21T09:52:56.396612: train-step 8294, loss 0.146005, acc 0.98
2016-05-21T09:53:02.653515: train-step 8295, loss 0.177823, acc 0.96
2016-05-21T09:53:08.811104: train-step 8296, loss 0.186796, acc 0.97
2016-05-21T09:53:14.981121: train-step 8297, loss 0.158159, acc 0.97
2016-05-21T09:53:21.205971: train-step 8298, loss 0.17362, acc 0.98
2016-05-21T09:53:27.421837: train-step 8299, loss 0.146178, acc 0.98
2016-05-21T09:53:33.601441: train-step 8300, loss 0.142603, acc 1
2016-05-21T09:53:39.815504: train-step 8301, loss 0.141847, acc 0.99
2016-05-21T09:53:45.987510: train-step 8302, loss 0.170547, acc 0.98
2016-05-21T09:53:52.181484: train-step 8303, loss 0.22983, acc 0.95
2016-05-21T09:53:58.327534: train-step 8304, loss 0.180808, acc 0.96
2016-05-21T09:54:04.460271: train-step 8305, loss 0.131148, acc 1
2016-05-21T09:54:10.616017: train-step 8306, loss 0.177837, acc 0.99
2016-05-21T09:54:16.813330: train-step 8307, loss 0.199899, acc 0.98
2016-05-21T09:54:23.044717: train-step 8308, loss 0.205521, acc 0.96
2016-05-21T09:54:29.417520: train-step 8309, loss 0.132442, acc 1
2016-05-21T09:54:35.619005: train-step 8310, loss 0.168705, acc 0.97
2016-05-21T09:54:41.768391: train-step 8311, loss 0.152388, acc 0.99
2016-05-21T09:54:47.983206: train-step 8312, loss 0.20057, acc 0.96
2016-05-21T09:54:54.224237: train-step 8313, loss 0.169861, acc 0.98
2016-05-21T09:55:00.480411: train-step 8314, loss 0.153348, acc 0.99
2016-05-21T09:55:06.776050: train-step 8315, loss 0.18415, acc 0.95
2016-05-21T09:55:13.005138: train-step 8316, loss 0.151852, acc 0.99
2016-05-21T09:55:19.178565: train-step 8317, loss 0.153272, acc 0.99
2016-05-21T09:55:25.379091: train-step 8318, loss 0.152889, acc 0.99
2016-05-21T09:55:31.737633: train-step 8319, loss 0.158096, acc 0.97
2016-05-21T09:55:38.001362: train-step 8320, loss 0.191358, acc 0.97
2016-05-21T09:55:44.217590: train-step 8321, loss 0.15273, acc 0.98
2016-05-21T09:55:50.425313: train-step 8322, loss 0.160161, acc 0.99
2016-05-21T09:55:56.644152: train-step 8323, loss 0.153992, acc 0.97
2016-05-21T09:56:02.890319: train-step 8324, loss 0.138169, acc 1
2016-05-21T09:56:09.083691: train-step 8325, loss 0.144606, acc 0.99
2016-05-21T09:56:15.306159: train-step 8326, loss 0.123374, acc 1
2016-05-21T09:56:21.497816: train-step 8327, loss 0.235699, acc 0.93
2016-05-21T09:56:27.676577: train-step 8328, loss 0.176504, acc 0.98
2016-05-21T09:56:33.822932: train-step 8329, loss 0.174402, acc 0.96
2016-05-21T09:56:40.031593: train-step 8330, loss 0.141956, acc 0.99
2016-05-21T09:56:46.260223: train-step 8331, loss 0.128123, acc 0.98
2016-05-21T09:56:52.474397: train-step 8332, loss 0.153129, acc 1
2016-05-21T09:56:58.660197: train-step 8333, loss 0.176613, acc 0.96
2016-05-21T09:57:04.873511: train-step 8334, loss 0.171164, acc 0.97
2016-05-21T09:57:11.029870: train-step 8335, loss 0.147002, acc 0.98
2016-05-21T09:57:17.237455: train-step 8336, loss 0.15323, acc 0.98
2016-05-21T09:57:23.451730: train-step 8337, loss 0.149884, acc 1
2016-05-21T09:57:29.661776: train-step 8338, loss 0.173972, acc 0.98
2016-05-21T09:57:35.862934: train-step 8339, loss 0.174488, acc 0.99
2016-05-21T09:57:42.024104: train-step 8340, loss 0.144848, acc 1
2016-05-21T09:57:48.213338: train-step 8341, loss 0.166847, acc 0.98
2016-05-21T09:57:54.589367: train-step 8342, loss 0.176563, acc 0.99
2016-05-21T09:58:00.809856: train-step 8343, loss 0.15253, acc 0.99
2016-05-21T09:58:07.041978: train-step 8344, loss 0.14891, acc 0.99
2016-05-21T09:58:13.254883: train-step 8345, loss 0.169782, acc 0.98
2016-05-21T09:58:19.643393: train-step 8346, loss 0.162313, acc 0.98
2016-05-21T09:58:25.866235: train-step 8347, loss 0.145891, acc 0.99
2016-05-21T09:58:32.135789: train-step 8348, loss 0.168438, acc 0.99
2016-05-21T09:58:38.349717: train-step 8349, loss 0.175234, acc 0.96
2016-05-21T09:58:44.507159: train-step 8350, loss 0.138907, acc 0.99
2016-05-21T09:58:50.707156: train-step 8351, loss 0.141853, acc 0.99
2016-05-21T09:58:56.929738: train-step 8352, loss 0.151088, acc 0.99
2016-05-21T09:59:03.145761: train-step 8353, loss 0.188422, acc 0.96
2016-05-21T09:59:09.365763: train-step 8354, loss 0.150542, acc 0.98
2016-05-21T09:59:15.560783: train-step 8355, loss 0.184735, acc 0.96
2016-05-21T09:59:21.767751: train-step 8356, loss 0.170427, acc 0.96
2016-05-21T09:59:27.997535: train-step 8357, loss 0.220865, acc 0.95
2016-05-21T09:59:34.624169: train-step 8358, loss 0.168, acc 0.98
2016-05-21T09:59:41.263375: train-step 8359, loss 0.167407, acc 0.98
2016-05-21T09:59:47.627607: train-step 8360, loss 0.164152, acc 0.95
2016-05-21T09:59:53.979219: train-step 8361, loss 0.169592, acc 0.96
2016-05-21T10:00:00.167247: train-step 8362, loss 0.148988, acc 0.99
2016-05-21T10:00:06.297104: train-step 8363, loss 0.17682, acc 0.96
2016-05-21T10:00:12.491182: train-step 8364, loss 0.156383, acc 0.99
2016-05-21T10:00:18.689472: train-step 8365, loss 0.144587, acc 0.99
2016-05-21T10:00:24.895465: train-step 8366, loss 0.166176, acc 1
2016-05-21T10:00:31.111296: train-step 8367, loss 0.152037, acc 0.99
2016-05-21T10:00:37.326885: train-step 8368, loss 0.163123, acc 0.98
2016-05-21T10:00:44.061298: train-step 8369, loss 0.17794, acc 0.99
2016-05-21T10:00:50.658899: train-step 8370, loss 0.168389, acc 0.99
2016-05-21T10:00:56.938281: train-step 8371, loss 0.173542, acc 0.96
2016-05-21T10:01:03.259294: train-step 8372, loss 0.157229, acc 0.98
2016-05-21T10:01:09.417564: train-step 8373, loss 0.156555, acc 0.99
2016-05-21T10:01:15.608313: train-step 8374, loss 0.188395, acc 0.97
2016-05-21T10:01:21.956100: train-step 8375, loss 0.168512, acc 0.98
2016-05-21T10:01:28.335605: train-step 8376, loss 0.181032, acc 0.97
2016-05-21T10:01:34.539729: train-step 8377, loss 0.165415, acc 0.97
2016-05-21T10:01:40.938307: train-step 8378, loss 0.135153, acc 0.98
2016-05-21T10:01:47.447551: train-step 8379, loss 0.154584, acc 0.98
2016-05-21T10:01:53.752985: train-step 8380, loss 0.158445, acc 1
2016-05-21T10:02:00.091247: train-step 8381, loss 0.156173, acc 0.99
2016-05-21T10:02:06.368840: train-step 8382, loss 0.123178, acc 0.99
2016-05-21T10:02:12.951928: train-step 8383, loss 0.137423, acc 0.99
2016-05-21T10:02:19.277759: train-step 8384, loss 0.17334, acc 0.98
2016-05-21T10:02:25.658425: train-step 8385, loss 0.156513, acc 0.99
2016-05-21T10:02:31.827598: train-step 8386, loss 0.135822, acc 1
2016-05-21T10:02:38.084523: train-step 8387, loss 0.157067, acc 0.99
2016-05-21T10:02:44.322996: train-step 8388, loss 0.158762, acc 0.98
2016-05-21T10:02:50.478536: train-step 8389, loss 0.174367, acc 0.96
2016-05-21T10:02:56.709359: train-step 8390, loss 0.141687, acc 0.99
2016-05-21T10:03:03.013368: train-step 8391, loss 0.162539, acc 0.98
2016-05-21T10:03:09.178523: train-step 8392, loss 0.160421, acc 0.99
2016-05-21T10:03:15.381076: train-step 8393, loss 0.166649, acc 0.98
2016-05-21T10:03:21.578985: train-step 8394, loss 0.144504, acc 0.98
2016-05-21T10:03:27.946272: train-step 8395, loss 0.169497, acc 0.98
2016-05-21T10:03:34.187262: train-step 8396, loss 0.135117, acc 0.99
2016-05-21T10:03:40.546249: train-step 8397, loss 0.186138, acc 0.98
2016-05-21T10:03:47.086367: train-step 8398, loss 0.119511, acc 1
2016-05-21T10:03:53.398089: train-step 8399, loss 0.172322, acc 0.98
2016-05-21T10:03:59.628534: train-step 8400, loss 0.133277, acc 1
2016-05-21T10:04:05.866588: train-step 8401, loss 0.151271, acc 0.97
2016-05-21T10:04:12.461950: train-step 8402, loss 0.162258, acc 0.98
2016-05-21T10:04:18.756470: train-step 8403, loss 0.197516, acc 0.96
2016-05-21T10:04:25.098810: train-step 8404, loss 0.155036, acc 0.98
2016-05-21T10:04:31.300483: train-step 8405, loss 0.155876, acc 0.99
2016-05-21T10:04:37.459350: train-step 8406, loss 0.122484, acc 1
2016-05-21T10:04:43.641182: train-step 8407, loss 0.164639, acc 0.98
2016-05-21T10:04:50.054311: train-step 8408, loss 0.188576, acc 0.98
2016-05-21T10:04:56.519780: train-step 8409, loss 0.181295, acc 0.96
2016-05-21T10:05:02.910303: train-step 8410, loss 0.172003, acc 0.98
2016-05-21T10:05:09.793623: train-step 8411, loss 0.196267, acc 0.96
2016-05-21T10:05:16.301038: train-step 8412, loss 0.216353, acc 0.97
2016-05-21T10:05:22.548800: train-step 8413, loss 0.142355, acc 0.99
2016-05-21T10:05:28.722560: train-step 8414, loss 0.143247, acc 0.98
2016-05-21T10:05:34.931011: train-step 8415, loss 0.141237, acc 0.97
2016-05-21T10:05:41.160027: train-step 8416, loss 0.175223, acc 0.99
2016-05-21T10:05:47.374591: train-step 8417, loss 0.260464, acc 0.92
2016-05-21T10:05:53.564793: train-step 8418, loss 0.158579, acc 0.97
2016-05-21T10:05:59.776702: train-step 8419, loss 0.192728, acc 0.96
2016-05-21T10:06:06.218682: train-step 8420, loss 0.191873, acc 0.96
2016-05-21T10:06:12.631894: train-step 8421, loss 0.193622, acc 0.97
2016-05-21T10:06:18.928127: train-step 8422, loss 0.124189, acc 1
2016-05-21T10:06:25.085474: train-step 8423, loss 0.119699, acc 1
2016-05-21T10:06:31.264063: train-step 8424, loss 0.218442, acc 0.95
2016-05-21T10:06:37.495240: train-step 8425, loss 0.201935, acc 0.94
2016-05-21T10:06:43.697705: train-step 8426, loss 0.151886, acc 1
2016-05-21T10:06:49.914166: train-step 8427, loss 0.141079, acc 1
2016-05-21T10:06:56.132277: train-step 8428, loss 0.184045, acc 0.97
2016-05-21T10:07:02.443507: train-step 8429, loss 0.165529, acc 0.97
2016-05-21T10:07:08.686650: train-step 8430, loss 0.206099, acc 0.95
2016-05-21T10:07:14.939920: train-step 8431, loss 0.171664, acc 0.98
2016-05-21T10:07:21.102431: train-step 8432, loss 0.180607, acc 0.96
2016-05-21T10:07:27.295749: train-step 8433, loss 0.175812, acc 0.96
2016-05-21T10:07:33.467051: train-step 8434, loss 0.146666, acc 0.99
2016-05-21T10:07:39.655694: train-step 8435, loss 0.18413, acc 0.97
2016-05-21T10:07:45.866759: train-step 8436, loss 0.156677, acc 1
2016-05-21T10:07:52.084678: train-step 8437, loss 0.150835, acc 1
2016-05-21T10:07:58.281607: train-step 8438, loss 0.204451, acc 0.96
2016-05-21T10:08:04.462396: train-step 8439, loss 0.123785, acc 1
2016-05-21T10:08:10.673691: train-step 8440, loss 0.147792, acc 0.97
2016-05-21T10:08:16.877266: train-step 8441, loss 0.182907, acc 0.97
2016-05-21T10:08:23.230570: train-step 8442, loss 0.136975, acc 0.99
2016-05-21T10:08:29.555392: train-step 8443, loss 0.144227, acc 0.99
2016-05-21T10:08:35.785697: train-step 8444, loss 0.144569, acc 0.99
2016-05-21T10:08:42.228562: train-step 8445, loss 0.150062, acc 0.98
2016-05-21T10:08:48.691302: train-step 8446, loss 0.16828, acc 0.97
2016-05-21T10:08:54.955742: train-step 8447, loss 0.186775, acc 0.97
2016-05-21T10:09:01.336648: train-step 8448, loss 0.190397, acc 0.96
2016-05-21T10:09:07.910032: train-step 8449, loss 0.145311, acc 0.97
2016-05-21T10:09:14.174509: train-step 8450, loss 0.140991, acc 0.98
2016-05-21T10:09:20.445497: train-step 8451, loss 0.179535, acc 0.97
2016-05-21T10:09:26.678626: train-step 8452, loss 0.164092, acc 0.97
2016-05-21T10:09:33.292290: train-step 8453, loss 0.151965, acc 0.97
2016-05-21T10:09:39.573553: train-step 8454, loss 0.174101, acc 0.96
2016-05-21T10:09:45.756647: train-step 8455, loss 0.197913, acc 0.92
2016-05-21T10:09:51.954670: train-step 8456, loss 0.173237, acc 0.96
2016-05-21T10:09:58.160128: train-step 8457, loss 0.135533, acc 0.99
2016-05-21T10:10:04.440817: train-step 8458, loss 0.147836, acc 1
2016-05-21T10:10:10.694774: train-step 8459, loss 0.148793, acc 0.97
2016-05-21T10:10:17.288942: train-step 8460, loss 0.156632, acc 0.98
2016-05-21T10:10:23.606035: train-step 8461, loss 0.172375, acc 0.96
2016-05-21T10:10:29.778346: train-step 8462, loss 0.197845, acc 0.95
2016-05-21T10:10:35.929583: train-step 8463, loss 0.163135, acc 0.97
2016-05-21T10:10:42.301399: train-step 8464, loss 0.221678, acc 0.95
2016-05-21T10:10:48.519860: train-step 8465, loss 0.15678, acc 0.98
2016-05-21T10:10:54.768335: train-step 8466, loss 0.14384, acc 0.99
2016-05-21T10:11:01.047553: train-step 8467, loss 0.153047, acc 0.98
2016-05-21T10:11:07.233408: train-step 8468, loss 0.15449, acc 0.97
2016-05-21T10:11:13.419713: train-step 8469, loss 0.193759, acc 0.96
2016-05-21T10:11:19.583988: train-step 8470, loss 0.171945, acc 0.98
2016-05-21T10:11:25.762057: train-step 8471, loss 0.139605, acc 0.99
2016-05-21T10:11:31.965162: train-step 8472, loss 0.155235, acc 0.98
2016-05-21T10:11:38.201302: train-step 8473, loss 0.169158, acc 0.94
2016-05-21T10:11:44.441523: train-step 8474, loss 0.164378, acc 0.99
2016-05-21T10:11:50.646161: train-step 8475, loss 0.170833, acc 0.99
2016-05-21T10:11:56.886590: train-step 8476, loss 0.182479, acc 0.97
2016-05-21T10:12:03.238617: train-step 8477, loss 0.1354, acc 1
2016-05-21T10:12:09.619011: train-step 8478, loss 0.162088, acc 0.98
2016-05-21T10:12:16.018561: train-step 8479, loss 0.196043, acc 0.95
2016-05-21T10:12:22.274424: train-step 8480, loss 0.148995, acc 0.98
2016-05-21T10:12:28.522232: train-step 8481, loss 0.161117, acc 0.99
2016-05-21T10:12:34.768228: train-step 8482, loss 0.15917, acc 0.97
2016-05-21T10:12:41.316600: train-step 8483, loss 0.131952, acc 0.99
2016-05-21T10:12:47.700233: train-step 8484, loss 0.211117, acc 0.95
2016-05-21T10:12:53.868325: train-step 8485, loss 0.131445, acc 1
2016-05-21T10:13:00.074060: train-step 8486, loss 0.144805, acc 0.98
2016-05-21T10:13:06.234357: train-step 8487, loss 0.165297, acc 0.98
2016-05-21T10:13:12.476874: train-step 8488, loss 0.156178, acc 0.97
2016-05-21T10:13:18.665943: train-step 8489, loss 0.168802, acc 0.96
2016-05-21T10:13:24.905711: train-step 8490, loss 0.171989, acc 0.99
2016-05-21T10:13:31.059944: train-step 8491, loss 0.141818, acc 0.99
2016-05-21T10:13:37.194208: train-step 8492, loss 0.132181, acc 0.99
2016-05-21T10:13:43.412175: train-step 8493, loss 0.171996, acc 0.96
2016-05-21T10:13:49.591568: train-step 8494, loss 0.132927, acc 1
2016-05-21T10:13:55.933815: train-step 8495, loss 0.129118, acc 1
2016-05-21T10:14:02.140883: train-step 8496, loss 0.167792, acc 0.97
2016-05-21T10:14:08.374370: train-step 8497, loss 0.175198, acc 0.96
2016-05-21T10:14:14.596817: train-step 8498, loss 0.156836, acc 0.98
2016-05-21T10:14:20.773931: train-step 8499, loss 0.195476, acc 0.96
2016-05-21T10:14:26.992131: train-step 8500, loss 0.167692, acc 1
epoch number is: 34
2016-05-21T10:14:33.364858: train-step 8501, loss 0.159858, acc 0.98
2016-05-21T10:14:39.518864: train-step 8502, loss 0.177225, acc 0.98
2016-05-21T10:14:45.721639: train-step 8503, loss 0.161428, acc 0.98
2016-05-21T10:14:51.872356: train-step 8504, loss 0.175449, acc 0.95
2016-05-21T10:14:58.076056: train-step 8505, loss 0.151759, acc 0.99
2016-05-21T10:15:04.396302: train-step 8506, loss 0.170383, acc 0.95
2016-05-21T10:15:10.611425: train-step 8507, loss 0.186096, acc 0.97
2016-05-21T10:15:16.894652: train-step 8508, loss 0.156462, acc 0.98
2016-05-21T10:15:23.030820: train-step 8509, loss 0.134085, acc 1
2016-05-21T10:15:29.196250: train-step 8510, loss 0.163519, acc 0.96
2016-05-21T10:15:35.427500: train-step 8511, loss 0.155345, acc 1
2016-05-21T10:15:41.633653: train-step 8512, loss 0.134081, acc 0.99
2016-05-21T10:15:47.821633: train-step 8513, loss 0.153113, acc 0.99
2016-05-21T10:15:54.030450: train-step 8514, loss 0.196839, acc 0.96
2016-05-21T10:16:00.172335: train-step 8515, loss 0.17759, acc 0.96
2016-05-21T10:16:06.379302: train-step 8516, loss 0.13676, acc 1
2016-05-21T10:16:12.511380: train-step 8517, loss 0.173881, acc 0.98
2016-05-21T10:16:18.659757: train-step 8518, loss 0.169691, acc 0.97
2016-05-21T10:16:24.866117: train-step 8519, loss 0.143044, acc 0.98
2016-05-21T10:16:31.048022: train-step 8520, loss 0.148593, acc 0.98
2016-05-21T10:16:37.206660: train-step 8521, loss 0.163976, acc 0.99
2016-05-21T10:16:43.490848: train-step 8522, loss 0.188974, acc 0.96
2016-05-21T10:16:49.743830: train-step 8523, loss 0.175012, acc 0.97
2016-05-21T10:16:56.008610: train-step 8524, loss 0.176828, acc 0.99
2016-05-21T10:17:02.188822: train-step 8525, loss 0.177711, acc 0.98
2016-05-21T10:17:08.349607: train-step 8526, loss 0.132653, acc 0.99
2016-05-21T10:17:14.597784: train-step 8527, loss 0.138366, acc 0.97
2016-05-21T10:17:20.799193: train-step 8528, loss 0.170994, acc 0.94
2016-05-21T10:17:27.003480: train-step 8529, loss 0.167576, acc 0.98
2016-05-21T10:17:33.211767: train-step 8530, loss 0.164495, acc 0.97
2016-05-21T10:17:39.454980: train-step 8531, loss 0.178998, acc 0.97
2016-05-21T10:17:45.704368: train-step 8532, loss 0.173263, acc 0.99
2016-05-21T10:17:51.904664: train-step 8533, loss 0.157311, acc 0.98
2016-05-21T10:17:58.078070: train-step 8534, loss 0.238433, acc 0.93
2016-05-21T10:18:04.287001: train-step 8535, loss 0.171347, acc 0.99
2016-05-21T10:18:10.529543: train-step 8536, loss 0.134693, acc 0.99
2016-05-21T10:18:16.705133: train-step 8537, loss 0.167262, acc 0.99
2016-05-21T10:18:23.081863: train-step 8538, loss 0.131705, acc 0.99
2016-05-21T10:18:29.466894: train-step 8539, loss 0.227792, acc 0.95
2016-05-21T10:18:35.668483: train-step 8540, loss 0.168666, acc 0.98
2016-05-21T10:18:41.944740: train-step 8541, loss 0.160907, acc 0.97
2016-05-21T10:18:48.201750: train-step 8542, loss 0.161941, acc 0.98
2016-05-21T10:18:54.357754: train-step 8543, loss 0.178836, acc 0.96
2016-05-21T10:19:00.591055: train-step 8544, loss 0.150962, acc 0.98
2016-05-21T10:19:06.992006: train-step 8545, loss 0.152218, acc 0.97
2016-05-21T10:19:13.238759: train-step 8546, loss 0.167383, acc 0.96
2016-05-21T10:19:19.461066: train-step 8547, loss 0.167136, acc 0.95
2016-05-21T10:19:25.688649: train-step 8548, loss 0.167728, acc 0.96
2016-05-21T10:19:31.860430: train-step 8549, loss 0.146446, acc 0.99
2016-05-21T10:19:38.051885: train-step 8550, loss 0.166971, acc 0.98
2016-05-21T10:19:44.218460: train-step 8551, loss 0.140604, acc 0.99
2016-05-21T10:19:50.381047: train-step 8552, loss 0.146598, acc 0.98
2016-05-21T10:19:56.695594: train-step 8553, loss 0.154929, acc 0.98
2016-05-21T10:20:02.981788: train-step 8554, loss 0.178282, acc 0.97
2016-05-21T10:20:09.301426: train-step 8555, loss 0.148668, acc 0.99
2016-05-21T10:20:15.634895: train-step 8556, loss 0.116234, acc 0.99
2016-05-21T10:20:22.044886: train-step 8557, loss 0.184418, acc 0.96
2016-05-21T10:20:28.249049: train-step 8558, loss 0.165159, acc 0.98
2016-05-21T10:20:34.492271: train-step 8559, loss 0.172244, acc 0.97
2016-05-21T10:20:40.828379: train-step 8560, loss 0.134528, acc 1
2016-05-21T10:20:47.192734: train-step 8561, loss 0.139372, acc 1
2016-05-21T10:20:53.491970: train-step 8562, loss 0.141285, acc 0.99
2016-05-21T10:20:59.664237: train-step 8563, loss 0.134034, acc 0.99
2016-05-21T10:21:06.302001: train-step 8564, loss 0.150861, acc 0.97
2016-05-21T10:21:12.667037: train-step 8565, loss 0.146585, acc 1
2016-05-21T10:21:18.892311: train-step 8566, loss 0.16246, acc 0.98
2016-05-21T10:21:25.105788: train-step 8567, loss 0.14749, acc 1
2016-05-21T10:21:31.498641: train-step 8568, loss 0.171512, acc 0.97
2016-05-21T10:21:37.898715: train-step 8569, loss 0.184741, acc 0.96
2016-05-21T10:21:44.076140: train-step 8570, loss 0.141086, acc 0.99
2016-05-21T10:21:50.281101: train-step 8571, loss 0.163469, acc 0.98
2016-05-21T10:21:56.475133: train-step 8572, loss 0.140228, acc 0.98
2016-05-21T10:22:02.644122: train-step 8573, loss 0.175305, acc 0.98
2016-05-21T10:22:08.854361: train-step 8574, loss 0.166366, acc 0.98
2016-05-21T10:22:15.112611: train-step 8575, loss 0.194049, acc 0.95
2016-05-21T10:22:21.372555: train-step 8576, loss 0.12847, acc 1
2016-05-21T10:22:27.559523: train-step 8577, loss 0.163035, acc 0.99
2016-05-21T10:22:34.079751: train-step 8578, loss 0.134669, acc 0.99
2016-05-21T10:22:40.448716: train-step 8579, loss 0.162614, acc 0.99
2016-05-21T10:22:46.610669: train-step 8580, loss 0.178949, acc 0.96
2016-05-21T10:22:53.147435: train-step 8581, loss 0.112154, acc 1
2016-05-21T10:22:59.548809: train-step 8582, loss 0.145331, acc 1
2016-05-21T10:23:05.969262: train-step 8583, loss 0.161458, acc 0.98
2016-05-21T10:23:12.246973: train-step 8584, loss 0.132137, acc 0.99
2016-05-21T10:23:18.433458: train-step 8585, loss 0.16332, acc 0.96
2016-05-21T10:23:24.748774: train-step 8586, loss 0.163936, acc 0.99
2016-05-21T10:23:30.980200: train-step 8587, loss 0.167626, acc 0.97
2016-05-21T10:23:37.230857: train-step 8588, loss 0.133557, acc 0.99
2016-05-21T10:23:43.784874: train-step 8589, loss 0.163434, acc 0.98
2016-05-21T10:23:50.011851: train-step 8590, loss 0.147585, acc 0.98
2016-05-21T10:23:56.192846: train-step 8591, loss 0.200118, acc 0.96
2016-05-21T10:24:02.404482: train-step 8592, loss 0.172441, acc 0.97
2016-05-21T10:24:08.621162: train-step 8593, loss 0.163282, acc 0.99
2016-05-21T10:24:14.943165: train-step 8594, loss 0.164984, acc 0.99
2016-05-21T10:24:21.220070: train-step 8595, loss 0.217725, acc 0.95
2016-05-21T10:24:27.387861: train-step 8596, loss 0.130773, acc 0.99
2016-05-21T10:24:33.567339: train-step 8597, loss 0.14619, acc 1
2016-05-21T10:24:39.703929: train-step 8598, loss 0.174106, acc 0.96
2016-05-21T10:24:46.060649: train-step 8599, loss 0.165787, acc 0.99
2016-05-21T10:24:52.341453: train-step 8600, loss 0.150799, acc 0.99
2016-05-21T10:24:58.497908: train-step 8601, loss 0.150989, acc 0.99
2016-05-21T10:25:04.715895: train-step 8602, loss 0.162694, acc 0.98
2016-05-21T10:25:10.872996: train-step 8603, loss 0.155545, acc 0.97
2016-05-21T10:25:17.071817: train-step 8604, loss 0.176926, acc 0.97
2016-05-21T10:25:23.270914: train-step 8605, loss 0.148477, acc 0.99
2016-05-21T10:25:29.500330: train-step 8606, loss 0.172907, acc 0.96
2016-05-21T10:25:35.829625: train-step 8607, loss 0.190372, acc 0.97
2016-05-21T10:25:42.189613: train-step 8608, loss 0.146013, acc 0.98
2016-05-21T10:25:48.434454: train-step 8609, loss 0.19562, acc 0.95
2016-05-21T10:25:54.661972: train-step 8610, loss 0.147609, acc 0.98
2016-05-21T10:26:01.014190: train-step 8611, loss 0.173441, acc 0.98
2016-05-21T10:26:07.267945: train-step 8612, loss 0.1876, acc 0.94
2016-05-21T10:26:13.534812: train-step 8613, loss 0.175744, acc 0.97
2016-05-21T10:26:19.693339: train-step 8614, loss 0.153914, acc 0.98
2016-05-21T10:26:25.904815: train-step 8615, loss 0.143847, acc 0.98
2016-05-21T10:26:32.223465: train-step 8616, loss 0.134747, acc 0.99
2016-05-21T10:26:38.572766: train-step 8617, loss 0.173941, acc 0.99
2016-05-21T10:26:44.793717: train-step 8618, loss 0.17114, acc 0.98
2016-05-21T10:26:51.009185: train-step 8619, loss 0.196793, acc 0.96
2016-05-21T10:26:57.208790: train-step 8620, loss 0.154901, acc 0.99
2016-05-21T10:27:03.460047: train-step 8621, loss 0.169971, acc 0.98
2016-05-21T10:27:10.029719: train-step 8622, loss 0.139023, acc 1
2016-05-21T10:27:16.325384: train-step 8623, loss 0.165219, acc 1
2016-05-21T10:27:22.733510: train-step 8624, loss 0.152243, acc 1
2016-05-21T10:27:28.931019: train-step 8625, loss 0.143621, acc 0.99
2016-05-21T10:27:35.077699: train-step 8626, loss 0.156801, acc 0.99
2016-05-21T10:27:41.387998: train-step 8627, loss 0.157844, acc 0.98
2016-05-21T10:27:47.579576: train-step 8628, loss 0.177093, acc 0.99
2016-05-21T10:27:53.840866: train-step 8629, loss 0.192189, acc 0.98
2016-05-21T10:28:00.076279: train-step 8630, loss 0.166602, acc 0.96
2016-05-21T10:28:06.302872: train-step 8631, loss 0.158553, acc 0.98
2016-05-21T10:28:12.558058: train-step 8632, loss 0.152235, acc 0.98
2016-05-21T10:28:18.748473: train-step 8633, loss 0.150526, acc 0.99
2016-05-21T10:28:24.900767: train-step 8634, loss 0.142006, acc 0.98
2016-05-21T10:28:31.303023: train-step 8635, loss 0.144097, acc 0.98
2016-05-21T10:28:37.656902: train-step 8636, loss 0.1326, acc 0.99
2016-05-21T10:28:43.918609: train-step 8637, loss 0.166543, acc 0.98
2016-05-21T10:28:50.137055: train-step 8638, loss 0.129137, acc 0.99
2016-05-21T10:28:56.343463: train-step 8639, loss 0.175961, acc 0.96
2016-05-21T10:29:02.513787: train-step 8640, loss 0.134336, acc 1
2016-05-21T10:29:08.730204: train-step 8641, loss 0.157275, acc 0.99
2016-05-21T10:29:14.925554: train-step 8642, loss 0.154684, acc 0.99
2016-05-21T10:29:21.290271: train-step 8643, loss 0.126207, acc 0.99
2016-05-21T10:29:27.544605: train-step 8644, loss 0.130171, acc 0.99
2016-05-21T10:29:33.846980: train-step 8645, loss 0.168182, acc 0.95
2016-05-21T10:29:40.436086: train-step 8646, loss 0.167817, acc 0.98
2016-05-21T10:29:46.716198: train-step 8647, loss 0.17906, acc 0.97
2016-05-21T10:29:52.977850: train-step 8648, loss 0.183325, acc 0.98
2016-05-21T10:29:59.195983: train-step 8649, loss 0.135537, acc 0.99
2016-05-21T10:30:05.769020: train-step 8650, loss 0.144412, acc 0.97
2016-05-21T10:30:12.085673: train-step 8651, loss 0.133758, acc 1
2016-05-21T10:30:18.439015: train-step 8652, loss 0.156373, acc 0.99
2016-05-21T10:30:24.706736: train-step 8653, loss 0.164843, acc 0.98
2016-05-21T10:30:30.952603: train-step 8654, loss 0.159765, acc 0.99
2016-05-21T10:30:37.262386: train-step 8655, loss 0.152614, acc 0.99
2016-05-21T10:30:43.625665: train-step 8656, loss 0.216613, acc 0.96
2016-05-21T10:30:49.890065: train-step 8657, loss 0.19423, acc 0.96
2016-05-21T10:30:56.133941: train-step 8658, loss 0.142451, acc 0.99
2016-05-21T10:31:02.309278: train-step 8659, loss 0.146234, acc 0.98
2016-05-21T10:31:08.470751: train-step 8660, loss 0.202586, acc 0.95
2016-05-21T10:31:14.715071: train-step 8661, loss 0.132237, acc 1
2016-05-21T10:31:20.915413: train-step 8662, loss 0.178703, acc 0.98
2016-05-21T10:31:27.161778: train-step 8663, loss 0.211828, acc 0.96
2016-05-21T10:31:33.407817: train-step 8664, loss 0.15543, acc 0.98
2016-05-21T10:31:39.599834: train-step 8665, loss 0.145551, acc 1
2016-05-21T10:31:45.816832: train-step 8666, loss 0.150106, acc 0.98
2016-05-21T10:31:52.029069: train-step 8667, loss 0.174253, acc 0.96
2016-05-21T10:31:58.254444: train-step 8668, loss 0.178432, acc 0.97
2016-05-21T10:32:04.481365: train-step 8669, loss 0.157139, acc 0.98
2016-05-21T10:32:10.630662: train-step 8670, loss 0.198062, acc 0.96
2016-05-21T10:32:16.881980: train-step 8671, loss 0.156313, acc 0.96
2016-05-21T10:32:23.024979: train-step 8672, loss 0.150049, acc 0.98
2016-05-21T10:32:29.172507: train-step 8673, loss 0.16008, acc 0.96
2016-05-21T10:32:35.325831: train-step 8674, loss 0.175351, acc 0.98
2016-05-21T10:32:41.557022: train-step 8675, loss 0.157336, acc 0.99
2016-05-21T10:32:47.775111: train-step 8676, loss 0.185792, acc 0.96
2016-05-21T10:32:53.997653: train-step 8677, loss 0.192155, acc 0.95
2016-05-21T10:33:00.206179: train-step 8678, loss 0.146099, acc 0.99
2016-05-21T10:33:06.376147: train-step 8679, loss 0.164829, acc 0.99
2016-05-21T10:33:12.561692: train-step 8680, loss 0.168332, acc 0.99
2016-05-21T10:33:18.784181: train-step 8681, loss 0.144052, acc 0.99
2016-05-21T10:33:24.984003: train-step 8682, loss 0.156807, acc 0.97
2016-05-21T10:33:31.246608: train-step 8683, loss 0.183017, acc 0.96
2016-05-21T10:33:37.453034: train-step 8684, loss 0.161263, acc 0.98
2016-05-21T10:33:43.637241: train-step 8685, loss 0.217943, acc 0.95
2016-05-21T10:33:49.793834: train-step 8686, loss 0.1432, acc 0.96
2016-05-21T10:33:56.194311: train-step 8687, loss 0.136144, acc 0.97
2016-05-21T10:34:02.643863: train-step 8688, loss 0.192051, acc 0.96
2016-05-21T10:34:08.904894: train-step 8689, loss 0.150262, acc 0.98
2016-05-21T10:34:15.168567: train-step 8690, loss 0.170657, acc 0.98
2016-05-21T10:34:21.377460: train-step 8691, loss 0.16668, acc 0.96
2016-05-21T10:34:27.599859: train-step 8692, loss 0.152766, acc 0.99
2016-05-21T10:34:33.817178: train-step 8693, loss 0.180111, acc 0.96
2016-05-21T10:34:39.995837: train-step 8694, loss 0.178087, acc 0.97
2016-05-21T10:34:46.233348: train-step 8695, loss 0.132431, acc 1
2016-05-21T10:34:52.415147: train-step 8696, loss 0.187692, acc 0.97
2016-05-21T10:34:58.595971: train-step 8697, loss 0.167559, acc 0.97
2016-05-21T10:35:04.856194: train-step 8698, loss 0.161273, acc 0.99
2016-05-21T10:35:11.013736: train-step 8699, loss 0.148081, acc 0.99
2016-05-21T10:35:17.187108: train-step 8700, loss 0.154317, acc 0.97
2016-05-21T10:35:23.355901: train-step 8701, loss 0.143824, acc 1
2016-05-21T10:35:29.587809: train-step 8702, loss 0.172997, acc 0.99
2016-05-21T10:35:35.816512: train-step 8703, loss 0.164405, acc 0.95
2016-05-21T10:35:42.076272: train-step 8704, loss 0.159246, acc 0.98
2016-05-21T10:35:48.420931: train-step 8705, loss 0.142135, acc 0.97
2016-05-21T10:35:54.846019: train-step 8706, loss 0.169064, acc 0.96
2016-05-21T10:36:01.063342: train-step 8707, loss 0.138694, acc 1
2016-05-21T10:36:07.238349: train-step 8708, loss 0.170411, acc 0.97
2016-05-21T10:36:13.376844: train-step 8709, loss 0.160279, acc 0.99
2016-05-21T10:36:19.601431: train-step 8710, loss 0.176628, acc 0.98
2016-05-21T10:36:25.765035: train-step 8711, loss 0.150828, acc 0.99
2016-05-21T10:36:31.920182: train-step 8712, loss 0.159281, acc 0.98
2016-05-21T10:36:38.091635: train-step 8713, loss 0.203581, acc 0.96
2016-05-21T10:36:44.467299: train-step 8714, loss 0.149647, acc 0.99
2016-05-21T10:36:50.866548: train-step 8715, loss 0.180454, acc 0.99
2016-05-21T10:36:57.083520: train-step 8716, loss 0.160981, acc 0.98
2016-05-21T10:37:03.286257: train-step 8717, loss 0.165715, acc 0.96
2016-05-21T10:37:09.450708: train-step 8718, loss 0.172117, acc 0.97
2016-05-21T10:37:15.648511: train-step 8719, loss 0.204647, acc 0.95
2016-05-21T10:37:21.995691: train-step 8720, loss 0.139466, acc 1
2016-05-21T10:37:28.187538: train-step 8721, loss 0.154926, acc 0.97
2016-05-21T10:37:34.440903: train-step 8722, loss 0.173838, acc 0.97
2016-05-21T10:37:40.717614: train-step 8723, loss 0.166359, acc 0.99
2016-05-21T10:37:47.355285: train-step 8724, loss 0.162124, acc 1
2016-05-21T10:37:53.647580: train-step 8725, loss 0.170546, acc 0.97
2016-05-21T10:37:59.815807: train-step 8726, loss 0.166749, acc 0.98
2016-05-21T10:38:06.404585: train-step 8727, loss 0.162923, acc 0.98
2016-05-21T10:38:12.765218: train-step 8728, loss 0.144677, acc 1
2016-05-21T10:38:18.982385: train-step 8729, loss 0.170565, acc 0.98
2016-05-21T10:38:25.199922: train-step 8730, loss 0.203067, acc 0.96
2016-05-21T10:38:31.403365: train-step 8731, loss 0.149895, acc 0.97
2016-05-21T10:38:37.746452: train-step 8732, loss 0.151344, acc 0.98
2016-05-21T10:38:44.106852: train-step 8733, loss 0.167139, acc 0.99
2016-05-21T10:38:50.409112: train-step 8734, loss 0.142607, acc 0.99
2016-05-21T10:38:56.660746: train-step 8735, loss 0.160881, acc 0.99
2016-05-21T10:39:03.267228: train-step 8736, loss 0.184822, acc 0.96
2016-05-21T10:39:09.551594: train-step 8737, loss 0.160365, acc 0.99
2016-05-21T10:39:15.735622: train-step 8738, loss 0.144865, acc 0.99
2016-05-21T10:39:21.981622: train-step 8739, loss 0.194654, acc 0.97
2016-05-21T10:39:28.294801: train-step 8740, loss 0.173683, acc 0.96
2016-05-21T10:39:34.532194: train-step 8741, loss 0.193071, acc 0.97
2016-05-21T10:39:40.745151: train-step 8742, loss 0.155262, acc 0.99
2016-05-21T10:39:46.936609: train-step 8743, loss 0.190922, acc 0.97
2016-05-21T10:39:53.147438: train-step 8744, loss 0.141104, acc 0.98
2016-05-21T10:39:59.396151: train-step 8745, loss 0.130437, acc 1
2016-05-21T10:40:05.562107: train-step 8746, loss 0.192686, acc 0.96
2016-05-21T10:40:11.749867: train-step 8747, loss 0.220298, acc 0.96
2016-05-21T10:40:17.891687: train-step 8748, loss 0.143353, acc 0.98
2016-05-21T10:40:24.130112: train-step 8749, loss 0.213507, acc 0.94
2016-05-21T10:40:30.282966: train-step 8750, loss 0.18647, acc 0.97
epoch number is: 35
2016-05-21T10:40:36.823291: train-step 8751, loss 0.196344, acc 0.98
2016-05-21T10:40:43.010720: train-step 8752, loss 0.171028, acc 0.97
2016-05-21T10:40:49.204240: train-step 8753, loss 0.141366, acc 0.98
2016-05-21T10:40:55.354148: train-step 8754, loss 0.134568, acc 0.98
2016-05-21T10:41:01.536463: train-step 8755, loss 0.176589, acc 0.98
2016-05-21T10:41:07.720204: train-step 8756, loss 0.15493, acc 0.97
2016-05-21T10:41:13.904128: train-step 8757, loss 0.156148, acc 0.98
2016-05-21T10:41:20.104621: train-step 8758, loss 0.126405, acc 1
2016-05-21T10:41:26.320462: train-step 8759, loss 0.140444, acc 1
2016-05-21T10:41:32.540022: train-step 8760, loss 0.200825, acc 0.95
2016-05-21T10:41:38.693538: train-step 8761, loss 0.142327, acc 1
2016-05-21T10:41:44.919363: train-step 8762, loss 0.150909, acc 0.97
2016-05-21T10:41:51.156298: train-step 8763, loss 0.160332, acc 0.98
2016-05-21T10:41:57.362394: train-step 8764, loss 0.15052, acc 0.99
2016-05-21T10:42:03.522542: train-step 8765, loss 0.133197, acc 0.99
2016-05-21T10:42:09.709897: train-step 8766, loss 0.161898, acc 0.97
2016-05-21T10:42:15.925119: train-step 8767, loss 0.163906, acc 0.97
2016-05-21T10:42:22.147451: train-step 8768, loss 0.179397, acc 0.96
2016-05-21T10:42:28.580781: train-step 8769, loss 0.128797, acc 1
2016-05-21T10:42:35.043621: train-step 8770, loss 0.140408, acc 0.99
2016-05-21T10:42:41.290204: train-step 8771, loss 0.152631, acc 0.98
2016-05-21T10:42:47.479243: train-step 8772, loss 0.147419, acc 0.98
2016-05-21T10:42:53.687566: train-step 8773, loss 0.165968, acc 0.98
2016-05-21T10:42:59.928970: train-step 8774, loss 0.180808, acc 0.94
2016-05-21T10:43:06.215120: train-step 8775, loss 0.144271, acc 1
2016-05-21T10:43:12.432349: train-step 8776, loss 0.17161, acc 0.98
2016-05-21T10:43:18.668174: train-step 8777, loss 0.170877, acc 0.98
2016-05-21T10:43:24.926351: train-step 8778, loss 0.144074, acc 0.99
2016-05-21T10:43:31.132820: train-step 8779, loss 0.181438, acc 0.95
2016-05-21T10:43:37.341713: train-step 8780, loss 0.14532, acc 0.99
2016-05-21T10:43:43.571244: train-step 8781, loss 0.121247, acc 0.99
2016-05-21T10:43:49.784543: train-step 8782, loss 0.168439, acc 0.98
2016-05-21T10:43:56.004403: train-step 8783, loss 0.179958, acc 0.97
2016-05-21T10:44:02.241155: train-step 8784, loss 0.169826, acc 0.97
2016-05-21T10:44:08.453879: train-step 8785, loss 0.134639, acc 1
2016-05-21T10:44:15.581116: train-step 8786, loss 0.207296, acc 0.94
2016-05-21T10:44:22.321565: train-step 8787, loss 0.133916, acc 0.99
2016-05-21T10:44:28.639238: train-step 8788, loss 0.132516, acc 1
2016-05-21T10:44:35.052467: train-step 8789, loss 0.180497, acc 0.97
2016-05-21T10:44:41.271829: train-step 8790, loss 0.153413, acc 0.99
2016-05-21T10:44:47.517237: train-step 8791, loss 0.156, acc 0.99
2016-05-21T10:44:53.735419: train-step 8792, loss 0.149053, acc 0.99
2016-05-21T10:44:59.980324: train-step 8793, loss 0.15087, acc 0.99
2016-05-21T10:45:06.228874: train-step 8794, loss 0.135792, acc 0.99
2016-05-21T10:45:12.420773: train-step 8795, loss 0.163989, acc 0.97
2016-05-21T10:45:18.636305: train-step 8796, loss 0.145705, acc 0.98
2016-05-21T10:45:24.897117: train-step 8797, loss 0.157729, acc 0.98
2016-05-21T10:45:31.288176: train-step 8798, loss 0.102591, acc 1
2016-05-21T10:45:37.845550: train-step 8799, loss 0.180201, acc 0.97
2016-05-21T10:45:44.107201: train-step 8800, loss 0.15611, acc 0.99
2016-05-21T10:45:50.293860: train-step 8801, loss 0.132169, acc 1
2016-05-21T10:45:56.460080: train-step 8802, loss 0.186745, acc 0.96
2016-05-21T10:46:02.650331: train-step 8803, loss 0.134133, acc 0.98
2016-05-21T10:46:08.862944: train-step 8804, loss 0.153046, acc 1
2016-05-21T10:46:15.042190: train-step 8805, loss 0.189495, acc 0.95
2016-05-21T10:46:21.302881: train-step 8806, loss 0.126853, acc 0.99
2016-05-21T10:46:27.516675: train-step 8807, loss 0.143412, acc 0.99
2016-05-21T10:46:33.784572: train-step 8808, loss 0.14236, acc 0.99
2016-05-21T10:46:39.983555: train-step 8809, loss 0.183197, acc 0.95
2016-05-21T10:46:46.136483: train-step 8810, loss 0.16374, acc 0.97
2016-05-21T10:46:52.331734: train-step 8811, loss 0.14467, acc 0.97
2016-05-21T10:46:58.569380: train-step 8812, loss 0.167836, acc 0.96
2016-05-21T10:47:04.753730: train-step 8813, loss 0.158151, acc 0.97
2016-05-21T10:47:10.964815: train-step 8814, loss 0.167087, acc 0.98
2016-05-21T10:47:17.202223: train-step 8815, loss 0.146022, acc 0.99
2016-05-21T10:47:23.387712: train-step 8816, loss 0.141218, acc 1
2016-05-21T10:47:29.570517: train-step 8817, loss 0.165591, acc 0.98
2016-05-21T10:47:35.769411: train-step 8818, loss 0.175081, acc 0.96
2016-05-21T10:47:41.957890: train-step 8819, loss 0.225228, acc 0.94
2016-05-21T10:47:48.085574: train-step 8820, loss 0.123631, acc 1
2016-05-21T10:47:54.307700: train-step 8821, loss 0.17064, acc 0.98
2016-05-21T10:48:00.555255: train-step 8822, loss 0.136737, acc 0.99
2016-05-21T10:48:06.709727: train-step 8823, loss 0.143623, acc 0.97
2016-05-21T10:48:12.929922: train-step 8824, loss 0.15512, acc 0.98
2016-05-21T10:48:19.141374: train-step 8825, loss 0.18296, acc 0.96
2016-05-21T10:48:25.336854: train-step 8826, loss 0.214271, acc 0.96
2016-05-21T10:48:31.566734: train-step 8827, loss 0.177303, acc 0.98
2016-05-21T10:48:37.774339: train-step 8828, loss 0.137213, acc 0.99
2016-05-21T10:48:43.972482: train-step 8829, loss 0.16138, acc 0.97
2016-05-21T10:48:50.185488: train-step 8830, loss 0.132717, acc 1
2016-05-21T10:48:56.795780: train-step 8831, loss 0.17022, acc 1
2016-05-21T10:49:03.042876: train-step 8832, loss 0.153848, acc 0.99
2016-05-21T10:49:09.197970: train-step 8833, loss 0.156818, acc 0.98
2016-05-21T10:49:15.437493: train-step 8834, loss 0.15644, acc 0.99
2016-05-21T10:49:21.630300: train-step 8835, loss 0.192983, acc 0.97
2016-05-21T10:49:27.864564: train-step 8836, loss 0.144347, acc 0.99
2016-05-21T10:49:34.114819: train-step 8837, loss 0.18032, acc 0.97
2016-05-21T10:49:40.314076: train-step 8838, loss 0.152289, acc 0.99
2016-05-21T10:49:46.484804: train-step 8839, loss 0.132013, acc 1
2016-05-21T10:49:52.738364: train-step 8840, loss 0.18408, acc 0.97
2016-05-21T10:49:59.023500: train-step 8841, loss 0.124835, acc 0.99
2016-05-21T10:50:05.243092: train-step 8842, loss 0.155323, acc 0.98
2016-05-21T10:50:11.513395: train-step 8843, loss 0.168954, acc 0.98
2016-05-21T10:50:17.856973: train-step 8844, loss 0.157766, acc 0.99
2016-05-21T10:50:24.066719: train-step 8845, loss 0.178876, acc 0.97
2016-05-21T10:50:30.298571: train-step 8846, loss 0.138744, acc 0.99
2016-05-21T10:50:36.626757: train-step 8847, loss 0.149877, acc 1
2016-05-21T10:50:42.864948: train-step 8848, loss 0.166832, acc 0.98
2016-05-21T10:50:49.304563: train-step 8849, loss 0.15869, acc 0.96
2016-05-21T10:50:55.707831: train-step 8850, loss 0.160936, acc 0.97
2016-05-21T10:51:01.949512: train-step 8851, loss 0.134855, acc 1
2016-05-21T10:51:08.129109: train-step 8852, loss 0.167035, acc 0.98
2016-05-21T10:51:14.354545: train-step 8853, loss 0.144415, acc 1
2016-05-21T10:51:20.526973: train-step 8854, loss 0.144167, acc 0.99
2016-05-21T10:51:26.739844: train-step 8855, loss 0.134598, acc 1
2016-05-21T10:51:32.907377: train-step 8856, loss 0.219097, acc 0.94
2016-05-21T10:51:39.120592: train-step 8857, loss 0.202615, acc 0.95
2016-05-21T10:51:45.493190: train-step 8858, loss 0.168713, acc 0.95
2016-05-21T10:51:51.856451: train-step 8859, loss 0.149484, acc 0.99
2016-05-21T10:51:58.093681: train-step 8860, loss 0.165952, acc 0.99
2016-05-21T10:52:04.269369: train-step 8861, loss 0.187826, acc 0.96
2016-05-21T10:52:10.495853: train-step 8862, loss 0.135915, acc 1
2016-05-21T10:52:16.653550: train-step 8863, loss 0.164972, acc 0.97
2016-05-21T10:52:22.840492: train-step 8864, loss 0.195826, acc 0.95
2016-05-21T10:52:29.037063: train-step 8865, loss 0.230602, acc 0.97
2016-05-21T10:52:35.297075: train-step 8866, loss 0.146314, acc 0.97
2016-05-21T10:52:41.642864: train-step 8867, loss 0.175535, acc 0.99
2016-05-21T10:52:48.044587: train-step 8868, loss 0.142565, acc 0.99
2016-05-21T10:52:54.277528: train-step 8869, loss 0.165471, acc 0.96
2016-05-21T10:53:00.474125: train-step 8870, loss 0.148551, acc 0.98
2016-05-21T10:53:06.709963: train-step 8871, loss 0.121301, acc 1
2016-05-21T10:53:13.166610: train-step 8872, loss 0.133831, acc 0.99
2016-05-21T10:53:19.621905: train-step 8873, loss 0.132726, acc 1
2016-05-21T10:53:25.870763: train-step 8874, loss 0.170262, acc 0.96
2016-05-21T10:53:32.090712: train-step 8875, loss 0.159689, acc 0.97
2016-05-21T10:53:38.332681: train-step 8876, loss 0.174301, acc 0.97
2016-05-21T10:53:44.557881: train-step 8877, loss 0.172833, acc 0.98
2016-05-21T10:53:50.874162: train-step 8878, loss 0.148803, acc 0.98
2016-05-21T10:53:57.454150: train-step 8879, loss 0.141653, acc 1
2016-05-21T10:54:03.804032: train-step 8880, loss 0.154318, acc 1
2016-05-21T10:54:10.117082: train-step 8881, loss 0.192993, acc 0.96
2016-05-21T10:54:16.306836: train-step 8882, loss 0.155001, acc 0.98
2016-05-21T10:54:22.506784: train-step 8883, loss 0.128254, acc 1
2016-05-21T10:54:28.883127: train-step 8884, loss 0.140909, acc 1
2016-05-21T10:54:35.139714: train-step 8885, loss 0.171118, acc 0.98
2016-05-21T10:54:41.372515: train-step 8886, loss 0.149139, acc 1
2016-05-21T10:54:47.615321: train-step 8887, loss 0.170289, acc 0.97
2016-05-21T10:54:53.758990: train-step 8888, loss 0.14156, acc 0.99
2016-05-21T10:54:59.957888: train-step 8889, loss 0.16628, acc 0.97
2016-05-21T10:55:06.193279: train-step 8890, loss 0.187732, acc 0.95
2016-05-21T10:55:12.399820: train-step 8891, loss 0.189125, acc 0.96
2016-05-21T10:55:18.627035: train-step 8892, loss 0.183953, acc 0.95
2016-05-21T10:55:24.843704: train-step 8893, loss 0.175157, acc 0.97
2016-05-21T10:55:31.040761: train-step 8894, loss 0.148642, acc 0.97
2016-05-21T10:55:37.208776: train-step 8895, loss 0.163359, acc 0.98
2016-05-21T10:55:43.364533: train-step 8896, loss 0.150675, acc 0.98
2016-05-21T10:55:49.571718: train-step 8897, loss 0.177475, acc 0.96
2016-05-21T10:55:55.719627: train-step 8898, loss 0.14571, acc 0.97
2016-05-21T10:56:01.925634: train-step 8899, loss 0.13609, acc 1
2016-05-21T10:56:08.096108: train-step 8900, loss 0.150163, acc 0.97
2016-05-21T10:56:14.268568: train-step 8901, loss 0.15378, acc 0.97
2016-05-21T10:56:20.419748: train-step 8902, loss 0.156782, acc 0.98
2016-05-21T10:56:26.614854: train-step 8903, loss 0.151868, acc 0.99
2016-05-21T10:56:32.802962: train-step 8904, loss 0.12943, acc 0.99
2016-05-21T10:56:39.026079: train-step 8905, loss 0.148064, acc 0.99
2016-05-21T10:56:45.300447: train-step 8906, loss 0.176534, acc 0.99
2016-05-21T10:56:51.496612: train-step 8907, loss 0.1756, acc 0.95
2016-05-21T10:56:57.700238: train-step 8908, loss 0.139283, acc 1
2016-05-21T10:57:03.904044: train-step 8909, loss 0.158978, acc 0.98
2016-05-21T10:57:10.074135: train-step 8910, loss 0.210386, acc 0.95
2016-05-21T10:57:16.247580: train-step 8911, loss 0.162101, acc 0.99
2016-05-21T10:57:22.413820: train-step 8912, loss 0.146312, acc 1
2016-05-21T10:57:28.555573: train-step 8913, loss 0.137598, acc 0.99
2016-05-21T10:57:34.768452: train-step 8914, loss 0.162222, acc 0.97
2016-05-21T10:57:41.023679: train-step 8915, loss 0.184783, acc 0.96
2016-05-21T10:57:47.217514: train-step 8916, loss 0.157856, acc 0.97
2016-05-21T10:57:53.389102: train-step 8917, loss 0.130706, acc 1
2016-05-21T10:57:59.604637: train-step 8918, loss 0.146218, acc 0.98
2016-05-21T10:58:05.782211: train-step 8919, loss 0.162809, acc 0.97
2016-05-21T10:58:12.000097: train-step 8920, loss 0.166712, acc 0.98
2016-05-21T10:58:18.154282: train-step 8921, loss 0.138638, acc 0.98
2016-05-21T10:58:24.371226: train-step 8922, loss 0.148683, acc 0.98
2016-05-21T10:58:30.602039: train-step 8923, loss 0.158217, acc 0.97
2016-05-21T10:58:36.762473: train-step 8924, loss 0.185075, acc 0.96
2016-05-21T10:58:42.990296: train-step 8925, loss 0.154186, acc 0.97
2016-05-21T10:58:49.162006: train-step 8926, loss 0.181382, acc 0.98
2016-05-21T10:58:55.346180: train-step 8927, loss 0.158335, acc 0.98
2016-05-21T10:59:01.477553: train-step 8928, loss 0.153864, acc 0.98
2016-05-21T10:59:07.661553: train-step 8929, loss 0.137997, acc 0.99
2016-05-21T10:59:13.842856: train-step 8930, loss 0.183996, acc 0.97
2016-05-21T10:59:20.111425: train-step 8931, loss 0.180635, acc 0.97
2016-05-21T10:59:26.363916: train-step 8932, loss 0.179805, acc 0.95
2016-05-21T10:59:32.514930: train-step 8933, loss 0.145605, acc 0.99
2016-05-21T10:59:38.736893: train-step 8934, loss 0.131992, acc 0.99
2016-05-21T10:59:44.961450: train-step 8935, loss 0.171569, acc 0.98
2016-05-21T10:59:51.292240: train-step 8936, loss 0.196965, acc 0.93
2016-05-21T10:59:57.499340: train-step 8937, loss 0.151837, acc 0.97
2016-05-21T11:00:03.683147: train-step 8938, loss 0.162277, acc 0.98
2016-05-21T11:00:09.896220: train-step 8939, loss 0.190749, acc 0.97
2016-05-21T11:00:16.098333: train-step 8940, loss 0.126449, acc 1
2016-05-21T11:00:22.263986: train-step 8941, loss 0.154432, acc 0.98
2016-05-21T11:00:28.467429: train-step 8942, loss 0.137031, acc 1
2016-05-21T11:00:34.624397: train-step 8943, loss 0.186207, acc 0.96
2016-05-21T11:00:40.861187: train-step 8944, loss 0.122518, acc 0.99
2016-05-21T11:00:47.035533: train-step 8945, loss 0.133777, acc 0.98
2016-05-21T11:00:53.263021: train-step 8946, loss 0.159135, acc 0.99
2016-05-21T11:00:59.471963: train-step 8947, loss 0.140905, acc 0.99
2016-05-21T11:01:05.634822: train-step 8948, loss 0.173426, acc 0.98
2016-05-21T11:01:11.848593: train-step 8949, loss 0.197586, acc 0.95
2016-05-21T11:01:18.107233: train-step 8950, loss 0.151211, acc 0.99
2016-05-21T11:01:24.355114: train-step 8951, loss 0.129697, acc 0.99
2016-05-21T11:01:30.607479: train-step 8952, loss 0.162879, acc 0.98
2016-05-21T11:01:36.804620: train-step 8953, loss 0.178865, acc 0.97
2016-05-21T11:01:43.059658: train-step 8954, loss 0.166055, acc 0.98
2016-05-21T11:01:49.260996: train-step 8955, loss 0.153765, acc 0.98
2016-05-21T11:01:55.570836: train-step 8956, loss 0.178248, acc 0.96
2016-05-21T11:02:02.104648: train-step 8957, loss 0.136479, acc 0.98
2016-05-21T11:02:08.706038: train-step 8958, loss 0.131993, acc 0.99
2016-05-21T11:02:15.027291: train-step 8959, loss 0.123865, acc 1
2016-05-21T11:02:21.260707: train-step 8960, loss 0.16978, acc 0.97
2016-05-21T11:02:27.434181: train-step 8961, loss 0.171258, acc 0.97
2016-05-21T11:02:33.762256: train-step 8962, loss 0.177474, acc 0.96
2016-05-21T11:02:39.952658: train-step 8963, loss 0.15208, acc 0.97
2016-05-21T11:02:46.090207: train-step 8964, loss 0.146183, acc 0.99
2016-05-21T11:02:52.241736: train-step 8965, loss 0.189583, acc 0.95
2016-05-21T11:02:58.523217: train-step 8966, loss 0.144002, acc 0.99
2016-05-21T11:03:04.713164: train-step 8967, loss 0.180703, acc 0.97
2016-05-21T11:03:11.058650: train-step 8968, loss 0.20597, acc 0.97
2016-05-21T11:03:17.297460: train-step 8969, loss 0.129455, acc 0.98
2016-05-21T11:03:23.525752: train-step 8970, loss 0.168952, acc 0.97
2016-05-21T11:03:29.757094: train-step 8971, loss 0.145199, acc 1
2016-05-21T11:03:36.013833: train-step 8972, loss 0.1525, acc 1
2016-05-21T11:03:42.152467: train-step 8973, loss 0.167869, acc 0.97
2016-05-21T11:03:48.484668: train-step 8974, loss 0.135291, acc 0.98
2016-05-21T11:03:54.716950: train-step 8975, loss 0.142849, acc 0.99
2016-05-21T11:04:00.862904: train-step 8976, loss 0.143766, acc 0.98
2016-05-21T11:04:07.219089: train-step 8977, loss 0.157701, acc 0.99
2016-05-21T11:04:13.400202: train-step 8978, loss 0.161645, acc 0.97
2016-05-21T11:04:19.581237: train-step 8979, loss 0.197986, acc 0.96
2016-05-21T11:04:25.832757: train-step 8980, loss 0.169335, acc 1
2016-05-21T11:04:31.997206: train-step 8981, loss 0.19458, acc 0.96
2016-05-21T11:04:38.555687: train-step 8982, loss 0.128624, acc 1
2016-05-21T11:04:44.963953: train-step 8983, loss 0.128529, acc 1
2016-05-21T11:04:51.358839: train-step 8984, loss 0.152665, acc 0.98
2016-05-21T11:04:57.571946: train-step 8985, loss 0.143182, acc 0.99
2016-05-21T11:05:03.779340: train-step 8986, loss 0.182264, acc 0.95
2016-05-21T11:05:09.933072: train-step 8987, loss 0.170988, acc 0.97
2016-05-21T11:05:16.123383: train-step 8988, loss 0.200363, acc 0.96
2016-05-21T11:05:22.510358: train-step 8989, loss 0.244996, acc 0.92
2016-05-21T11:05:28.696510: train-step 8990, loss 0.141319, acc 0.99
2016-05-21T11:05:34.917785: train-step 8991, loss 0.154875, acc 0.98
2016-05-21T11:05:41.070576: train-step 8992, loss 0.197227, acc 0.95
2016-05-21T11:05:47.273006: train-step 8993, loss 0.164371, acc 0.96
2016-05-21T11:05:53.733387: train-step 8994, loss 0.153831, acc 0.98
2016-05-21T11:06:00.237668: train-step 8995, loss 0.130869, acc 1
2016-05-21T11:06:06.427139: train-step 8996, loss 0.158119, acc 0.99
2016-05-21T11:06:12.734462: train-step 8997, loss 0.136229, acc 0.99
2016-05-21T11:06:18.996400: train-step 8998, loss 0.146603, acc 0.97
2016-05-21T11:06:25.220288: train-step 8999, loss 0.163724, acc 0.96
2016-05-21T11:06:31.447814: train-step 9000, loss 0.16568, acc 0.97
epoch number is: 36
2016-05-21T11:06:37.732918: train-step 9001, loss 0.159039, acc 1
2016-05-21T11:06:43.972783: train-step 9002, loss 0.230948, acc 0.94
2016-05-21T11:06:50.175165: train-step 9003, loss 0.156599, acc 0.98
2016-05-21T11:06:56.353740: train-step 9004, loss 0.160301, acc 0.97
2016-05-21T11:07:02.522192: train-step 9005, loss 0.159185, acc 0.97
2016-05-21T11:07:08.788556: train-step 9006, loss 0.166736, acc 0.97
2016-05-21T11:07:15.126158: train-step 9007, loss 0.140941, acc 1
2016-05-21T11:07:21.347366: train-step 9008, loss 0.195582, acc 0.96
2016-05-21T11:07:27.571321: train-step 9009, loss 0.144038, acc 0.99
2016-05-21T11:07:33.977050: train-step 9010, loss 0.146749, acc 0.98
2016-05-21T11:07:40.175927: train-step 9011, loss 0.171176, acc 0.96
2016-05-21T11:07:46.424223: train-step 9012, loss 0.142495, acc 0.98
2016-05-21T11:07:52.682478: train-step 9013, loss 0.166847, acc 0.97
2016-05-21T11:07:58.865819: train-step 9014, loss 0.175536, acc 0.98
2016-05-21T11:08:05.176046: train-step 9015, loss 0.121795, acc 0.99
2016-05-21T11:08:11.515325: train-step 9016, loss 0.156744, acc 0.99
2016-05-21T11:08:17.814803: train-step 9017, loss 0.144263, acc 0.98
2016-05-21T11:08:24.091384: train-step 9018, loss 0.164598, acc 0.97
2016-05-21T11:08:30.407859: train-step 9019, loss 0.130179, acc 0.99
2016-05-21T11:08:37.698114: train-step 9020, loss 0.166702, acc 0.97
2016-05-21T11:08:44.537028: train-step 9021, loss 0.145258, acc 1
2016-05-21T11:08:51.003672: train-step 9022, loss 0.174811, acc 0.99
2016-05-21T11:08:57.311373: train-step 9023, loss 0.158264, acc 0.98
2016-05-21T11:09:03.639280: train-step 9024, loss 0.164226, acc 0.98
2016-05-21T11:09:09.846751: train-step 9025, loss 0.143993, acc 0.98
2016-05-21T11:09:16.090942: train-step 9026, loss 0.193766, acc 0.95
2016-05-21T11:09:22.273489: train-step 9027, loss 0.135897, acc 0.99
2016-05-21T11:09:28.530192: train-step 9028, loss 0.172461, acc 0.97
2016-05-21T11:09:34.735868: train-step 9029, loss 0.155804, acc 0.99
2016-05-21T11:09:40.984253: train-step 9030, loss 0.147384, acc 1
2016-05-21T11:09:47.217966: train-step 9031, loss 0.165096, acc 0.97
2016-05-21T11:09:53.595916: train-step 9032, loss 0.150837, acc 0.98
2016-05-21T11:09:59.801210: train-step 9033, loss 0.170387, acc 0.97
2016-05-21T11:10:06.056132: train-step 9034, loss 0.145495, acc 0.99
2016-05-21T11:10:12.448804: train-step 9035, loss 0.140332, acc 0.99
2016-05-21T11:10:19.094090: train-step 9036, loss 0.160781, acc 0.99
2016-05-21T11:10:25.511540: train-step 9037, loss 0.140828, acc 0.99
2016-05-21T11:10:31.856963: train-step 9038, loss 0.131365, acc 0.99
2016-05-21T11:10:38.070194: train-step 9039, loss 0.153181, acc 0.98
2016-05-21T11:10:44.273409: train-step 9040, loss 0.163253, acc 0.96
2016-05-21T11:10:50.499622: train-step 9041, loss 0.155254, acc 0.99
2016-05-21T11:10:56.723217: train-step 9042, loss 0.168699, acc 0.97
2016-05-21T11:11:02.945652: train-step 9043, loss 0.17142, acc 0.97
2016-05-21T11:11:09.128688: train-step 9044, loss 0.138663, acc 0.99
2016-05-21T11:11:15.502080: train-step 9045, loss 0.156318, acc 0.97
2016-05-21T11:11:21.731984: train-step 9046, loss 0.134136, acc 1
2016-05-21T11:11:27.937464: train-step 9047, loss 0.13679, acc 0.98
2016-05-21T11:11:34.198535: train-step 9048, loss 0.135455, acc 0.99
2016-05-21T11:11:40.450874: train-step 9049, loss 0.16698, acc 0.95
2016-05-21T11:11:46.716504: train-step 9050, loss 0.158131, acc 0.99
2016-05-21T11:11:52.962003: train-step 9051, loss 0.139687, acc 0.98
2016-05-21T11:11:59.229163: train-step 9052, loss 0.156366, acc 0.99
2016-05-21T11:12:05.477124: train-step 9053, loss 0.171596, acc 0.99
2016-05-21T11:12:11.725010: train-step 9054, loss 0.129662, acc 0.98
2016-05-21T11:12:18.094375: train-step 9055, loss 0.126139, acc 0.99
2016-05-21T11:12:24.521912: train-step 9056, loss 0.186739, acc 0.99
2016-05-21T11:12:30.778094: train-step 9057, loss 0.169427, acc 0.99
2016-05-21T11:12:36.993095: train-step 9058, loss 0.149785, acc 0.97
2016-05-21T11:12:43.271680: train-step 9059, loss 0.15209, acc 0.98
2016-05-21T11:12:49.554975: train-step 9060, loss 0.15658, acc 0.99
2016-05-21T11:12:55.835530: train-step 9061, loss 0.154292, acc 0.99
2016-05-21T11:13:02.055983: train-step 9062, loss 0.129927, acc 0.99
2016-05-21T11:13:08.268355: train-step 9063, loss 0.14571, acc 0.98
2016-05-21T11:13:14.559677: train-step 9064, loss 0.13233, acc 0.99
2016-05-21T11:13:20.852368: train-step 9065, loss 0.151334, acc 0.98
2016-05-21T11:13:27.020200: train-step 9066, loss 0.169174, acc 0.98
2016-05-21T11:13:33.215140: train-step 9067, loss 0.181485, acc 0.98
2016-05-21T11:13:39.435663: train-step 9068, loss 0.159833, acc 0.99
2016-05-21T11:13:45.679302: train-step 9069, loss 0.189281, acc 0.96
2016-05-21T11:13:51.874075: train-step 9070, loss 0.15256, acc 0.98
2016-05-21T11:13:58.128499: train-step 9071, loss 0.146823, acc 0.99
2016-05-21T11:14:04.405955: train-step 9072, loss 0.149533, acc 0.99
2016-05-21T11:14:10.592458: train-step 9073, loss 0.147483, acc 0.96
2016-05-21T11:14:16.862388: train-step 9074, loss 0.132862, acc 0.99
2016-05-21T11:14:23.091401: train-step 9075, loss 0.159734, acc 0.97
2016-05-21T11:14:29.317187: train-step 9076, loss 0.140826, acc 0.97
2016-05-21T11:14:35.559575: train-step 9077, loss 0.133783, acc 1
2016-05-21T11:14:41.766313: train-step 9078, loss 0.12841, acc 0.99
2016-05-21T11:14:48.022982: train-step 9079, loss 0.136166, acc 0.99
2016-05-21T11:14:54.656611: train-step 9080, loss 0.199457, acc 0.95
2016-05-21T11:15:00.975446: train-step 9081, loss 0.138885, acc 0.99
2016-05-21T11:15:07.359619: train-step 9082, loss 0.126756, acc 0.99
2016-05-21T11:15:13.551454: train-step 9083, loss 0.16139, acc 0.99
2016-05-21T11:15:20.137681: train-step 9084, loss 0.153293, acc 1
2016-05-21T11:15:26.528978: train-step 9085, loss 0.183661, acc 0.97
2016-05-21T11:15:32.793678: train-step 9086, loss 0.150494, acc 0.97
2016-05-21T11:15:39.019458: train-step 9087, loss 0.175467, acc 0.96
2016-05-21T11:15:45.185011: train-step 9088, loss 0.172611, acc 0.97
2016-05-21T11:15:51.400387: train-step 9089, loss 0.133051, acc 0.98
2016-05-21T11:15:57.611104: train-step 9090, loss 0.169597, acc 0.97
2016-05-21T11:16:03.850284: train-step 9091, loss 0.153963, acc 0.98
2016-05-21T11:16:10.032317: train-step 9092, loss 0.162391, acc 0.98
2016-05-21T11:16:16.217384: train-step 9093, loss 0.137496, acc 0.99
2016-05-21T11:16:22.494997: train-step 9094, loss 0.174939, acc 0.98
2016-05-21T11:16:28.704695: train-step 9095, loss 0.13967, acc 1
2016-05-21T11:16:34.971785: train-step 9096, loss 0.153975, acc 0.98
2016-05-21T11:16:41.493899: train-step 9097, loss 0.121686, acc 1
2016-05-21T11:16:47.830802: train-step 9098, loss 0.136383, acc 0.99
2016-05-21T11:16:54.045214: train-step 9099, loss 0.124835, acc 1
2016-05-21T11:17:00.270769: train-step 9100, loss 0.122053, acc 1
2016-05-21T11:17:06.471282: train-step 9101, loss 0.165985, acc 0.98
2016-05-21T11:17:12.654977: train-step 9102, loss 0.138835, acc 1
2016-05-21T11:17:18.904737: train-step 9103, loss 0.157185, acc 0.97
2016-05-21T11:17:25.152264: train-step 9104, loss 0.132374, acc 1
2016-05-21T11:17:31.319267: train-step 9105, loss 0.132859, acc 0.99
2016-05-21T11:17:37.474805: train-step 9106, loss 0.186552, acc 0.96
2016-05-21T11:17:43.715193: train-step 9107, loss 0.124003, acc 0.99
2016-05-21T11:17:49.930245: train-step 9108, loss 0.145023, acc 1
2016-05-21T11:17:56.144430: train-step 9109, loss 0.192952, acc 0.97
2016-05-21T11:18:02.299442: train-step 9110, loss 0.163632, acc 0.98
2016-05-21T11:18:08.529159: train-step 9111, loss 0.13471, acc 0.99
2016-05-21T11:18:14.741553: train-step 9112, loss 0.159025, acc 0.99
2016-05-21T11:18:20.940139: train-step 9113, loss 0.152441, acc 0.99
2016-05-21T11:18:27.146366: train-step 9114, loss 0.18389, acc 0.97
2016-05-21T11:18:33.380348: train-step 9115, loss 0.128199, acc 0.98
2016-05-21T11:18:39.633928: train-step 9116, loss 0.163272, acc 0.97
2016-05-21T11:18:45.845954: train-step 9117, loss 0.157433, acc 0.98
2016-05-21T11:18:52.409989: train-step 9118, loss 0.233026, acc 0.95
2016-05-21T11:18:58.791078: train-step 9119, loss 0.164312, acc 0.99
2016-05-21T11:19:04.962053: train-step 9120, loss 0.17278, acc 0.98
2016-05-21T11:19:11.199208: train-step 9121, loss 0.196941, acc 0.97
2016-05-21T11:19:17.435734: train-step 9122, loss 0.1679, acc 0.98
2016-05-21T11:19:23.707454: train-step 9123, loss 0.172961, acc 0.97
2016-05-21T11:19:29.921045: train-step 9124, loss 0.175479, acc 0.96
2016-05-21T11:19:36.128593: train-step 9125, loss 0.138481, acc 0.99
2016-05-21T11:19:42.319768: train-step 9126, loss 0.165371, acc 0.98
2016-05-21T11:19:48.485591: train-step 9127, loss 0.17303, acc 0.96
2016-05-21T11:19:54.709077: train-step 9128, loss 0.162806, acc 0.98
2016-05-21T11:20:00.979268: train-step 9129, loss 0.157851, acc 0.98
2016-05-21T11:20:07.185747: train-step 9130, loss 0.166917, acc 0.98
2016-05-21T11:20:13.826197: train-step 9131, loss 0.153582, acc 0.98
2016-05-21T11:20:20.163239: train-step 9132, loss 0.139524, acc 0.99
2016-05-21T11:20:26.363987: train-step 9133, loss 0.134948, acc 1
2016-05-21T11:20:32.579994: train-step 9134, loss 0.126735, acc 0.99
2016-05-21T11:20:38.916070: train-step 9135, loss 0.152703, acc 0.99
2016-05-21T11:20:45.291616: train-step 9136, loss 0.14206, acc 0.99
2016-05-21T11:20:51.545478: train-step 9137, loss 0.162293, acc 0.98
2016-05-21T11:20:57.753212: train-step 9138, loss 0.168855, acc 0.97
2016-05-21T11:21:03.997888: train-step 9139, loss 0.171426, acc 0.97
2016-05-21T11:21:10.216965: train-step 9140, loss 0.144263, acc 0.99
2016-05-21T11:21:16.412397: train-step 9141, loss 0.179304, acc 0.97
2016-05-21T11:21:22.586559: train-step 9142, loss 0.149898, acc 0.99
2016-05-21T11:21:28.810303: train-step 9143, loss 0.160864, acc 0.98
2016-05-21T11:21:34.941215: train-step 9144, loss 0.16438, acc 0.96
2016-05-21T11:21:41.133665: train-step 9145, loss 0.133028, acc 0.98
2016-05-21T11:21:47.349970: train-step 9146, loss 0.15501, acc 0.98
2016-05-21T11:21:53.524217: train-step 9147, loss 0.150817, acc 0.97
2016-05-21T11:21:59.724325: train-step 9148, loss 0.168515, acc 0.96
2016-05-21T11:22:05.926762: train-step 9149, loss 0.163454, acc 0.96
2016-05-21T11:22:12.169341: train-step 9150, loss 0.135053, acc 1
2016-05-21T11:22:18.364977: train-step 9151, loss 0.130896, acc 1
2016-05-21T11:22:24.585444: train-step 9152, loss 0.124038, acc 0.99
2016-05-21T11:22:30.844080: train-step 9153, loss 0.133135, acc 1
2016-05-21T11:22:37.016775: train-step 9154, loss 0.161568, acc 0.98
2016-05-21T11:22:43.150772: train-step 9155, loss 0.173578, acc 0.98
2016-05-21T11:22:49.367066: train-step 9156, loss 0.162768, acc 0.97
2016-05-21T11:22:55.617191: train-step 9157, loss 0.160256, acc 0.99
2016-05-21T11:23:01.803445: train-step 9158, loss 0.184236, acc 0.95
2016-05-21T11:23:08.007788: train-step 9159, loss 0.152971, acc 0.99
2016-05-21T11:23:14.212947: train-step 9160, loss 0.168315, acc 0.97
2016-05-21T11:23:20.370786: train-step 9161, loss 0.149246, acc 0.98
2016-05-21T11:23:26.597894: train-step 9162, loss 0.159392, acc 0.98
2016-05-21T11:23:32.823205: train-step 9163, loss 0.178641, acc 0.96
2016-05-21T11:23:39.040224: train-step 9164, loss 0.160616, acc 0.99
2016-05-21T11:23:45.272166: train-step 9165, loss 0.151689, acc 1
2016-05-21T11:23:51.527173: train-step 9166, loss 0.177177, acc 0.97
2016-05-21T11:23:57.734401: train-step 9167, loss 0.121229, acc 0.99
2016-05-21T11:24:03.996566: train-step 9168, loss 0.16028, acc 0.99
2016-05-21T11:24:10.220031: train-step 9169, loss 0.155243, acc 0.99
2016-05-21T11:24:16.428774: train-step 9170, loss 0.165791, acc 0.99
2016-05-21T11:24:22.585410: train-step 9171, loss 0.150208, acc 0.97
2016-05-21T11:24:28.749169: train-step 9172, loss 0.126153, acc 0.99
2016-05-21T11:24:34.926906: train-step 9173, loss 0.183662, acc 0.97
2016-05-21T11:24:41.153361: train-step 9174, loss 0.158329, acc 0.99
2016-05-21T11:24:47.321324: train-step 9175, loss 0.188203, acc 0.96
2016-05-21T11:24:53.545759: train-step 9176, loss 0.179348, acc 0.97
2016-05-21T11:24:59.742699: train-step 9177, loss 0.142104, acc 0.99
2016-05-21T11:25:05.992567: train-step 9178, loss 0.145732, acc 0.99
2016-05-21T11:25:12.196788: train-step 9179, loss 0.148291, acc 0.98
2016-05-21T11:25:18.944813: train-step 9180, loss 0.178343, acc 0.98
2016-05-21T11:25:25.896893: train-step 9181, loss 0.204659, acc 0.95
2016-05-21T11:25:32.380639: train-step 9182, loss 0.192736, acc 0.97
2016-05-21T11:25:38.577080: train-step 9183, loss 0.17356, acc 0.97
2016-05-21T11:25:44.815129: train-step 9184, loss 0.167112, acc 0.98
2016-05-21T11:25:51.066855: train-step 9185, loss 0.140335, acc 1
2016-05-21T11:25:57.229052: train-step 9186, loss 0.134146, acc 1
2016-05-21T11:26:03.586522: train-step 9187, loss 0.171797, acc 0.96
2016-05-21T11:26:09.763243: train-step 9188, loss 0.170654, acc 0.96
2016-05-21T11:26:16.023352: train-step 9189, loss 0.136255, acc 0.98
2016-05-21T11:26:22.242855: train-step 9190, loss 0.168148, acc 0.95
2016-05-21T11:26:28.416548: train-step 9191, loss 0.15019, acc 0.99
2016-05-21T11:26:34.848268: train-step 9192, loss 0.146426, acc 0.98
2016-05-21T11:26:41.314084: train-step 9193, loss 0.216771, acc 0.94
2016-05-21T11:26:47.549515: train-step 9194, loss 0.131348, acc 0.98
2016-05-21T11:26:53.835634: train-step 9195, loss 0.146983, acc 0.98
2016-05-21T11:27:00.131195: train-step 9196, loss 0.156681, acc 0.98
2016-05-21T11:27:06.379363: train-step 9197, loss 0.212271, acc 0.93
2016-05-21T11:27:12.899780: train-step 9198, loss 0.143589, acc 0.99
2016-05-21T11:27:19.278704: train-step 9199, loss 0.153635, acc 0.99
2016-05-21T11:27:25.645190: train-step 9200, loss 0.154896, acc 0.97
2016-05-21T11:27:31.867254: train-step 9201, loss 0.148281, acc 0.99
2016-05-21T11:27:38.064632: train-step 9202, loss 0.152058, acc 0.98
2016-05-21T11:27:44.260664: train-step 9203, loss 0.159605, acc 0.96
2016-05-21T11:27:50.478461: train-step 9204, loss 0.198128, acc 0.95
2016-05-21T11:27:56.686759: train-step 9205, loss 0.183571, acc 0.98
2016-05-21T11:28:02.884824: train-step 9206, loss 0.15724, acc 0.98
2016-05-21T11:28:09.094340: train-step 9207, loss 0.147844, acc 0.97
2016-05-21T11:28:15.529464: train-step 9208, loss 0.167022, acc 0.98
2016-05-21T11:28:21.964430: train-step 9209, loss 0.15474, acc 0.98
2016-05-21T11:28:28.204337: train-step 9210, loss 0.183738, acc 0.96
2016-05-21T11:28:34.434640: train-step 9211, loss 0.135383, acc 1
2016-05-21T11:28:40.629789: train-step 9212, loss 0.173887, acc 0.97
2016-05-21T11:28:46.826987: train-step 9213, loss 0.10703, acc 1
2016-05-21T11:28:53.087740: train-step 9214, loss 0.152534, acc 0.99
2016-05-21T11:28:59.305967: train-step 9215, loss 0.189484, acc 0.98
2016-05-21T11:29:05.500499: train-step 9216, loss 0.129595, acc 0.98
2016-05-21T11:29:11.877020: train-step 9217, loss 0.159526, acc 0.96
2016-05-21T11:29:18.282484: train-step 9218, loss 0.213083, acc 0.96
2016-05-21T11:29:24.539908: train-step 9219, loss 0.170181, acc 0.98
2016-05-21T11:29:30.754447: train-step 9220, loss 0.159924, acc 0.96
2016-05-21T11:29:37.178987: train-step 9221, loss 0.11121, acc 1
2016-05-21T11:29:43.449423: train-step 9222, loss 0.117154, acc 0.99
2016-05-21T11:29:49.731073: train-step 9223, loss 0.196213, acc 0.98
2016-05-21T11:29:55.937322: train-step 9224, loss 0.151048, acc 0.99
2016-05-21T11:30:02.156744: train-step 9225, loss 0.168415, acc 0.97
2016-05-21T11:30:08.359919: train-step 9226, loss 0.131373, acc 0.99
2016-05-21T11:30:14.575440: train-step 9227, loss 0.147585, acc 0.98
2016-05-21T11:30:20.746744: train-step 9228, loss 0.170973, acc 0.96
2016-05-21T11:30:26.974213: train-step 9229, loss 0.167799, acc 0.97
2016-05-21T11:30:33.177136: train-step 9230, loss 0.167275, acc 0.98
2016-05-21T11:30:39.401409: train-step 9231, loss 0.165408, acc 0.99
2016-05-21T11:30:45.609001: train-step 9232, loss 0.175131, acc 0.98
2016-05-21T11:30:51.835829: train-step 9233, loss 0.158855, acc 0.98
2016-05-21T11:30:58.062516: train-step 9234, loss 0.134203, acc 0.99
2016-05-21T11:31:04.292735: train-step 9235, loss 0.133797, acc 0.99
2016-05-21T11:31:10.506203: train-step 9236, loss 0.168449, acc 0.95
2016-05-21T11:31:16.711865: train-step 9237, loss 0.188871, acc 0.94
2016-05-21T11:31:22.881116: train-step 9238, loss 0.146639, acc 0.98
2016-05-21T11:31:29.074375: train-step 9239, loss 0.130358, acc 0.99
2016-05-21T11:31:35.243559: train-step 9240, loss 0.152448, acc 0.98
2016-05-21T11:31:41.450595: train-step 9241, loss 0.16159, acc 0.96
2016-05-21T11:31:47.673596: train-step 9242, loss 0.181273, acc 0.97
2016-05-21T11:31:53.891591: train-step 9243, loss 0.126385, acc 1
2016-05-21T11:32:00.106576: train-step 9244, loss 0.14697, acc 0.97
2016-05-21T11:32:06.337969: train-step 9245, loss 0.165804, acc 0.98
2016-05-21T11:32:12.550580: train-step 9246, loss 0.159333, acc 0.97
2016-05-21T11:32:18.749044: train-step 9247, loss 0.128545, acc 0.99
2016-05-21T11:32:24.953133: train-step 9248, loss 0.125521, acc 0.99
2016-05-21T11:32:31.333303: train-step 9249, loss 0.206183, acc 0.95
2016-05-21T11:32:37.763647: train-step 9250, loss 0.187822, acc 0.94
epoch number is: 37
2016-05-21T11:32:44.157202: train-step 9251, loss 0.151166, acc 0.99
2016-05-21T11:32:50.385884: train-step 9252, loss 0.133738, acc 1
2016-05-21T11:32:56.565923: train-step 9253, loss 0.14634, acc 0.99
2016-05-21T11:33:02.766429: train-step 9254, loss 0.159456, acc 0.98
2016-05-21T11:33:08.902550: train-step 9255, loss 0.120053, acc 1
2016-05-21T11:33:15.040077: train-step 9256, loss 0.158758, acc 0.97
2016-05-21T11:33:21.217987: train-step 9257, loss 0.158452, acc 0.98
2016-05-21T11:33:27.457687: train-step 9258, loss 0.187961, acc 0.96
2016-05-21T11:33:33.711450: train-step 9259, loss 0.138716, acc 0.99
2016-05-21T11:33:39.913431: train-step 9260, loss 0.186818, acc 0.95
2016-05-21T11:33:46.133761: train-step 9261, loss 0.18462, acc 0.98
2016-05-21T11:33:52.353330: train-step 9262, loss 0.160529, acc 0.98
2016-05-21T11:33:58.569737: train-step 9263, loss 0.190676, acc 0.97
2016-05-21T11:34:04.762342: train-step 9264, loss 0.122316, acc 1
2016-05-21T11:34:11.151305: train-step 9265, loss 0.164739, acc 0.98
2016-05-21T11:34:17.329160: train-step 9266, loss 0.161096, acc 0.97
2016-05-21T11:34:23.556887: train-step 9267, loss 0.163905, acc 0.97
2016-05-21T11:34:29.911336: train-step 9268, loss 0.15657, acc 0.98
2016-05-21T11:34:36.435664: train-step 9269, loss 0.141349, acc 0.99
2016-05-21T11:34:42.732423: train-step 9270, loss 0.138443, acc 0.99
2016-05-21T11:34:48.996261: train-step 9271, loss 0.1681, acc 0.98
2016-05-21T11:34:55.560214: train-step 9272, loss 0.144945, acc 0.99
2016-05-21T11:35:01.839076: train-step 9273, loss 0.174733, acc 0.97
2016-05-21T11:35:08.067652: train-step 9274, loss 0.150269, acc 0.99
2016-05-21T11:35:14.677155: train-step 9275, loss 0.164703, acc 0.97
2016-05-21T11:35:21.046795: train-step 9276, loss 0.138858, acc 0.99
2016-05-21T11:35:27.196682: train-step 9277, loss 0.148629, acc 0.98
2016-05-21T11:35:33.423258: train-step 9278, loss 0.132355, acc 1
2016-05-21T11:35:39.760539: train-step 9279, loss 0.139828, acc 0.99
2016-05-21T11:35:45.933767: train-step 9280, loss 0.148838, acc 0.98
2016-05-21T11:35:52.089906: train-step 9281, loss 0.142651, acc 0.99
2016-05-21T11:35:58.318855: train-step 9282, loss 0.181395, acc 0.95
2016-05-21T11:36:04.525806: train-step 9283, loss 0.172511, acc 0.96
2016-05-21T11:36:10.737330: train-step 9284, loss 0.134749, acc 0.98
2016-05-21T11:36:16.954345: train-step 9285, loss 0.159395, acc 0.98
2016-05-21T11:36:23.164304: train-step 9286, loss 0.177699, acc 0.97
2016-05-21T11:36:29.375497: train-step 9287, loss 0.154331, acc 0.97
2016-05-21T11:36:35.581100: train-step 9288, loss 0.140171, acc 0.98
2016-05-21T11:36:41.803945: train-step 9289, loss 0.141789, acc 0.98
2016-05-21T11:36:47.982939: train-step 9290, loss 0.144556, acc 0.97
2016-05-21T11:36:54.135730: train-step 9291, loss 0.162161, acc 0.98
2016-05-21T11:37:00.389474: train-step 9292, loss 0.15515, acc 0.99
2016-05-21T11:37:06.655826: train-step 9293, loss 0.158622, acc 0.97
2016-05-21T11:37:12.901591: train-step 9294, loss 0.1675, acc 0.98
2016-05-21T11:37:19.081540: train-step 9295, loss 0.133006, acc 1
2016-05-21T11:37:25.265758: train-step 9296, loss 0.147201, acc 0.98
2016-05-21T11:37:31.452534: train-step 9297, loss 0.191892, acc 0.96
2016-05-21T11:37:37.821858: train-step 9298, loss 0.136144, acc 1
2016-05-21T11:37:44.105835: train-step 9299, loss 0.158552, acc 0.97
2016-05-21T11:37:50.427971: train-step 9300, loss 0.167571, acc 0.98
2016-05-21T11:37:56.991922: train-step 9301, loss 0.149728, acc 0.99
2016-05-21T11:38:03.208814: train-step 9302, loss 0.199627, acc 0.95
2016-05-21T11:38:09.431602: train-step 9303, loss 0.142693, acc 0.99
2016-05-21T11:38:15.667220: train-step 9304, loss 0.12008, acc 0.99
2016-05-21T11:38:21.881227: train-step 9305, loss 0.138628, acc 1
2016-05-21T11:38:28.097744: train-step 9306, loss 0.190501, acc 0.94
2016-05-21T11:38:34.371057: train-step 9307, loss 0.128134, acc 0.99
2016-05-21T11:38:40.613982: train-step 9308, loss 0.134358, acc 1
2016-05-21T11:38:46.784715: train-step 9309, loss 0.145963, acc 0.97
2016-05-21T11:38:53.019711: train-step 9310, loss 0.121203, acc 1
2016-05-21T11:38:59.242519: train-step 9311, loss 0.181458, acc 0.95
2016-05-21T11:39:05.484570: train-step 9312, loss 0.146862, acc 1
2016-05-21T11:39:11.626903: train-step 9313, loss 0.140006, acc 0.99
2016-05-21T11:39:17.830196: train-step 9314, loss 0.176851, acc 0.97
2016-05-21T11:39:24.058252: train-step 9315, loss 0.167016, acc 0.97
2016-05-21T11:39:30.274015: train-step 9316, loss 0.167622, acc 0.98
2016-05-21T11:39:36.426329: train-step 9317, loss 0.121221, acc 0.99
2016-05-21T11:39:42.605672: train-step 9318, loss 0.143225, acc 1
2016-05-21T11:39:48.763740: train-step 9319, loss 0.170851, acc 0.97
2016-05-21T11:39:54.978884: train-step 9320, loss 0.139035, acc 0.99
2016-05-21T11:40:01.161990: train-step 9321, loss 0.131944, acc 0.99
2016-05-21T11:40:07.398643: train-step 9322, loss 0.153014, acc 0.98
2016-05-21T11:40:13.709865: train-step 9323, loss 0.159555, acc 0.99
2016-05-21T11:40:20.309524: train-step 9324, loss 0.159287, acc 0.97
2016-05-21T11:40:26.611016: train-step 9325, loss 0.139215, acc 0.98
2016-05-21T11:40:32.744264: train-step 9326, loss 0.153539, acc 0.98
2016-05-21T11:40:38.948281: train-step 9327, loss 0.133153, acc 0.99
2016-05-21T11:40:45.206188: train-step 9328, loss 0.141009, acc 0.99
2016-05-21T11:40:51.349770: train-step 9329, loss 0.167305, acc 0.99
2016-05-21T11:40:57.516558: train-step 9330, loss 0.177574, acc 0.95
2016-05-21T11:41:03.875052: train-step 9331, loss 0.171046, acc 0.98
2016-05-21T11:41:10.253672: train-step 9332, loss 0.13055, acc 1
2016-05-21T11:41:16.514505: train-step 9333, loss 0.152839, acc 0.98
2016-05-21T11:41:22.779468: train-step 9334, loss 0.158678, acc 0.98
2016-05-21T11:41:28.994586: train-step 9335, loss 0.133909, acc 0.97
2016-05-21T11:41:35.185997: train-step 9336, loss 0.157975, acc 0.98
2016-05-21T11:41:41.419105: train-step 9337, loss 0.130793, acc 0.99
2016-05-21T11:41:47.593129: train-step 9338, loss 0.157698, acc 0.97
2016-05-21T11:41:53.755458: train-step 9339, loss 0.164453, acc 0.98
2016-05-21T11:42:00.012991: train-step 9340, loss 0.146925, acc 0.99
2016-05-21T11:42:06.282752: train-step 9341, loss 0.16462, acc 0.97
2016-05-21T11:42:12.449018: train-step 9342, loss 0.161074, acc 0.96
2016-05-21T11:42:18.662020: train-step 9343, loss 0.14335, acc 1
2016-05-21T11:42:24.860830: train-step 9344, loss 0.135175, acc 0.99
2016-05-21T11:42:31.020796: train-step 9345, loss 0.137795, acc 1
2016-05-21T11:42:37.220941: train-step 9346, loss 0.150717, acc 0.99
2016-05-21T11:42:43.441203: train-step 9347, loss 0.153751, acc 1
2016-05-21T11:42:49.641898: train-step 9348, loss 0.144401, acc 0.98
2016-05-21T11:42:55.809101: train-step 9349, loss 0.195992, acc 0.95
2016-05-21T11:43:02.006663: train-step 9350, loss 0.142751, acc 1
2016-05-21T11:43:08.260011: train-step 9351, loss 0.130107, acc 0.99
2016-05-21T11:43:14.424869: train-step 9352, loss 0.17574, acc 0.97
2016-05-21T11:43:20.619905: train-step 9353, loss 0.158896, acc 0.96
2016-05-21T11:43:26.817400: train-step 9354, loss 0.176706, acc 0.98
2016-05-21T11:43:33.039544: train-step 9355, loss 0.112284, acc 0.99
2016-05-21T11:43:39.266531: train-step 9356, loss 0.119676, acc 1
2016-05-21T11:43:45.463549: train-step 9357, loss 0.119946, acc 1
2016-05-21T11:43:51.619794: train-step 9358, loss 0.163174, acc 0.97
2016-05-21T11:43:57.924581: train-step 9359, loss 0.132991, acc 0.99
2016-05-21T11:44:04.277759: train-step 9360, loss 0.14119, acc 0.98
2016-05-21T11:44:10.609247: train-step 9361, loss 0.143164, acc 0.98
2016-05-21T11:44:16.989065: train-step 9362, loss 0.103459, acc 1
2016-05-21T11:44:23.168104: train-step 9363, loss 0.168365, acc 0.97
2016-05-21T11:44:29.738788: train-step 9364, loss 0.129301, acc 1
2016-05-21T11:44:36.041345: train-step 9365, loss 0.134086, acc 1
2016-05-21T11:44:42.214827: train-step 9366, loss 0.144345, acc 0.99
2016-05-21T11:44:48.428996: train-step 9367, loss 0.170509, acc 0.99
2016-05-21T11:44:54.682582: train-step 9368, loss 0.146987, acc 1
2016-05-21T11:45:00.926538: train-step 9369, loss 0.176711, acc 0.96
2016-05-21T11:45:07.238106: train-step 9370, loss 0.130938, acc 0.98
2016-05-21T11:45:13.588757: train-step 9371, loss 0.133919, acc 0.98
2016-05-21T11:45:19.796487: train-step 9372, loss 0.158201, acc 0.99
2016-05-21T11:45:26.378903: train-step 9373, loss 0.130112, acc 0.99
2016-05-21T11:45:32.687828: train-step 9374, loss 0.181189, acc 0.96
2016-05-21T11:45:39.086074: train-step 9375, loss 0.136578, acc 0.99
2016-05-21T11:45:45.349592: train-step 9376, loss 0.140028, acc 0.98
2016-05-21T11:45:51.507944: train-step 9377, loss 0.126403, acc 0.99
2016-05-21T11:45:57.882806: train-step 9378, loss 0.154464, acc 0.98
2016-05-21T11:46:04.173530: train-step 9379, loss 0.143354, acc 0.99
2016-05-21T11:46:10.376053: train-step 9380, loss 0.150759, acc 0.98
2016-05-21T11:46:16.535469: train-step 9381, loss 0.141955, acc 1
2016-05-21T11:46:22.705877: train-step 9382, loss 0.15665, acc 0.98
2016-05-21T11:46:28.898051: train-step 9383, loss 0.160652, acc 0.97
2016-05-21T11:46:35.127767: train-step 9384, loss 0.146973, acc 0.98
2016-05-21T11:46:41.314125: train-step 9385, loss 0.205105, acc 0.95
2016-05-21T11:46:47.521097: train-step 9386, loss 0.139342, acc 1
2016-05-21T11:46:53.733279: train-step 9387, loss 0.124788, acc 0.99
2016-05-21T11:46:59.924633: train-step 9388, loss 0.202449, acc 0.98
2016-05-21T11:47:06.093833: train-step 9389, loss 0.144953, acc 0.99
2016-05-21T11:47:12.293304: train-step 9390, loss 0.14719, acc 0.98
2016-05-21T11:47:18.513335: train-step 9391, loss 0.161355, acc 0.97
2016-05-21T11:47:24.673701: train-step 9392, loss 0.158251, acc 0.99
2016-05-21T11:47:30.857650: train-step 9393, loss 0.132719, acc 0.99
2016-05-21T11:47:37.059635: train-step 9394, loss 0.141848, acc 0.97
2016-05-21T11:47:43.217091: train-step 9395, loss 0.151647, acc 0.98
2016-05-21T11:47:49.462470: train-step 9396, loss 0.205639, acc 0.96
2016-05-21T11:47:55.683078: train-step 9397, loss 0.135966, acc 0.99
2016-05-21T11:48:01.892953: train-step 9398, loss 0.158635, acc 0.97
2016-05-21T11:48:08.063018: train-step 9399, loss 0.194283, acc 0.96
2016-05-21T11:48:14.244562: train-step 9400, loss 0.133013, acc 0.98
2016-05-21T11:48:20.438110: train-step 9401, loss 0.144494, acc 0.99
2016-05-21T11:48:26.690130: train-step 9402, loss 0.147589, acc 0.99
2016-05-21T11:48:33.025062: train-step 9403, loss 0.151925, acc 0.98
2016-05-21T11:48:39.255314: train-step 9404, loss 0.140465, acc 0.99
2016-05-21T11:48:45.514689: train-step 9405, loss 0.135519, acc 0.99
2016-05-21T11:48:51.749519: train-step 9406, loss 0.150467, acc 0.98
2016-05-21T11:48:57.974207: train-step 9407, loss 0.134398, acc 0.99
2016-05-21T11:49:04.232155: train-step 9408, loss 0.151885, acc 0.98
2016-05-21T11:49:10.463201: train-step 9409, loss 0.1381, acc 0.99
2016-05-21T11:49:16.614671: train-step 9410, loss 0.153533, acc 0.98
2016-05-21T11:49:22.811215: train-step 9411, loss 0.124639, acc 1
2016-05-21T11:49:29.019059: train-step 9412, loss 0.137148, acc 0.98
2016-05-21T11:49:35.242472: train-step 9413, loss 0.117081, acc 1
2016-05-21T11:49:41.484004: train-step 9414, loss 0.144514, acc 0.97
2016-05-21T11:49:47.703902: train-step 9415, loss 0.150922, acc 0.98
2016-05-21T11:49:53.922651: train-step 9416, loss 0.153155, acc 0.98
2016-05-21T11:50:00.149997: train-step 9417, loss 0.207761, acc 0.96
2016-05-21T11:50:06.648240: train-step 9418, loss 0.131297, acc 1
2016-05-21T11:50:13.006026: train-step 9419, loss 0.123968, acc 1
2016-05-21T11:50:19.401834: train-step 9420, loss 0.123261, acc 1
2016-05-21T11:50:25.653070: train-step 9421, loss 0.160037, acc 0.97
2016-05-21T11:50:31.885901: train-step 9422, loss 0.156943, acc 0.98
2016-05-21T11:50:38.105846: train-step 9423, loss 0.180823, acc 0.97
2016-05-21T11:50:44.334747: train-step 9424, loss 0.134282, acc 0.99
2016-05-21T11:50:50.547814: train-step 9425, loss 0.162986, acc 0.99
2016-05-21T11:50:56.789868: train-step 9426, loss 0.157142, acc 0.98
2016-05-21T11:51:03.009199: train-step 9427, loss 0.194001, acc 0.98
2016-05-21T11:51:09.216178: train-step 9428, loss 0.147347, acc 0.99
2016-05-21T11:51:15.617344: train-step 9429, loss 0.166576, acc 0.97
2016-05-21T11:51:21.830440: train-step 9430, loss 0.16569, acc 0.98
2016-05-21T11:51:28.048762: train-step 9431, loss 0.161955, acc 0.98
2016-05-21T11:51:34.295668: train-step 9432, loss 0.1313, acc 0.99
2016-05-21T11:51:40.479114: train-step 9433, loss 0.157561, acc 0.98
2016-05-21T11:51:46.643702: train-step 9434, loss 0.145577, acc 0.99
2016-05-21T11:51:52.857051: train-step 9435, loss 0.170782, acc 0.97
2016-05-21T11:51:59.084247: train-step 9436, loss 0.140002, acc 1
2016-05-21T11:52:05.328161: train-step 9437, loss 0.14397, acc 1
2016-05-21T11:52:11.558650: train-step 9438, loss 0.14055, acc 0.99
2016-05-21T11:52:17.903353: train-step 9439, loss 0.140157, acc 0.99
2016-05-21T11:52:24.146419: train-step 9440, loss 0.143905, acc 0.97
2016-05-21T11:52:30.331680: train-step 9441, loss 0.175372, acc 0.97
2016-05-21T11:52:36.572895: train-step 9442, loss 0.145519, acc 0.97
2016-05-21T11:52:42.807081: train-step 9443, loss 0.15679, acc 0.97
2016-05-21T11:52:49.014211: train-step 9444, loss 0.156028, acc 0.98
2016-05-21T11:52:55.174981: train-step 9445, loss 0.164025, acc 0.96
2016-05-21T11:53:01.425961: train-step 9446, loss 0.151375, acc 0.97
2016-05-21T11:53:07.643890: train-step 9447, loss 0.133728, acc 1
2016-05-21T11:53:14.040834: train-step 9448, loss 0.159441, acc 1
2016-05-21T11:53:20.409738: train-step 9449, loss 0.167226, acc 0.94
2016-05-21T11:53:26.722774: train-step 9450, loss 0.155365, acc 0.99
2016-05-21T11:53:32.876502: train-step 9451, loss 0.141477, acc 0.98
2016-05-21T11:53:39.064814: train-step 9452, loss 0.188663, acc 0.97
2016-05-21T11:53:45.402798: train-step 9453, loss 0.176065, acc 0.96
2016-05-21T11:53:51.811329: train-step 9454, loss 0.185503, acc 0.97
2016-05-21T11:53:58.081350: train-step 9455, loss 0.150722, acc 0.99
2016-05-21T11:54:04.347902: train-step 9456, loss 0.125639, acc 0.99
2016-05-21T11:54:10.686632: train-step 9457, loss 0.126371, acc 1
2016-05-21T11:54:17.265667: train-step 9458, loss 0.149113, acc 0.98
2016-05-21T11:54:23.901954: train-step 9459, loss 0.138198, acc 0.97
2016-05-21T11:54:30.275874: train-step 9460, loss 0.161952, acc 0.97
2016-05-21T11:54:36.457233: train-step 9461, loss 0.168763, acc 0.97
2016-05-21T11:54:42.683484: train-step 9462, loss 0.160458, acc 0.97
2016-05-21T11:54:49.079678: train-step 9463, loss 0.123299, acc 0.99
2016-05-21T11:54:55.422480: train-step 9464, loss 0.126132, acc 0.99
2016-05-21T11:55:01.618394: train-step 9465, loss 0.158701, acc 0.98
2016-05-21T11:55:07.829862: train-step 9466, loss 0.135445, acc 0.99
2016-05-21T11:55:13.985808: train-step 9467, loss 0.165072, acc 0.97
2016-05-21T11:55:20.202090: train-step 9468, loss 0.146701, acc 0.97
2016-05-21T11:55:26.521946: train-step 9469, loss 0.150442, acc 1
2016-05-21T11:55:33.064078: train-step 9470, loss 0.178358, acc 1
2016-05-21T11:55:39.348951: train-step 9471, loss 0.16682, acc 0.98
2016-05-21T11:55:45.503323: train-step 9472, loss 0.159762, acc 0.96
2016-05-21T11:55:51.664362: train-step 9473, loss 0.159753, acc 0.96
2016-05-21T11:55:57.902191: train-step 9474, loss 0.175722, acc 0.97
2016-05-21T11:56:04.103817: train-step 9475, loss 0.173386, acc 0.97
2016-05-21T11:56:10.328082: train-step 9476, loss 0.172026, acc 0.98
2016-05-21T11:56:16.685777: train-step 9477, loss 0.148775, acc 0.99
2016-05-21T11:56:22.919130: train-step 9478, loss 0.148781, acc 0.99
2016-05-21T11:56:29.189268: train-step 9479, loss 0.179354, acc 0.97
2016-05-21T11:56:35.426043: train-step 9480, loss 0.148994, acc 0.97
2016-05-21T11:56:41.630722: train-step 9481, loss 0.120385, acc 1
2016-05-21T11:56:47.800401: train-step 9482, loss 0.12618, acc 1
2016-05-21T11:56:54.082580: train-step 9483, loss 0.145052, acc 0.99
2016-05-21T11:57:00.241277: train-step 9484, loss 0.131229, acc 0.98
2016-05-21T11:57:06.420705: train-step 9485, loss 0.157694, acc 0.99
2016-05-21T11:57:12.626159: train-step 9486, loss 0.164323, acc 0.98
2016-05-21T11:57:18.877671: train-step 9487, loss 0.1516, acc 0.99
2016-05-21T11:57:25.120466: train-step 9488, loss 0.171615, acc 0.96
2016-05-21T11:57:31.360849: train-step 9489, loss 0.140359, acc 1
2016-05-21T11:57:37.531777: train-step 9490, loss 0.131662, acc 0.98
2016-05-21T11:57:43.758102: train-step 9491, loss 0.163025, acc 0.97
2016-05-21T11:57:49.902587: train-step 9492, loss 0.176373, acc 0.96
2016-05-21T11:57:56.148589: train-step 9493, loss 0.134611, acc 0.99
2016-05-21T11:58:02.301226: train-step 9494, loss 0.164063, acc 0.98
2016-05-21T11:58:08.538224: train-step 9495, loss 0.150392, acc 0.99
2016-05-21T11:58:14.729455: train-step 9496, loss 0.172425, acc 0.95
2016-05-21T11:58:20.927787: train-step 9497, loss 0.189427, acc 0.94
2016-05-21T11:58:27.115748: train-step 9498, loss 0.145119, acc 0.99
2016-05-21T11:58:33.650606: train-step 9499, loss 0.179497, acc 0.97
2016-05-21T11:58:40.094435: train-step 9500, loss 0.15336, acc 0.98
epoch number is: 38
2016-05-21T11:58:46.611755: train-step 9501, loss 0.193484, acc 0.97
2016-05-21T11:58:52.772783: train-step 9502, loss 0.154251, acc 0.96
2016-05-21T11:58:59.009363: train-step 9503, loss 0.150338, acc 0.98
2016-05-21T11:59:05.225105: train-step 9504, loss 0.158011, acc 0.98
2016-05-21T11:59:11.415521: train-step 9505, loss 0.156427, acc 0.98
2016-05-21T11:59:17.626453: train-step 9506, loss 0.154368, acc 0.99
2016-05-21T11:59:23.842892: train-step 9507, loss 0.161222, acc 0.99
2016-05-21T11:59:30.043880: train-step 9508, loss 0.205927, acc 0.93
2016-05-21T11:59:36.255624: train-step 9509, loss 0.171016, acc 0.98
2016-05-21T11:59:42.413028: train-step 9510, loss 0.149962, acc 1
2016-05-21T11:59:48.582235: train-step 9511, loss 0.115821, acc 1
2016-05-21T11:59:54.824878: train-step 9512, loss 0.166061, acc 0.97
2016-05-21T12:00:01.015814: train-step 9513, loss 0.178104, acc 0.97
2016-05-21T12:00:07.193069: train-step 9514, loss 0.161399, acc 0.98
2016-05-21T12:00:13.342842: train-step 9515, loss 0.13129, acc 0.98
2016-05-21T12:00:19.563540: train-step 9516, loss 0.147881, acc 0.97
2016-05-21T12:00:25.767460: train-step 9517, loss 0.162924, acc 0.97
2016-05-21T12:00:31.925029: train-step 9518, loss 0.159676, acc 0.97
2016-05-21T12:00:38.165283: train-step 9519, loss 0.126753, acc 0.99
2016-05-21T12:00:44.369170: train-step 9520, loss 0.149995, acc 0.98
2016-05-21T12:00:50.735731: train-step 9521, loss 0.149589, acc 0.99
2016-05-21T12:00:57.133367: train-step 9522, loss 0.130839, acc 0.99
2016-05-21T12:01:03.426940: train-step 9523, loss 0.137974, acc 0.99
2016-05-21T12:01:09.573685: train-step 9524, loss 0.138735, acc 1
2016-05-21T12:01:15.748857: train-step 9525, loss 0.141809, acc 0.98
2016-05-21T12:01:21.912495: train-step 9526, loss 0.132649, acc 0.99
2016-05-21T12:01:28.115388: train-step 9527, loss 0.125739, acc 0.99
2016-05-21T12:01:34.351120: train-step 9528, loss 0.137237, acc 1
2016-05-21T12:01:40.615942: train-step 9529, loss 0.152101, acc 0.98
2016-05-21T12:01:46.961650: train-step 9530, loss 0.152811, acc 0.98
2016-05-21T12:01:53.229605: train-step 9531, loss 0.149938, acc 0.99
2016-05-21T12:01:59.491426: train-step 9532, loss 0.14524, acc 0.97
2016-05-21T12:02:05.705241: train-step 9533, loss 0.115028, acc 1
2016-05-21T12:02:11.929409: train-step 9534, loss 0.133183, acc 0.99
2016-05-21T12:02:18.157190: train-step 9535, loss 0.132842, acc 0.99
2016-05-21T12:02:24.313955: train-step 9536, loss 0.176822, acc 0.98
2016-05-21T12:02:30.666241: train-step 9537, loss 0.179208, acc 0.97
2016-05-21T12:02:36.919038: train-step 9538, loss 0.15679, acc 0.99
2016-05-21T12:02:43.182991: train-step 9539, loss 0.160554, acc 0.98
2016-05-21T12:02:49.423079: train-step 9540, loss 0.211459, acc 0.96
2016-05-21T12:02:55.982010: train-step 9541, loss 0.168059, acc 0.97
2016-05-21T12:03:02.302233: train-step 9542, loss 0.161708, acc 0.98
2016-05-21T12:03:08.459061: train-step 9543, loss 0.141808, acc 0.97
2016-05-21T12:03:14.717089: train-step 9544, loss 0.179709, acc 0.96
2016-05-21T12:03:20.925051: train-step 9545, loss 0.147936, acc 1
2016-05-21T12:03:27.152770: train-step 9546, loss 0.163488, acc 0.97
2016-05-21T12:03:33.342020: train-step 9547, loss 0.146704, acc 0.98
2016-05-21T12:03:39.569576: train-step 9548, loss 0.125262, acc 1
2016-05-21T12:03:45.860511: train-step 9549, loss 0.175997, acc 0.97
2016-05-21T12:03:52.504892: train-step 9550, loss 0.132648, acc 0.98
2016-05-21T12:03:58.841558: train-step 9551, loss 0.1187, acc 1
2016-05-21T12:04:05.020017: train-step 9552, loss 0.164041, acc 0.98
2016-05-21T12:04:11.200884: train-step 9553, loss 0.156808, acc 0.99
2016-05-21T12:04:17.504486: train-step 9554, loss 0.138139, acc 0.99
2016-05-21T12:04:23.906824: train-step 9555, loss 0.160909, acc 0.97
2016-05-21T12:04:30.172087: train-step 9556, loss 0.135678, acc 1
2016-05-21T12:04:36.374423: train-step 9557, loss 0.136091, acc 0.99
2016-05-21T12:04:42.676969: train-step 9558, loss 0.13959, acc 0.99
2016-05-21T12:04:48.910813: train-step 9559, loss 0.16435, acc 0.98
2016-05-21T12:04:55.212908: train-step 9560, loss 0.141745, acc 0.98
2016-05-21T12:05:01.451310: train-step 9561, loss 0.18834, acc 0.96
2016-05-21T12:05:07.648259: train-step 9562, loss 0.157988, acc 0.98
2016-05-21T12:05:13.876781: train-step 9563, loss 0.134328, acc 0.99
2016-05-21T12:05:20.092318: train-step 9564, loss 0.161425, acc 0.99
2016-05-21T12:05:26.282794: train-step 9565, loss 0.147286, acc 0.98
2016-05-21T12:05:32.502826: train-step 9566, loss 0.18032, acc 0.98
2016-05-21T12:05:38.673931: train-step 9567, loss 0.146311, acc 0.98
2016-05-21T12:05:44.911236: train-step 9568, loss 0.169146, acc 0.98
2016-05-21T12:05:51.096798: train-step 9569, loss 0.126111, acc 1
2016-05-21T12:05:57.305440: train-step 9570, loss 0.128206, acc 0.98
2016-05-21T12:06:03.511059: train-step 9571, loss 0.143392, acc 0.99
2016-05-21T12:06:09.725341: train-step 9572, loss 0.158549, acc 0.97
2016-05-21T12:06:15.969526: train-step 9573, loss 0.166473, acc 0.99
2016-05-21T12:06:22.275419: train-step 9574, loss 0.131239, acc 1
2016-05-21T12:06:28.488666: train-step 9575, loss 0.155763, acc 0.96
2016-05-21T12:06:34.647460: train-step 9576, loss 0.131957, acc 1
2016-05-21T12:06:41.019892: train-step 9577, loss 0.151041, acc 0.97
2016-05-21T12:06:47.415483: train-step 9578, loss 0.148598, acc 1
2016-05-21T12:06:53.660232: train-step 9579, loss 0.16566, acc 0.98
2016-05-21T12:06:59.941621: train-step 9580, loss 0.143204, acc 0.98
2016-05-21T12:07:06.199402: train-step 9581, loss 0.154007, acc 0.98
2016-05-21T12:07:12.422874: train-step 9582, loss 0.129833, acc 0.99
2016-05-21T12:07:18.983649: train-step 9583, loss 0.152803, acc 0.99
2016-05-21T12:07:25.395307: train-step 9584, loss 0.160813, acc 0.98
2016-05-21T12:07:31.645118: train-step 9585, loss 0.159488, acc 0.99
2016-05-21T12:07:37.864725: train-step 9586, loss 0.140966, acc 0.98
2016-05-21T12:07:44.108426: train-step 9587, loss 0.17171, acc 0.97
2016-05-21T12:07:50.353705: train-step 9588, loss 0.183662, acc 0.97
2016-05-21T12:07:56.552923: train-step 9589, loss 0.172608, acc 0.98
2016-05-21T12:08:02.730703: train-step 9590, loss 0.104837, acc 1
2016-05-21T12:08:08.938096: train-step 9591, loss 0.155782, acc 0.99
2016-05-21T12:08:15.142714: train-step 9592, loss 0.147678, acc 0.98
2016-05-21T12:08:21.695839: train-step 9593, loss 0.141236, acc 0.98
2016-05-21T12:08:28.393235: train-step 9594, loss 0.133492, acc 0.99
2016-05-21T12:08:34.723181: train-step 9595, loss 0.145586, acc 0.98
2016-05-21T12:08:40.944014: train-step 9596, loss 0.175712, acc 0.96
2016-05-21T12:08:47.123697: train-step 9597, loss 0.145867, acc 0.98
2016-05-21T12:08:53.342309: train-step 9598, loss 0.181098, acc 0.96
2016-05-21T12:08:59.552297: train-step 9599, loss 0.132119, acc 1
2016-05-21T12:09:05.716440: train-step 9600, loss 0.113673, acc 1
2016-05-21T12:09:11.958736: train-step 9601, loss 0.138241, acc 0.99
2016-05-21T12:09:18.182262: train-step 9602, loss 0.126437, acc 1
2016-05-21T12:09:24.397805: train-step 9603, loss 0.139237, acc 1
2016-05-21T12:09:30.592545: train-step 9604, loss 0.153403, acc 0.98
2016-05-21T12:09:36.817772: train-step 9605, loss 0.12895, acc 0.99
2016-05-21T12:09:43.155383: train-step 9606, loss 0.160888, acc 0.96
2016-05-21T12:09:49.424110: train-step 9607, loss 0.198108, acc 0.97
2016-05-21T12:09:55.759746: train-step 9608, loss 0.170112, acc 0.98
2016-05-21T12:10:02.311360: train-step 9609, loss 0.162354, acc 0.99
2016-05-21T12:10:08.909067: train-step 9610, loss 0.122981, acc 0.99
2016-05-21T12:10:15.234263: train-step 9611, loss 0.144377, acc 0.99
2016-05-21T12:10:21.430330: train-step 9612, loss 0.153311, acc 0.98
2016-05-21T12:10:27.641991: train-step 9613, loss 0.107817, acc 1
2016-05-21T12:10:33.873177: train-step 9614, loss 0.155559, acc 0.98
2016-05-21T12:10:40.076647: train-step 9615, loss 0.130738, acc 1
2016-05-21T12:10:46.221811: train-step 9616, loss 0.143459, acc 1
2016-05-21T12:10:52.495279: train-step 9617, loss 0.148814, acc 0.99
2016-05-21T12:10:58.867995: train-step 9618, loss 0.153407, acc 0.98
2016-05-21T12:11:05.020377: train-step 9619, loss 0.134006, acc 1
2016-05-21T12:11:11.205371: train-step 9620, loss 0.199515, acc 0.97
2016-05-21T12:11:17.391623: train-step 9621, loss 0.127988, acc 1
2016-05-21T12:11:23.588915: train-step 9622, loss 0.186151, acc 0.95
2016-05-21T12:11:29.803256: train-step 9623, loss 0.143595, acc 0.99
2016-05-21T12:11:36.029428: train-step 9624, loss 0.196047, acc 0.96
2016-05-21T12:11:42.181263: train-step 9625, loss 0.149416, acc 0.98
2016-05-21T12:11:48.364796: train-step 9626, loss 0.156156, acc 0.98
2016-05-21T12:11:54.546526: train-step 9627, loss 0.152757, acc 0.98
2016-05-21T12:12:00.736118: train-step 9628, loss 0.145598, acc 0.99
2016-05-21T12:12:06.907167: train-step 9629, loss 0.174931, acc 0.97
2016-05-21T12:12:13.101784: train-step 9630, loss 0.158497, acc 0.97
2016-05-21T12:12:19.241797: train-step 9631, loss 0.154324, acc 0.99
2016-05-21T12:12:25.409112: train-step 9632, loss 0.118008, acc 0.99
2016-05-21T12:12:31.599667: train-step 9633, loss 0.124754, acc 1
2016-05-21T12:12:37.800965: train-step 9634, loss 0.126796, acc 0.98
2016-05-21T12:12:43.992735: train-step 9635, loss 0.152094, acc 0.99
2016-05-21T12:12:50.186840: train-step 9636, loss 0.153265, acc 0.98
2016-05-21T12:12:56.426599: train-step 9637, loss 0.126582, acc 0.99
2016-05-21T12:13:02.649114: train-step 9638, loss 0.152229, acc 0.99
2016-05-21T12:13:08.806046: train-step 9639, loss 0.117187, acc 1
2016-05-21T12:13:15.013931: train-step 9640, loss 0.157807, acc 0.98
2016-05-21T12:13:21.223539: train-step 9641, loss 0.146617, acc 0.99
2016-05-21T12:13:27.426804: train-step 9642, loss 0.150776, acc 0.96
2016-05-21T12:13:33.669031: train-step 9643, loss 0.154059, acc 0.97
2016-05-21T12:13:39.864945: train-step 9644, loss 0.138288, acc 0.99
2016-05-21T12:13:46.088059: train-step 9645, loss 0.147171, acc 0.97
2016-05-21T12:13:52.272107: train-step 9646, loss 0.143725, acc 1
2016-05-21T12:13:58.509424: train-step 9647, loss 0.141935, acc 0.99
2016-05-21T12:14:04.699247: train-step 9648, loss 0.135722, acc 0.99
2016-05-21T12:14:10.917415: train-step 9649, loss 0.178457, acc 0.97
2016-05-21T12:14:17.097384: train-step 9650, loss 0.124861, acc 1
2016-05-21T12:14:23.309435: train-step 9651, loss 0.143738, acc 0.98
2016-05-21T12:14:29.525949: train-step 9652, loss 0.15074, acc 1
2016-05-21T12:14:35.722553: train-step 9653, loss 0.145407, acc 0.99
2016-05-21T12:14:41.880543: train-step 9654, loss 0.195116, acc 0.96
2016-05-21T12:14:48.065340: train-step 9655, loss 0.133403, acc 0.98
2016-05-21T12:14:54.304712: train-step 9656, loss 0.128086, acc 1
2016-05-21T12:15:01.140977: train-step 9657, loss 0.162521, acc 0.95
2016-05-21T12:15:07.669491: train-step 9658, loss 0.136631, acc 0.98
2016-05-21T12:15:13.956107: train-step 9659, loss 0.161386, acc 0.98
2016-05-21T12:15:20.164302: train-step 9660, loss 0.16383, acc 0.97
2016-05-21T12:15:26.394000: train-step 9661, loss 0.133509, acc 0.98
2016-05-21T12:15:32.608892: train-step 9662, loss 0.150747, acc 0.97
2016-05-21T12:15:38.798601: train-step 9663, loss 0.14936, acc 0.98
2016-05-21T12:15:45.048179: train-step 9664, loss 0.134139, acc 0.99
2016-05-21T12:15:51.225032: train-step 9665, loss 0.144424, acc 0.99
2016-05-21T12:15:57.419614: train-step 9666, loss 0.146383, acc 0.99
2016-05-21T12:16:03.943807: train-step 9667, loss 0.152533, acc 0.99
2016-05-21T12:16:10.310327: train-step 9668, loss 0.158122, acc 0.97
2016-05-21T12:16:16.730553: train-step 9669, loss 0.17043, acc 0.99
2016-05-21T12:16:22.987471: train-step 9670, loss 0.137156, acc 0.98
2016-05-21T12:16:29.211047: train-step 9671, loss 0.149623, acc 0.99
2016-05-21T12:16:35.446866: train-step 9672, loss 0.155995, acc 0.99
2016-05-21T12:16:41.705102: train-step 9673, loss 0.159645, acc 0.98
2016-05-21T12:16:47.900529: train-step 9674, loss 0.199888, acc 0.94
2016-05-21T12:16:54.116839: train-step 9675, loss 0.155244, acc 1
2016-05-21T12:17:00.344257: train-step 9676, loss 0.145262, acc 0.98
2016-05-21T12:17:06.527029: train-step 9677, loss 0.180247, acc 0.96
2016-05-21T12:17:12.944935: train-step 9678, loss 0.142818, acc 0.97
2016-05-21T12:17:19.335365: train-step 9679, loss 0.153789, acc 0.98
2016-05-21T12:17:25.578558: train-step 9680, loss 0.145324, acc 0.98
2016-05-21T12:17:31.831150: train-step 9681, loss 0.142457, acc 0.99
2016-05-21T12:17:38.070238: train-step 9682, loss 0.174536, acc 0.97
2016-05-21T12:17:44.316566: train-step 9683, loss 0.152235, acc 0.99
2016-05-21T12:17:50.659443: train-step 9684, loss 0.164374, acc 0.96
2016-05-21T12:17:57.049057: train-step 9685, loss 0.135925, acc 0.99
2016-05-21T12:18:03.303642: train-step 9686, loss 0.128338, acc 1
2016-05-21T12:18:09.545004: train-step 9687, loss 0.131054, acc 0.99
2016-05-21T12:18:15.875505: train-step 9688, loss 0.157889, acc 0.98
2016-05-21T12:18:22.105108: train-step 9689, loss 0.138507, acc 0.99
2016-05-21T12:18:28.315831: train-step 9690, loss 0.129556, acc 0.98
2016-05-21T12:18:34.520670: train-step 9691, loss 0.129848, acc 0.98
2016-05-21T12:18:40.793771: train-step 9692, loss 0.137294, acc 0.98
2016-05-21T12:18:47.150881: train-step 9693, loss 0.198277, acc 0.96
2016-05-21T12:18:53.396056: train-step 9694, loss 0.157219, acc 0.97
2016-05-21T12:18:59.627987: train-step 9695, loss 0.172126, acc 0.97
2016-05-21T12:19:05.791088: train-step 9696, loss 0.173485, acc 0.99
2016-05-21T12:19:12.154478: train-step 9697, loss 0.145864, acc 0.98
2016-05-21T12:19:18.392745: train-step 9698, loss 0.158408, acc 0.97
2016-05-21T12:19:24.655376: train-step 9699, loss 0.150116, acc 0.99
2016-05-21T12:19:30.894209: train-step 9700, loss 0.142006, acc 0.97
2016-05-21T12:19:37.111544: train-step 9701, loss 0.145339, acc 0.99
2016-05-21T12:19:43.350894: train-step 9702, loss 0.152926, acc 0.97
2016-05-21T12:19:49.567941: train-step 9703, loss 0.206112, acc 0.95
2016-05-21T12:19:55.879134: train-step 9704, loss 0.123676, acc 1
2016-05-21T12:20:02.187809: train-step 9705, loss 0.178873, acc 0.96
2016-05-21T12:20:08.453522: train-step 9706, loss 0.135905, acc 1
2016-05-21T12:20:14.656301: train-step 9707, loss 0.15183, acc 0.98
2016-05-21T12:20:20.810750: train-step 9708, loss 0.17999, acc 0.97
2016-05-21T12:20:27.055844: train-step 9709, loss 0.165459, acc 0.98
2016-05-21T12:20:33.233417: train-step 9710, loss 0.133069, acc 0.98
2016-05-21T12:20:39.384503: train-step 9711, loss 0.124797, acc 1
2016-05-21T12:20:45.584264: train-step 9712, loss 0.146905, acc 0.99
2016-05-21T12:20:51.812977: train-step 9713, loss 0.13281, acc 0.97
2016-05-21T12:20:58.049335: train-step 9714, loss 0.222471, acc 0.96
2016-05-21T12:21:04.260569: train-step 9715, loss 0.148131, acc 0.97
2016-05-21T12:21:10.459150: train-step 9716, loss 0.161936, acc 0.99
2016-05-21T12:21:16.662287: train-step 9717, loss 0.149083, acc 0.98
2016-05-21T12:21:22.872247: train-step 9718, loss 0.183006, acc 0.95
2016-05-21T12:21:29.057466: train-step 9719, loss 0.162647, acc 0.98
2016-05-21T12:21:35.272723: train-step 9720, loss 0.157971, acc 0.98
2016-05-21T12:21:41.447366: train-step 9721, loss 0.147354, acc 0.98
2016-05-21T12:21:47.625678: train-step 9722, loss 0.162393, acc 0.98
2016-05-21T12:21:53.881837: train-step 9723, loss 0.134179, acc 0.99
2016-05-21T12:22:00.081187: train-step 9724, loss 0.1815, acc 0.97
2016-05-21T12:22:06.299008: train-step 9725, loss 0.11234, acc 0.99
2016-05-21T12:22:12.499506: train-step 9726, loss 0.175925, acc 0.96
2016-05-21T12:22:18.746622: train-step 9727, loss 0.144469, acc 0.99
2016-05-21T12:22:24.909542: train-step 9728, loss 0.152553, acc 0.99
2016-05-21T12:22:31.111865: train-step 9729, loss 0.152164, acc 0.98
2016-05-21T12:22:37.272861: train-step 9730, loss 0.150119, acc 0.97
2016-05-21T12:22:43.536101: train-step 9731, loss 0.193595, acc 0.95
2016-05-21T12:22:49.773371: train-step 9732, loss 0.135689, acc 1
2016-05-21T12:22:55.934199: train-step 9733, loss 0.132968, acc 0.99
2016-05-21T12:23:02.164752: train-step 9734, loss 0.119695, acc 1
2016-05-21T12:23:08.362264: train-step 9735, loss 0.153618, acc 0.99
2016-05-21T12:23:14.583277: train-step 9736, loss 0.13782, acc 0.97
2016-05-21T12:23:20.824904: train-step 9737, loss 0.146663, acc 0.98
2016-05-21T12:23:27.064860: train-step 9738, loss 0.15513, acc 0.97
2016-05-21T12:23:33.274428: train-step 9739, loss 0.181004, acc 0.96
2016-05-21T12:23:39.498950: train-step 9740, loss 0.162057, acc 0.97
2016-05-21T12:23:45.726856: train-step 9741, loss 0.164998, acc 0.98
2016-05-21T12:23:51.938400: train-step 9742, loss 0.145982, acc 0.97
2016-05-21T12:23:58.137123: train-step 9743, loss 0.174187, acc 0.97
2016-05-21T12:24:04.369249: train-step 9744, loss 0.148887, acc 0.99
2016-05-21T12:24:10.539509: train-step 9745, loss 0.188422, acc 0.95
2016-05-21T12:24:16.731992: train-step 9746, loss 0.165597, acc 0.98
2016-05-21T12:24:22.894374: train-step 9747, loss 0.126728, acc 0.99
2016-05-21T12:24:29.098069: train-step 9748, loss 0.107414, acc 1
2016-05-21T12:24:35.252021: train-step 9749, loss 0.171932, acc 0.98
2016-05-21T12:24:41.481074: train-step 9750, loss 0.175843, acc 0.96
epoch number is: 39
2016-05-21T12:24:47.839095: train-step 9751, loss 0.141349, acc 0.98
2016-05-21T12:24:53.983675: train-step 9752, loss 0.137195, acc 0.98
2016-05-21T12:25:00.194774: train-step 9753, loss 0.125161, acc 0.99
2016-05-21T12:25:06.431369: train-step 9754, loss 0.162963, acc 0.99
2016-05-21T12:25:12.657356: train-step 9755, loss 0.167329, acc 0.99
2016-05-21T12:25:18.900601: train-step 9756, loss 0.139408, acc 1
2016-05-21T12:25:25.078337: train-step 9757, loss 0.146035, acc 1
2016-05-21T12:25:31.265554: train-step 9758, loss 0.183174, acc 0.96
2016-05-21T12:25:37.479720: train-step 9759, loss 0.125314, acc 1
2016-05-21T12:25:43.716217: train-step 9760, loss 0.125131, acc 1
2016-05-21T12:25:50.095788: train-step 9761, loss 0.170879, acc 0.96
2016-05-21T12:25:56.457833: train-step 9762, loss 0.140554, acc 1
2016-05-21T12:26:02.668229: train-step 9763, loss 0.153826, acc 1
2016-05-21T12:26:08.931838: train-step 9764, loss 0.165964, acc 0.97
2016-05-21T12:26:15.075080: train-step 9765, loss 0.179592, acc 0.98
2016-05-21T12:26:21.317953: train-step 9766, loss 0.130972, acc 1
2016-05-21T12:26:27.678870: train-step 9767, loss 0.144051, acc 0.99
2016-05-21T12:26:33.886320: train-step 9768, loss 0.173076, acc 0.96
2016-05-21T12:26:40.142127: train-step 9769, loss 0.13148, acc 0.99
2016-05-21T12:26:46.485022: train-step 9770, loss 0.127364, acc 0.99
2016-05-21T12:26:52.648370: train-step 9771, loss 0.13986, acc 0.98
2016-05-21T12:26:59.184752: train-step 9772, loss 0.167553, acc 0.98
2016-05-21T12:27:05.617919: train-step 9773, loss 0.130285, acc 0.99
2016-05-21T12:27:11.810189: train-step 9774, loss 0.141034, acc 0.98
2016-05-21T12:27:18.083748: train-step 9775, loss 0.141915, acc 0.98
2016-05-21T12:27:24.408394: train-step 9776, loss 0.131411, acc 0.99
2016-05-21T12:27:30.935380: train-step 9777, loss 0.187115, acc 0.98
2016-05-21T12:27:37.245403: train-step 9778, loss 0.192151, acc 0.97
2016-05-21T12:27:43.465787: train-step 9779, loss 0.147444, acc 0.98
2016-05-21T12:27:49.685395: train-step 9780, loss 0.128517, acc 1
2016-05-21T12:27:55.873115: train-step 9781, loss 0.151109, acc 0.99
2016-05-21T12:28:02.044220: train-step 9782, loss 0.142827, acc 1
2016-05-21T12:28:08.191721: train-step 9783, loss 0.123633, acc 0.99
2016-05-21T12:28:14.598943: train-step 9784, loss 0.150315, acc 0.99
2016-05-21T12:28:20.804797: train-step 9785, loss 0.137259, acc 0.99
2016-05-21T12:28:27.081468: train-step 9786, loss 0.164389, acc 0.99
2016-05-21T12:28:33.335291: train-step 9787, loss 0.106426, acc 1
2016-05-21T12:28:39.536741: train-step 9788, loss 0.159318, acc 0.98
2016-05-21T12:28:45.702275: train-step 9789, loss 0.146569, acc 0.97
2016-05-21T12:28:51.865717: train-step 9790, loss 0.143928, acc 0.99
2016-05-21T12:28:58.042683: train-step 9791, loss 0.184771, acc 0.96
2016-05-21T12:29:04.196661: train-step 9792, loss 0.148492, acc 0.99
2016-05-21T12:29:10.437513: train-step 9793, loss 0.146712, acc 0.98
2016-05-21T12:29:17.539102: train-step 9794, loss 0.115781, acc 0.99
2016-05-21T12:29:24.330401: train-step 9795, loss 0.157577, acc 0.98
2016-05-21T12:29:30.707067: train-step 9796, loss 0.146835, acc 0.99
2016-05-21T12:29:36.950905: train-step 9797, loss 0.161141, acc 0.97
2016-05-21T12:29:43.152501: train-step 9798, loss 0.156973, acc 0.97
2016-05-21T12:29:49.428409: train-step 9799, loss 0.158477, acc 0.98
2016-05-21T12:29:55.641593: train-step 9800, loss 0.142421, acc 0.97
2016-05-21T12:30:01.850601: train-step 9801, loss 0.152769, acc 1
2016-05-21T12:30:08.049230: train-step 9802, loss 0.136777, acc 0.99
2016-05-21T12:30:14.619828: train-step 9803, loss 0.129099, acc 0.98
2016-05-21T12:30:20.926960: train-step 9804, loss 0.13827, acc 0.99
2016-05-21T12:30:27.154954: train-step 9805, loss 0.138842, acc 1
2016-05-21T12:30:33.765151: train-step 9806, loss 0.177288, acc 0.97
2016-05-21T12:30:40.114690: train-step 9807, loss 0.136435, acc 0.98
2016-05-21T12:30:46.492651: train-step 9808, loss 0.162594, acc 0.97
2016-05-21T12:30:52.736456: train-step 9809, loss 0.130038, acc 1
2016-05-21T12:30:58.934097: train-step 9810, loss 0.162067, acc 0.97
2016-05-21T12:31:05.148225: train-step 9811, loss 0.144934, acc 0.99
2016-05-21T12:31:11.407099: train-step 9812, loss 0.159768, acc 0.97
2016-05-21T12:31:17.641848: train-step 9813, loss 0.124156, acc 0.99
2016-05-21T12:31:23.899532: train-step 9814, loss 0.141702, acc 0.98
2016-05-21T12:31:30.128157: train-step 9815, loss 0.15982, acc 0.98
2016-05-21T12:31:36.286908: train-step 9816, loss 0.149124, acc 0.98
2016-05-21T12:31:42.692770: train-step 9817, loss 0.164947, acc 0.97
2016-05-21T12:31:49.123058: train-step 9818, loss 0.14267, acc 0.98
2016-05-21T12:31:55.360812: train-step 9819, loss 0.156169, acc 0.96
2016-05-21T12:32:01.562928: train-step 9820, loss 0.140626, acc 0.99
2016-05-21T12:32:07.746607: train-step 9821, loss 0.152259, acc 0.99
2016-05-21T12:32:13.961408: train-step 9822, loss 0.14596, acc 0.97
2016-05-21T12:32:20.237312: train-step 9823, loss 0.125786, acc 1
2016-05-21T12:32:26.461228: train-step 9824, loss 0.154088, acc 0.97
2016-05-21T12:32:32.640261: train-step 9825, loss 0.133494, acc 0.99
2016-05-21T12:32:39.121707: train-step 9826, loss 0.132084, acc 0.99
2016-05-21T12:32:45.564140: train-step 9827, loss 0.138268, acc 0.98
2016-05-21T12:32:51.804353: train-step 9828, loss 0.134246, acc 1
2016-05-21T12:32:58.044585: train-step 9829, loss 0.116453, acc 0.99
2016-05-21T12:33:04.232277: train-step 9830, loss 0.166747, acc 0.97
2016-05-21T12:33:10.408898: train-step 9831, loss 0.206936, acc 0.95
2016-05-21T12:33:16.622591: train-step 9832, loss 0.123537, acc 1
2016-05-21T12:33:22.933641: train-step 9833, loss 0.143038, acc 0.98
2016-05-21T12:33:29.168011: train-step 9834, loss 0.150793, acc 0.99
2016-05-21T12:33:35.360949: train-step 9835, loss 0.144409, acc 0.98
2016-05-21T12:33:41.957518: train-step 9836, loss 0.124534, acc 0.99
2016-05-21T12:33:48.324315: train-step 9837, loss 0.138015, acc 0.99
2016-05-21T12:33:54.554433: train-step 9838, loss 0.188047, acc 0.95
2016-05-21T12:34:00.741654: train-step 9839, loss 0.146471, acc 0.98
2016-05-21T12:34:06.949435: train-step 9840, loss 0.15003, acc 0.98
2016-05-21T12:34:13.140131: train-step 9841, loss 0.18477, acc 0.98
2016-05-21T12:34:19.394496: train-step 9842, loss 0.144871, acc 0.98
2016-05-21T12:34:25.968344: train-step 9843, loss 0.175979, acc 0.97
2016-05-21T12:34:32.287048: train-step 9844, loss 0.159496, acc 0.98
2016-05-21T12:34:38.425105: train-step 9845, loss 0.139417, acc 0.99
2016-05-21T12:34:44.608977: train-step 9846, loss 0.118082, acc 1
2016-05-21T12:34:51.008938: train-step 9847, loss 0.14796, acc 0.98
2016-05-21T12:34:57.211919: train-step 9848, loss 0.143582, acc 0.99
2016-05-21T12:35:03.466727: train-step 9849, loss 0.12799, acc 0.98
2016-05-21T12:35:09.728065: train-step 9850, loss 0.156587, acc 0.97
2016-05-21T12:35:15.925563: train-step 9851, loss 0.1556, acc 0.99
2016-05-21T12:35:22.101913: train-step 9852, loss 0.120591, acc 0.99
2016-05-21T12:35:28.322281: train-step 9853, loss 0.179073, acc 0.95
2016-05-21T12:35:34.529740: train-step 9854, loss 0.159079, acc 0.98
2016-05-21T12:35:40.711858: train-step 9855, loss 0.159396, acc 0.97
2016-05-21T12:35:46.884745: train-step 9856, loss 0.136535, acc 0.99
2016-05-21T12:35:53.108155: train-step 9857, loss 0.186995, acc 0.96
2016-05-21T12:35:59.338313: train-step 9858, loss 0.121797, acc 0.99
2016-05-21T12:36:05.594275: train-step 9859, loss 0.181925, acc 0.98
2016-05-21T12:36:11.752633: train-step 9860, loss 0.165353, acc 0.97
2016-05-21T12:36:17.926922: train-step 9861, loss 0.155355, acc 0.99
2016-05-21T12:36:24.115568: train-step 9862, loss 0.141608, acc 0.99
2016-05-21T12:36:30.300173: train-step 9863, loss 0.13603, acc 0.98
2016-05-21T12:36:36.429512: train-step 9864, loss 0.140903, acc 0.99
2016-05-21T12:36:42.620270: train-step 9865, loss 0.138175, acc 0.99
2016-05-21T12:36:48.787115: train-step 9866, loss 0.142513, acc 0.98
2016-05-21T12:36:54.954798: train-step 9867, loss 0.13859, acc 0.98
2016-05-21T12:37:01.187369: train-step 9868, loss 0.161653, acc 0.98
2016-05-21T12:37:07.524799: train-step 9869, loss 0.13703, acc 0.98
2016-05-21T12:37:13.756233: train-step 9870, loss 0.145751, acc 0.98
2016-05-21T12:37:19.925528: train-step 9871, loss 0.199099, acc 0.96
2016-05-21T12:37:26.143569: train-step 9872, loss 0.147672, acc 0.99
2016-05-21T12:37:32.361377: train-step 9873, loss 0.13594, acc 0.99
2016-05-21T12:37:38.508948: train-step 9874, loss 0.126338, acc 0.99
2016-05-21T12:37:44.723899: train-step 9875, loss 0.134464, acc 0.98
2016-05-21T12:37:51.122383: train-step 9876, loss 0.104675, acc 0.99
2016-05-21T12:37:57.366039: train-step 9877, loss 0.123702, acc 1
2016-05-21T12:38:03.576822: train-step 9878, loss 0.134126, acc 1
2016-05-21T12:38:09.949931: train-step 9879, loss 0.134171, acc 0.99
2016-05-21T12:38:16.216288: train-step 9880, loss 0.153485, acc 0.98
2016-05-21T12:38:22.452519: train-step 9881, loss 0.172803, acc 0.96
2016-05-21T12:38:28.612771: train-step 9882, loss 0.160366, acc 0.97
2016-05-21T12:38:34.834528: train-step 9883, loss 0.140892, acc 1
2016-05-21T12:38:41.059524: train-step 9884, loss 0.146498, acc 0.99
2016-05-21T12:38:47.276686: train-step 9885, loss 0.138042, acc 0.99
2016-05-21T12:38:53.796390: train-step 9886, loss 0.177209, acc 0.98
2016-05-21T12:39:00.143384: train-step 9887, loss 0.149053, acc 0.97
2016-05-21T12:39:06.354611: train-step 9888, loss 0.14889, acc 0.99
2016-05-21T12:39:12.508967: train-step 9889, loss 0.174792, acc 0.97
2016-05-21T12:39:18.705560: train-step 9890, loss 0.128549, acc 0.99
2016-05-21T12:39:25.073523: train-step 9891, loss 0.160779, acc 0.97
2016-05-21T12:39:31.273103: train-step 9892, loss 0.143286, acc 0.99
2016-05-21T12:39:37.523306: train-step 9893, loss 0.143409, acc 0.99
2016-05-21T12:39:43.730432: train-step 9894, loss 0.139724, acc 0.99
2016-05-21T12:39:49.956728: train-step 9895, loss 0.140667, acc 0.98
2016-05-21T12:39:56.301459: train-step 9896, loss 0.162911, acc 0.98
2016-05-21T12:40:02.541203: train-step 9897, loss 0.112874, acc 0.99
2016-05-21T12:40:09.052951: train-step 9898, loss 0.152373, acc 0.97
2016-05-21T12:40:15.366319: train-step 9899, loss 0.150216, acc 0.99
2016-05-21T12:40:21.760835: train-step 9900, loss 0.131095, acc 0.98
2016-05-21T12:40:27.995027: train-step 9901, loss 0.169376, acc 0.98
2016-05-21T12:40:34.309445: train-step 9902, loss 0.134873, acc 0.99
2016-05-21T12:40:40.531457: train-step 9903, loss 0.166776, acc 0.96
2016-05-21T12:40:46.748257: train-step 9904, loss 0.120859, acc 1
2016-05-21T12:40:52.941577: train-step 9905, loss 0.167633, acc 0.98
2016-05-21T12:40:59.220965: train-step 9906, loss 0.175155, acc 0.97
2016-05-21T12:41:05.483465: train-step 9907, loss 0.174653, acc 0.97
2016-05-21T12:41:11.690677: train-step 9908, loss 0.150896, acc 0.99
2016-05-21T12:41:18.249585: train-step 9909, loss 0.160095, acc 0.97
2016-05-21T12:41:24.563693: train-step 9910, loss 0.175233, acc 0.96
2016-05-21T12:41:30.769907: train-step 9911, loss 0.177535, acc 0.95
2016-05-21T12:41:36.987247: train-step 9912, loss 0.160845, acc 0.98
2016-05-21T12:41:43.171012: train-step 9913, loss 0.166098, acc 0.99
2016-05-21T12:41:49.489458: train-step 9914, loss 0.114602, acc 1
2016-05-21T12:41:55.813387: train-step 9915, loss 0.132836, acc 0.98
2016-05-21T12:42:02.072053: train-step 9916, loss 0.181067, acc 0.97
2016-05-21T12:42:08.245034: train-step 9917, loss 0.171178, acc 0.97
2016-05-21T12:42:14.499718: train-step 9918, loss 0.147496, acc 0.98
2016-05-21T12:42:20.654503: train-step 9919, loss 0.14989, acc 0.97
2016-05-21T12:42:26.855163: train-step 9920, loss 0.125144, acc 1
2016-05-21T12:42:33.058105: train-step 9921, loss 0.122263, acc 1
2016-05-21T12:42:39.230545: train-step 9922, loss 0.165709, acc 0.97
2016-05-21T12:42:45.429751: train-step 9923, loss 0.171207, acc 0.96
2016-05-21T12:42:51.593326: train-step 9924, loss 0.179631, acc 0.96
2016-05-21T12:42:57.799481: train-step 9925, loss 0.130292, acc 0.99
2016-05-21T12:43:04.184282: train-step 9926, loss 0.144645, acc 0.97
2016-05-21T12:43:10.535840: train-step 9927, loss 0.12467, acc 1
2016-05-21T12:43:16.785414: train-step 9928, loss 0.155341, acc 0.97
2016-05-21T12:43:23.030811: train-step 9929, loss 0.137024, acc 0.99
2016-05-21T12:43:29.230479: train-step 9930, loss 0.137256, acc 0.99
2016-05-21T12:43:35.399517: train-step 9931, loss 0.136435, acc 0.98
2016-05-21T12:43:41.602281: train-step 9932, loss 0.138099, acc 0.99
2016-05-21T12:43:47.932300: train-step 9933, loss 0.128406, acc 0.99
2016-05-21T12:43:54.320252: train-step 9934, loss 0.188026, acc 0.94
2016-05-21T12:44:00.558537: train-step 9935, loss 0.126692, acc 0.99
2016-05-21T12:44:06.718281: train-step 9936, loss 0.119518, acc 1
2016-05-21T12:44:12.978919: train-step 9937, loss 0.152377, acc 0.99
2016-05-21T12:44:19.195195: train-step 9938, loss 0.160462, acc 0.95
2016-05-21T12:44:25.424393: train-step 9939, loss 0.136956, acc 0.99
2016-05-21T12:44:31.616308: train-step 9940, loss 0.150772, acc 0.98
2016-05-21T12:44:37.841713: train-step 9941, loss 0.144459, acc 0.99
2016-05-21T12:44:44.113295: train-step 9942, loss 0.20859, acc 0.93
2016-05-21T12:44:50.534959: train-step 9943, loss 0.161633, acc 0.98
2016-05-21T12:44:57.107928: train-step 9944, loss 0.16574, acc 0.98
2016-05-21T12:45:03.301202: train-step 9945, loss 0.138952, acc 0.97
2016-05-21T12:45:09.517187: train-step 9946, loss 0.136699, acc 0.99
2016-05-21T12:45:15.738386: train-step 9947, loss 0.159004, acc 0.97
2016-05-21T12:45:22.323861: train-step 9948, loss 0.137247, acc 0.99
2016-05-21T12:45:28.661774: train-step 9949, loss 0.17539, acc 0.98
2016-05-21T12:45:34.871508: train-step 9950, loss 0.119485, acc 1
2016-05-21T12:45:41.087522: train-step 9951, loss 0.13612, acc 1
2016-05-21T12:45:47.276446: train-step 9952, loss 0.137266, acc 0.99
2016-05-21T12:45:53.423075: train-step 9953, loss 0.162427, acc 0.99
2016-05-21T12:45:59.705695: train-step 9954, loss 0.152919, acc 0.97
2016-05-21T12:46:05.926007: train-step 9955, loss 0.14509, acc 0.99
2016-05-21T12:46:12.090189: train-step 9956, loss 0.127172, acc 1
2016-05-21T12:46:18.301782: train-step 9957, loss 0.118427, acc 1
2016-05-21T12:46:24.526046: train-step 9958, loss 0.145706, acc 0.99
2016-05-21T12:46:30.760224: train-step 9959, loss 0.159328, acc 0.96
2016-05-21T12:46:36.978613: train-step 9960, loss 0.162805, acc 0.96
2016-05-21T12:46:43.183806: train-step 9961, loss 0.140381, acc 0.98
2016-05-21T12:46:49.420500: train-step 9962, loss 0.15344, acc 0.98
2016-05-21T12:46:55.583438: train-step 9963, loss 0.196777, acc 0.95
2016-05-21T12:47:01.835346: train-step 9964, loss 0.144057, acc 0.97
2016-05-21T12:47:07.968103: train-step 9965, loss 0.117175, acc 1
2016-05-21T12:47:14.187154: train-step 9966, loss 0.133874, acc 0.98
2016-05-21T12:47:20.352673: train-step 9967, loss 0.124961, acc 1
2016-05-21T12:47:26.588088: train-step 9968, loss 0.153423, acc 0.99
2016-05-21T12:47:32.835404: train-step 9969, loss 0.162038, acc 0.95
2016-05-21T12:47:39.036591: train-step 9970, loss 0.164526, acc 0.98
2016-05-21T12:47:45.230202: train-step 9971, loss 0.118398, acc 0.99
2016-05-21T12:47:51.449286: train-step 9972, loss 0.147036, acc 0.98
2016-05-21T12:47:57.663922: train-step 9973, loss 0.144229, acc 0.99
2016-05-21T12:48:03.886782: train-step 9974, loss 0.185714, acc 0.96
2016-05-21T12:48:10.093097: train-step 9975, loss 0.13802, acc 0.99
2016-05-21T12:48:16.577052: train-step 9976, loss 0.155652, acc 0.99
2016-05-21T12:48:22.829930: train-step 9977, loss 0.125695, acc 0.99
2016-05-21T12:48:29.093346: train-step 9978, loss 0.154624, acc 0.98
2016-05-21T12:48:35.341713: train-step 9979, loss 0.169568, acc 0.98
2016-05-21T12:48:41.614745: train-step 9980, loss 0.11858, acc 0.99
2016-05-21T12:48:47.942524: train-step 9981, loss 0.178146, acc 0.96
2016-05-21T12:48:54.879080: train-step 9982, loss 0.175326, acc 0.98
2016-05-21T12:49:01.492919: train-step 9983, loss 0.188075, acc 0.96
2016-05-21T12:49:07.883166: train-step 9984, loss 0.163542, acc 0.98
2016-05-21T12:49:14.240639: train-step 9985, loss 0.173231, acc 0.97
2016-05-21T12:49:20.524982: train-step 9986, loss 0.130539, acc 1
2016-05-21T12:49:26.699614: train-step 9987, loss 0.124802, acc 0.99
2016-05-21T12:49:32.958669: train-step 9988, loss 0.133282, acc 1
2016-05-21T12:49:39.213470: train-step 9989, loss 0.160683, acc 0.98
2016-05-21T12:49:45.505108: train-step 9990, loss 0.145098, acc 0.97
2016-05-21T12:49:52.105216: train-step 9991, loss 0.161873, acc 0.98
2016-05-21T12:49:58.388860: train-step 9992, loss 0.156433, acc 0.96
2016-05-21T12:50:04.702228: train-step 9993, loss 0.151543, acc 0.97
2016-05-21T12:50:10.995592: train-step 9994, loss 0.141411, acc 0.98
2016-05-21T12:50:17.403662: train-step 9995, loss 0.14458, acc 0.99
2016-05-21T12:50:23.621469: train-step 9996, loss 0.139657, acc 0.99
2016-05-21T12:50:29.915830: train-step 9997, loss 0.133869, acc 0.99
2016-05-21T12:50:36.365203: train-step 9998, loss 0.169938, acc 0.96
2016-05-21T12:50:42.828251: train-step 9999, loss 0.162864, acc 1
2016-05-21T12:50:49.232639: train-step 10000, loss 0.163596, acc 0.97
2016-05-21T12:50:52.429305  dev-step: 1  acc: 0.91
2016-05-21T12:50:54.976635  dev-step: 2  acc: 0.94
2016-05-21T12:50:57.551537  dev-step: 3  acc: 0.96
2016-05-21T12:51:00.213500  dev-step: 4  acc: 0.94
2016-05-21T12:51:02.804024  dev-step: 5  acc: 0.93
2016-05-21T12:51:05.368416  dev-step: 6  acc: 0.94
2016-05-21T12:51:07.908221  dev-step: 7  acc: 0.93
2016-05-21T12:51:10.476976  dev-step: 8  acc: 0.96
2016-05-21T12:51:13.032056  dev-step: 9  acc: 0.97
2016-05-21T12:51:15.534141  dev-step: 10  acc: 0.97
2016-05-21T12:51:18.105087  dev-step: 11  acc: 0.92
2016-05-21T12:51:20.706498  dev-step: 12  acc: 0.98
2016-05-21T12:51:23.267302  dev-step: 13  acc: 0.93
2016-05-21T12:51:25.772293  dev-step: 14  acc: 0.91
2016-05-21T12:51:28.428902  dev-step: 15  acc: 0.92
2016-05-21T12:51:30.942144  dev-step: 16  acc: 0.94
2016-05-21T12:51:33.491076  dev-step: 17  acc: 0.9
2016-05-21T12:51:36.064680  dev-step: 18  acc: 0.94
2016-05-21T12:51:38.593410  dev-step: 19  acc: 0.92
2016-05-21T12:51:41.112285  dev-step: 20  acc: 0.92
2016-05-21T12:51:43.653989  dev-step: 21  acc: 0.94
2016-05-21T12:51:46.193326  dev-step: 22  acc: 0.91
2016-05-21T12:51:48.724422  dev-step: 23  acc: 0.92
2016-05-21T12:51:51.375978  dev-step: 24  acc: 0.94
2016-05-21T12:51:54.015504  dev-step: 25  acc: 0.87
2016-05-21T12:51:56.630826  dev-step: 26  acc: 0.93
2016-05-21T12:51:59.181048  dev-step: 27  acc: 0.82
2016-05-21T12:52:01.713351  dev-step: 28  acc: 0.91
2016-05-21T12:52:04.300382  dev-step: 29  acc: 0.97
2016-05-21T12:52:07.033803  dev-step: 30  acc: 0.91
2016-05-21T12:52:09.818597  dev-step: 31  acc: 0.93
2016-05-21T12:52:12.499245  dev-step: 32  acc: 0.88
2016-05-21T12:52:15.118864  dev-step: 33  acc: 0.95
2016-05-21T12:52:17.682768  dev-step: 34  acc: 0.92
2016-05-21T12:52:20.391717  dev-step: 35  acc: 0.95
2016-05-21T12:52:22.985787  dev-step: 36  acc: 0.93
2016-05-21T12:52:25.529690  dev-step: 37  acc: 0.95
2016-05-21T12:52:28.018016  dev-step: 38  acc: 0.9
2016-05-21T12:52:30.584248  dev-step: 39  acc: 0.93
2016-05-21T12:52:33.191202  dev-step: 40  acc: 0.96
2016-05-21T12:52:35.683686  dev-step: 41  acc: 0.94
2016-05-21T12:52:38.206894  dev-step: 42  acc: 0.94
2016-05-21T12:52:40.812205  dev-step: 43  acc: 0.88
2016-05-21T12:52:43.355077  dev-step: 44  acc: 0.9
2016-05-21T12:52:45.907394  dev-step: 45  acc: 0.91
2016-05-21T12:52:48.457047  dev-step: 46  acc: 0.88
2016-05-21T12:52:50.982175  dev-step: 47  acc: 0.85
2016-05-21T12:52:53.688500  dev-step: 48  acc: 0.93
2016-05-21T12:52:56.239277  dev-step: 49  acc: 0.98
2016-05-21T12:52:58.901954  dev-step: 50  acc: 0.93
2016-05-21T12:53:01.534076  dev-step: 51  acc: 0.95
2016-05-21T12:53:04.107339  dev-step: 52  acc: 0.94
2016-05-21T12:53:06.615356  dev-step: 53  acc: 0.94
2016-05-21T12:53:09.227594  dev-step: 54  acc: 0.98
2016-05-21T12:53:11.744924  dev-step: 55  acc: 0.88
2016-05-21T12:53:14.242499  dev-step: 56  acc: 0.92
2016-05-21T12:53:16.735333  dev-step: 57  acc: 0.89
2016-05-21T12:53:19.319812  dev-step: 58  acc: 0.93
2016-05-21T12:53:21.965894  dev-step: 59  acc: 0.94
2016-05-21T12:53:24.481987  dev-step: 60  acc: 0.91
2016-05-21T12:53:27.038427  dev-step: 61  acc: 0.9
2016-05-21T12:53:29.574230  dev-step: 62  acc: 0.88
2016-05-21T12:53:32.172012  dev-step: 63  acc: 0.9
2016-05-21T12:53:34.717316  dev-step: 64  acc: 0.97
2016-05-21T12:53:37.254594  dev-step: 65  acc: 0.87
2016-05-21T12:53:39.805423  dev-step: 66  acc: 0.91
2016-05-21T12:53:42.353697  dev-step: 67  acc: 0.9
2016-05-21T12:53:44.833991  dev-step: 68  acc: 0.93
2016-05-21T12:53:47.468859  dev-step: 69  acc: 0.97
2016-05-21T12:53:50.055066  dev-step: 70  acc: 0.95
2016-05-21T12:53:52.691600  dev-step: 71  acc: 0.95
2016-05-21T12:53:55.446900  dev-step: 72  acc: 0.94
2016-05-21T12:53:58.403203  dev-step: 73  acc: 0.93
2016-05-21T12:54:01.231228  dev-step: 74  acc: 0.92
2016-05-21T12:54:03.725425  dev-step: 75  acc: 0.95
2016-05-21T12:54:06.266771  dev-step: 76  acc: 0.93
2016-05-21T12:54:08.855255  dev-step: 77  acc: 0.92
2016-05-21T12:54:11.540300  dev-step: 78  acc: 0.93
2016-05-21T12:54:14.280918  dev-step: 79  acc: 0.91
2016-05-21T12:54:16.936142  dev-step: 80  acc: 0.95
2016-05-21T12:54:19.517696  dev-step: 81  acc: 0.96
2016-05-21T12:54:22.049231  dev-step: 82  acc: 0.9
2016-05-21T12:54:24.573710  dev-step: 83  acc: 0.92
2016-05-21T12:54:27.100469  dev-step: 84  acc: 0.91
2016-05-21T12:54:29.672531  dev-step: 85  acc: 0.95
2016-05-21T12:54:32.208580  dev-step: 86  acc: 0.96
2016-05-21T12:54:34.756754  dev-step: 87  acc: 0.92
2016-05-21T12:54:37.294461  dev-step: 88  acc: 0.95
2016-05-21T12:54:39.847459  dev-step: 89  acc: 0.91
2016-05-21T12:54:42.393340  dev-step: 90  acc: 0.93
2016-05-21T12:54:44.962859  dev-step: 91  acc: 0.89
2016-05-21T12:54:47.443179  dev-step: 92  acc: 0.94
2016-05-21T12:54:49.941806  dev-step: 93  acc: 0.92
2016-05-21T12:54:52.515396  dev-step: 94  acc: 0.98
2016-05-21T12:54:55.160359  dev-step: 95  acc: 0.98
2016-05-21T12:54:57.849535  dev-step: 96  acc: 0.94
2016-05-21T12:55:00.466554  dev-step: 97  acc: 0.86
2016-05-21T12:55:03.059732  dev-step: 98  acc: 0.88
2016-05-21T12:55:05.630415  dev-step: 99  acc: 0.87
2016-05-21T12:55:08.160023  dev-step: 100  acc: 0.96
2016-05-21T12:55:10.676584  dev-step: 101  acc: 0.96
2016-05-21T12:55:13.257244  dev-step: 102  acc: 0.93
2016-05-21T12:55:15.797740  dev-step: 103  acc: 0.96
2016-05-21T12:55:18.370951  dev-step: 104  acc: 0.9
2016-05-21T12:55:20.921576  dev-step: 105  acc: 0.94
2016-05-21T12:55:23.418571  dev-step: 106  acc: 0.95
2016-05-21T12:55:25.970125  dev-step: 107  acc: 0.93
2016-05-21T12:55:28.719732  dev-step: 108  acc: 0.92
2016-05-21T12:55:31.476437  dev-step: 109  acc: 0.89
2016-05-21T12:55:34.228362  dev-step: 110  acc: 0.87
2016-05-21T12:55:36.892854  dev-step: 111  acc: 0.95
2016-05-21T12:55:39.563971  dev-step: 112  acc: 0.92
2016-05-21T12:55:42.143799  dev-step: 113  acc: 0.97
2016-05-21T12:55:44.646771  dev-step: 114  acc: 0.89
2016-05-21T12:55:47.216579  dev-step: 115  acc: 0.91
2016-05-21T12:55:49.779859  dev-step: 116  acc: 0.95
2016-05-21T12:55:52.541953  dev-step: 117  acc: 0.93
2016-05-21T12:55:55.241992  dev-step: 118  acc: 0.91
2016-05-21T12:55:57.861338  dev-step: 119  acc: 0.88
2016-05-21T12:56:00.400143  dev-step: 120  acc: 0.88
2016-05-21T12:56:02.946033  dev-step: 121  acc: 0.89
2016-05-21T12:56:05.476335  dev-step: 122  acc: 0.84
2016-05-21T12:56:08.011149  dev-step: 123  acc: 0.93
2016-05-21T12:56:10.507595  dev-step: 124  acc: 0.9
2016-05-21T12:56:13.019595  dev-step: 125  acc: 0.97
2016-05-21T12:56:15.610192  dev-step: 126  acc: 0.92
2016-05-21T12:56:18.203753  dev-step: 127  acc: 0.89
2016-05-21T12:56:20.781836  dev-step: 128  acc: 0.91
2016-05-21T12:56:23.376920  dev-step: 129  acc: 0.85
2016-05-21T12:56:26.015285  dev-step: 130  acc: 0.93
2016-05-21T12:56:28.555493  dev-step: 131  acc: 0.97
2016-05-21T12:56:31.136971  dev-step: 132  acc: 0.96
2016-05-21T12:56:33.691072  dev-step: 133  acc: 0.95
2016-05-21T12:56:36.215313  dev-step: 134  acc: 0.86
2016-05-21T12:56:38.774133  dev-step: 135  acc: 0.97
2016-05-21T12:56:41.311827  dev-step: 136  acc: 0.91
2016-05-21T12:56:43.843833  dev-step: 137  acc: 0.95
2016-05-21T12:56:46.365127  dev-step: 138  acc: 0.93
2016-05-21T12:56:48.858754  dev-step: 139  acc: 0.86
2016-05-21T12:56:51.437497  dev-step: 140  acc: 0.83
2016-05-21T12:56:54.070491  dev-step: 141  acc: 0.9
2016-05-21T12:56:56.646991  dev-step: 142  acc: 0.85
2016-05-21T12:56:59.184705  dev-step: 143  acc: 0.9
2016-05-21T12:57:01.689594  dev-step: 144  acc: 0.92
2016-05-21T12:57:04.355904  dev-step: 145  acc: 0.85
2016-05-21T12:57:07.042449  dev-step: 146  acc: 0.95
2016-05-21T12:57:09.595146  dev-step: 147  acc: 0.89
2016-05-21T12:57:12.085515  dev-step: 148  acc: 0.9
2016-05-21T12:57:14.728092  dev-step: 149  acc: 0.83
2016-05-21T12:57:17.272726  dev-step: 150  acc: 0.92
2016-05-21T12:57:19.762493  dev-step: 151  acc: 0.92
2016-05-21T12:57:22.303473  dev-step: 152  acc: 0.87
2016-05-21T12:57:24.856760  dev-step: 153  acc: 0.91
2016-05-21T12:57:27.488536  dev-step: 154  acc: 0.88
2016-05-21T12:57:30.101629  dev-step: 155  acc: 0.9
2016-05-21T12:57:32.705367  dev-step: 156  acc: 0.84
2016-05-21T12:57:35.256397  dev-step: 157  acc: 0.91
2016-05-21T12:57:37.949443  dev-step: 158  acc: 0.9
2016-05-21T12:57:40.791668  dev-step: 159  acc: 0.91
2016-05-21T12:57:43.615278  dev-step: 160  acc: 0.91
2016-05-21T12:57:46.423975  dev-step: 161  acc: 0.93
2016-05-21T12:57:49.287402  dev-step: 162  acc: 0.96
2016-05-21T12:57:51.837928  dev-step: 163  acc: 0.92
2016-05-21T12:57:54.401878  dev-step: 164  acc: 0.86
2016-05-21T12:57:56.997721  dev-step: 165  acc: 0.93
2016-05-21T12:57:59.745460  dev-step: 166  acc: 0.83
2016-05-21T12:58:02.458074  dev-step: 167  acc: 0.85
2016-05-21T12:58:05.108614  dev-step: 168  acc: 0.93
2016-05-21T12:58:07.688719  dev-step: 169  acc: 0.9
2016-05-21T12:58:10.275136  dev-step: 170  acc: 0.92
2016-05-21T12:58:12.746317  dev-step: 171  acc: 0.92
2016-05-21T12:58:15.323956  dev-step: 172  acc: 0.9
2016-05-21T12:58:17.854285  dev-step: 173  acc: 0.93
2016-05-21T12:58:20.366286  dev-step: 174  acc: 0.93
2016-05-21T12:58:22.942090  dev-step: 175  acc: 0.9
2016-05-21T12:58:25.495236  dev-step: 176  acc: 0.87
2016-05-21T12:58:28.034829  dev-step: 177  acc: 0.92
2016-05-21T12:58:30.603129  dev-step: 178  acc: 0.89
2016-05-21T12:58:33.204708  dev-step: 179  acc: 0.84
2016-05-21T12:58:35.740391  dev-step: 180  acc: 0.9
2016-05-21T12:58:38.556254  dev-step: 181  acc: 0.87
2016-05-21T12:58:41.446111  dev-step: 182  acc: 0.96
2016-05-21T12:58:44.189331  dev-step: 183  acc: 0.92
2016-05-21T12:58:47.032021  dev-step: 184  acc: 0.93
2016-05-21T12:58:49.904752  dev-step: 185  acc: 0.94
2016-05-21T12:58:53.069393  dev-step: 186  acc: 0.86
2016-05-21T12:58:55.946448  dev-step: 187  acc: 0.92
2016-05-21T12:58:58.519988  dev-step: 188  acc: 0.89
2016-05-21T12:59:01.116554  dev-step: 189  acc: 0.86
2016-05-21T12:59:03.745466  dev-step: 190  acc: 0.89
2016-05-21T12:59:06.394232  dev-step: 191  acc: 0.9
2016-05-21T12:59:08.951448  dev-step: 192  acc: 0.93
2016-05-21T12:59:11.586642  dev-step: 193  acc: 0.88
2016-05-21T12:59:14.223621  dev-step: 194  acc: 0.94
2016-05-21T12:59:16.768021  dev-step: 195  acc: 0.92
2016-05-21T12:59:19.359318  dev-step: 196  acc: 0.92
2016-05-21T12:59:22.004043  dev-step: 197  acc: 0.89
2016-05-21T12:59:24.574628  dev-step: 198  acc: 0.94
2016-05-21T12:59:27.190224  dev-step: 199  acc: 0.93
2016-05-21T12:59:29.806986  dev-step: 200  acc: 0.87
2016-05-21T12:59:32.393785  dev-step: 201  acc: 0.89
2016-05-21T12:59:34.902438  dev-step: 202  acc: 0.94
2016-05-21T12:59:37.408206  dev-step: 203  acc: 0.95
2016-05-21T12:59:39.948799  dev-step: 204  acc: 0.91
2016-05-21T12:59:42.478359  dev-step: 205  acc: 0.91
2016-05-21T12:59:45.010773  dev-step: 206  acc: 0.92
2016-05-21T12:59:47.580456  dev-step: 207  acc: 0.87
2016-05-21T12:59:50.157523  dev-step: 208  acc: 0.91
2016-05-21T12:59:52.729751  dev-step: 209  acc: 0.89
2016-05-21T12:59:55.359398  dev-step: 210  acc: 0.91
2016-05-21T12:59:57.990683  dev-step: 211  acc: 0.89
2016-05-21T13:00:00.551431  dev-step: 212  acc: 0.9
2016-05-21T13:00:03.089286  dev-step: 213  acc: 0.88
2016-05-21T13:00:05.614108  dev-step: 214  acc: 0.85
2016-05-21T13:00:08.097462  dev-step: 215  acc: 0.81
2016-05-21T13:00:10.584628  dev-step: 216  acc: 0.9
2016-05-21T13:00:13.075942  dev-step: 217  acc: 0.89
2016-05-21T13:00:15.603436  dev-step: 218  acc: 0.81
2016-05-21T13:00:18.124598  dev-step: 219  acc: 0.85
2016-05-21T13:00:20.656154  dev-step: 220  acc: 0.9
2016-05-21T13:00:23.207715  dev-step: 221  acc: 0.9
2016-05-21T13:00:25.822795  dev-step: 222  acc: 0.94
2016-05-21T13:00:28.361518  dev-step: 223  acc: 0.95
2016-05-21T13:00:30.871067  dev-step: 224  acc: 0.87
2016-05-21T13:00:33.350460  dev-step: 225  acc: 0.92
2016-05-21T13:00:35.907809  dev-step: 226  acc: 0.89
2016-05-21T13:00:38.487843  dev-step: 227  acc: 0.91
2016-05-21T13:00:41.207654  dev-step: 228  acc: 0.86
2016-05-21T13:00:43.863417  dev-step: 229  acc: 0.91
2016-05-21T13:00:46.493084  dev-step: 230  acc: 0.91
2016-05-21T13:00:49.071506  dev-step: 231  acc: 0.9
2016-05-21T13:00:51.707584  dev-step: 232  acc: 0.87
2016-05-21T13:00:54.339094  dev-step: 233  acc: 0.9
2016-05-21T13:00:56.870973  dev-step: 234  acc: 0.94
2016-05-21T13:00:59.445839  dev-step: 235  acc: 0.93
2016-05-21T13:01:02.103696  dev-step: 236  acc: 0.87
2016-05-21T13:01:04.678579  dev-step: 237  acc: 0.89
2016-05-21T13:01:07.310917  dev-step: 238  acc: 0.93
2016-05-21T13:01:09.917345  dev-step: 239  acc: 0.91
2016-05-21T13:01:12.501804  dev-step: 240  acc: 0.83
2016-05-21T13:01:15.111708  dev-step: 241  acc: 0.83
2016-05-21T13:01:17.727836  dev-step: 242  acc: 0.85
2016-05-21T13:01:20.266670  dev-step: 243  acc: 0.84
2016-05-21T13:01:22.868929  dev-step: 244  acc: 0.9
2016-05-21T13:01:25.477188  dev-step: 245  acc: 0.81
2016-05-21T13:01:28.021028  dev-step: 246  acc: 0.87
2016-05-21T13:01:30.639586  dev-step: 247  acc: 0.9
2016-05-21T13:01:33.309744  dev-step: 248  acc: 0.91
2016-05-21T13:01:36.212869  dev-step: 249  acc: 0.8
2016-05-21T13:01:39.072029  dev-step: 250  acc: 0.85
avg_loss 0.277586, avg_acc 0.91076, real_acc 0.91076
